#bib file
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={ICLR},
  year={2014}
}

@article{mishkin2015all,
  title={All you need is a good init},
  author={Mishkin, Dmytro and Matas, Jiri},
  journal={ICML},
  year={2016}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{vapnik98,
  title={An overview of statistical learning theory},
  author={Vapnik, Vladimir Naumovich},
  journal={IEEE transactions on neural networks},
  volume={10},
  number={5},
  pages={988--999},
  year={1999},
  publisher={IEEE}
}


@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1310--1318},
  year={2013}
}

@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}

@inproceedings{chapelle2001vicinal,
  title={Vicinal risk minimization},
  author={Chapelle, Olivier and Weston, Jason and Bottou, L{\'e}on and Vapnik, Vladimir},
  booktitle={Advances in neural information processing systems},
  pages={416--422},
  year={2001}
}

@incollection{robbins1985stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  booktitle={Herbert Robbins Selected Papers},
  pages={102--109},
  year={1985},
  publisher={Springer}
}


@article{smith2018don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={ICML},
  year={2018}
}

@article{Blier2018,
  title={Learning with Random Learning Rates},
  author={L. Blier, P. Wolinski, Y. Ollivier},
  journal={arXiv preprint arXiv:1810.01322},
  year={2018}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{Cauchy47,
  title={M\'{e}thode g\'{e}n\'{e}rale pour la r\'{e}solution des syst\`{e}mes d?\'{e}quations
simultan\'{e}es},
  author={A. Cauchy},
  journal={arXiv preprint arXiv:1608.03983},
  year={1847}
}


@article{Qian99,
  title={On the momentum term in gradient descent learning algorithms},
  author={N. Qian},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999}
}



@article{Nesterov83,
title={A method for solving the convex programming problem with convergence rate $O (1/k^ 2)$},
author={Y.E. Nesterov},
journal={Dokl. Akad. Nauk SSSR},
year={1983},
pages={543--547},
volume={269}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{zeiler2012adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year={2012}
}

@article{amari2000adaptive,
  title={Adaptive method of realizing natural gradient learning for multilayer perceptrons},
  author={Amari, Shun-Ichi and Park, Hyeyoung and Fukumizu, Kenji},
  journal={Neural Computation},
  volume={12},
  number={6},
  pages={1399--1409},
  year={2000},
  publisher={MIT Press}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={ICLR},
  year={2014}
}

@article{reddi2018convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={ICLR},
  year={2018}
}

@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get M for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={ICLR},
  year={2017}
}


@article{dozat2016deep,
  title={Deep biaffine attention for neural dependency parsing},
  author={Dozat, Timothy and Manning, Christopher D},
  journal={ICLR},
  year={2017}
}
