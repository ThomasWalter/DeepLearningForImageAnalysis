\documentclass[handout,xcolor=pdftex,dvipsnames,table,mathserif]{beamer}
%\documentclass[xcolor=pdftex,dvipsnames,table,mathserif]{beamer}
\input{../../setting.tex}
\usepackage{physics}
\usepackage{tikz}
\usepackage{algorithm,algorithmic}
\usetikzlibrary{fit,positioning}

%% \usepackage{animate}

\AtBeginSection[]{
  \begin{frame}{Contents}
  \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

\AtBeginSubsection[]{
  \begin{frame}{Contents}
  \tableofcontents[currentsection, subsectionstyle=show/shaded/hide]
  \end{frame}
}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{items}[square]

\title{Optimization for Deep Learning}
\author{Santiago VELASCO-FORERO \inst{1} \and Thomas WALTER \inst{2} }
\institute{\inst{1} \href{http://cmm.ensmp.fr/~velasco/}{http://cmm.ensmp.fr/~velasco/} \and
           \inst{2} \href{http://members.cbio.mines-paristech.fr/~twalter/}{http://members.cbio.mines-paristech.fr/~twalter/}}
% \and %
%           \inst{2} \href{http://members.cbio.mines-paristech.fr/~twalter/}}

%\author{Santiago VELASCO-FORERO \\ \href{http://cmm.ensmp.fr/~velasco/}{http://cmm.ensmp.fr/~velasco/}}
\date{MINES ParisTech\\
  PSL Research University\\
  Center for Mathematical Morphology\\
  Center for Computational Biology
}
\titlegraphic{\includegraphics[height=1.7cm]{../graphics/logoemp}}

\useinnertheme{rounded}
\usecolortheme{rose}

\usepackage{xcolor}
\definecolor{lightblue}{RGB}{0,200,255}


\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{items}[square]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{frame}
\titlepage
\end{frame}

\frame{
\frametitle{Overview}
\tableofcontents[]
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{frame}{Machine Learning as an optimization problem 1/2}
\begin{itemize}
	\item We have seen that most Machine Learning methods can be written as an optimization problem:
	\begin{equation*}
		\param ^{\ast} = \argmin_{\param} \loss (\param) + \lambda \mathcal{R}(\param)
	\end{equation*}
	where $\param$ are the parameters of the classifier. 
	\item The function we want to minimize is called \textbf{objective function}. 
	\item The ${\ast}$ denotes the solution of the optimization problem (argument that solves the optimization problem).  
\end{itemize}
\end{frame}

\begin{frame}{Machine Learning as an optimization problem 2/2}
\begin{itemize}
	\item $\loss (\param)$ should in principle be evaluated on the true data distribution: we would like that the loss is minimal for new data that is presented to the algorithm. 
	\item During training however, $\loss (\param)$ is evaluated on the training set $T$: we minimize the \textbf{empirical risk}.
	\item Optimization is central to most Machine Learning algorithms, but the degree of difficulty depends on the method:
	\begin{itemize}
		\item In Linear Discriminant Analysis, we can give an analytical solution.
		\item In Support Vector Machines, we solve a convex optimization problem under constraints.
		\item In Deep Learning, the problem is strongly non-convex and the optimization is difficult on all accounts. 
	\end{itemize}
	\item In Deep Learning, the optimization is also not very well understood: we do not have guarantees, and only few guidelines. Optimization in Deep Learning is an active research topic. 
\end{itemize}
\end{frame}

\section{Stochastic Gradient Descent}

\begin{frame}{Gradient Descent: the principle}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/Optimization_principle.pdf}
\end{figure}
\begin{itemize}
	\item The idea in numerical minimization is to start at some (more or less) random point in the parameter space, and then to find the path towards the minimum. 
	\item In Gradient Descent, we move in the opposite direction of the gradient $\nabla_{\param}L(\param)$
\end{itemize}
\end{frame}

\begin{frame}{Gradient Descent: the algorithm \cite{Cauchy47}}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
\STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
\REPEAT
\STATE t=t+1
\STATE $\mathbf{g}_t = \frac{1}{N} \nabla_{\param} \sum_{i=1}^N \loss(f(\x_i,\param_{t-1}), y_i)$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
\STATE $\param_t=\param_{t-1}  - \eta \mathbf{g}_t$
\UNTIL stopping criterion is met
\RETURN optimized parameters $\param_t$
\end{algorithmic}
\caption{pseudocode gradient descent }
\label{alg:GD}
\end{algorithm}
We note that in this first version, the gradient is calculated for the entire training set. 
\end{frame}

\begin{frame}{Stochastic Gradient Descent (SGD) \cite{robbins1985stochastic}}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
\STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
\REPEAT
\STATE t=t+1
\STATE $\alert{\{\x_1 \ldots \x_m\}=\texttt{SelectBatch} (\X)}$ \begin{tiny}\texttt{Select $m$ samples} \end{tiny}
\STATE $\mathbf{g}_t = \frac{1}{m} \nabla_{\param} \sum_{i=1}^m \loss(f(\x_i,\param_{t-1}), y_i)$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
\STATE $\param_t=\param_{t-1}  - \eta \mathbf{g}_t$
\UNTIL stopping criterion is met
\RETURN optimized parameters $\param_t$
\end{algorithmic}
\caption{pseudocode gradient descent }
\label{alg:SGD}
\end{algorithm}
Here, we draw $m$ samples to calculate the gradient. 
\end{frame}

\begin{frame}{Minibatch size}
\begin{itemize}
	\item The larger the batch-size $m$, the more accurate the gradient is, but the longer the computation takes. 
	\item Optimization algorithms converge faster (in terms of total number of computation) for $m<N$.
	\begin{itemize}
		\item Better more approximate updates than few very precise updates.
		\item Training sets are partly redundant. 
	\end{itemize}
	\item Small batch sizes have a regularizing effect. 
	\item Too small batch sizes underutilize the parallel hardware architectures.
	\item The optimal mini-batch-size also depends on the algorithm. 
\end{itemize}
\end{frame}

\begin{frame}{The step size $\eta$: a critical parameter}
\begin{figure}[htb]
	\centering
	\includegraphics[width=.75\columnwidth]{../graphics/BigLearningRate}
\end{figure}
\begin{itemize}
\item If $\eta$ is set too low, the optimization takes a long time, and the algorithm can get stuck in a local minimum. 
\item If $\eta$ is set too high, a good path might not be found in regions of high curvature. 
%\item It makes sense to adapt the learning rate: starting with a high value and successively decreasing. 
\end{itemize}
\end{frame}





%\begin{frame}{Stochastic Gradient Descent (SGD) update \cite{robbins1985stochastic}}
%\begin{itemize}
%\item \textbf{Require}: Learning rate $\epsilon$ (or a learning rate schedule)
%\item \textbf{Initialization}: $k=1$, $\param_1$ some random value.
%\item \textbf{while} stopping criterion not met \textbf{do}
%\item \quad\quad\quad\textbf{Sample} a value from the training set $(x^{(0)},y^{(0)})$
%\item \quad\quad\quad\textbf{Gradient} $\mathbf{g}(\param_{t}) =\frac{\partial  L(f(x^{(0)},\param_i),y^{(0)})}{ \partial{\param}}$
%\item \quad\quad\quad\textbf{Update} $\param_{t+1}= \param_{t}-\epsilon \mathbf{g}(\param_{t})$
%\item \quad\quad\quad\textbf{} $k=k+1$
%\end{itemize}
%\end{frame}
%
%\begin{frame}{MiniBatch Stochastic Gradient Descent (MSGD) update}
%\begin{itemize}
%\item \textbf{Require}: Learning rate $\epsilon$ (or a learning rate schedule)
%\item \textbf{Initialization}: $k=1$, $\param_1$ some random value.
%\item \textbf{while} stopping criterion not met \textbf{do}
%\item \quad\quad\quad\textbf{Sample} $m$ examples from the training set $\{ (x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\ldots,(x^{(m)},y^{(m)})\}$
%\item \quad\quad\quad\textbf{Gradient} $\mathbf{g}(\param_{t}) =\frac{1}{m}\frac{\partial  \sum_{i=0}^{m}L(f(x^{(i)},\param_i),y^{(i)})}{ \partial{\param}}$
%\item \quad\quad\quad\textbf{Update} $\param_{t+1}= \param_{t}-\epsilon \mathbf{g}(\param_{t})$
%\item \quad\quad\quad\textbf{} $k=k+1$
%\end{itemize}
%Of $m=n$ then we get the BSGD.
%\end{frame}
%
%\begin{frame}{Learning Rate}
%INCLUDE FIGURE
%\end{frame}

\section{Optimizers for Deep Neural Networks}

% \begin{frame}{Intuition: Gradient Descent \cite{Cauchy47}}
% Goal: Minimize $f(x)$ (evaluate \alert{as few times as possible} the function $f$)\\
% The Taylor series of a real $f(x)$ that is infinitely differentiable at a real $x+\epsilon$ is the power series
% \begin{eqnarray*}
% f(x+\epsilon)=f(x)+f'(x)\epsilon+\frac{f''(x)}{2}\epsilon^2+\dots \\
% \approx f(x)+f'(x)\epsilon
% \end{eqnarray*},
% by considering $\epsilon= -\eta f'(x)$ for small $\eta$, we have
% \begin{eqnarray*}
% f(x- \eta f'(x)) \approx f(x)-f'(x)\eta f'(x) =   f(x)-\eta f'(x)^ 2  \\
% f(x-\eta f'(x)) < f(x)
% \end{eqnarray*}
% Intuitively, since the first-order approximation is good only for small $\epsilon$, we want to choose a small $\eta > 0$ to make $\epsilon$ small in magnitude.
% Thus, if our goal is to minimise $f$, we can update
% \begin{equation}
% x_{i+1}=x_i-\eta f'(x)
% \end{equation}
% where $\eta \in \mathbb{R}^{+}$ is called \emph{learning rate}.
% \end{frame}

% \begin{frame}{Gradient Descent \cite{Cauchy47}}
% \begin{algorithm}[H]
% \begin{algorithmic}[1]
% \STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
% \STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
% \REPEAT
% \STATE t=t+1
% \STATE $\mathbf{g}_t = \nabla f_i(\X,\param_{t-1})$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
% \STATE $\param_t=\param_{t-1}  - \eta \mathbf{g}_t$
% \UNTIL stopping criterion is met
% \RETURN optimized parameters $\param_i$
% \end{algorithmic}
% \caption{pseudocode gradient descent }
% \label{alg:seq}
% \end{algorithm}
% Who is $f$ in deep learning? $\mathbb{E}_{\alert{(\mathcal{X},\mathcal{Y})}} [\loss_{\param}(\x,y)]$ for a given loss function $\loss$ 
% \end{frame}

% \begin{frame}{SGD \cite{robbins1985stochastic}}
% \begin{algorithm}[H]
% \begin{algorithmic}[1]
% \STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
% \STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
% \REPEAT
% \STATE t=t+1
% \STATE $\alert{\X_t=\texttt{SelectBatch} (\X)}$ \begin{tiny}\texttt{Select a batch from data, whole data, only one, ... } \end{tiny}
% \STATE $\mathbf{g}_t = \nabla f_i(\alert{\X_t},\param_{t-1})$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
% \STATE $\param_t=\param_{t-1}  - \eta \mathbf{g}_t$
% \UNTIL stopping criterion is met
% \RETURN optimized parameters $\param_i$
% \end{algorithmic}
% \caption{pseudocode for stochastic gradient descent }
% \label{alg:seq}
% \end{algorithm}
% \end{frame}


% \begin{frame}{$\texttt{SelectBatch} (\X)$}
% \begin{enumerate}
% \item Vanilla gradient descent, a.k.a. \emph{batch gradient descent}, computes the gradient of the cost function w.r.t. to the parameters $\param$
% for the entire training dataset.
% \item \emph{Stochastic gradient descent} (SGD) in contrast performs a parameter update for each training example. 
% \item \emph{Mini-batch gradient descent} performs an update for every mini-batch of $\nsize$ training examples.
% \end{enumerate}
% \textit{Some} difficulties:
% \begin{enumerate}
% \item Choosing a proper learning rate can be difficult.
% \item Same learning rate applies to all parameter updates
% \item Highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima.
% \end{enumerate}
% \end{frame}


% \begin{frame}{Difficulties in Deep Network Optimisation}
% \begin{itemize}
% \item[A]{Local minima / Global minima}
% \item[B]{Saddle Point (Plateaus or Flat Regions)}
% \end{itemize}
% \includegraphics[width=.45\columnwidth]{../graphics/LocalGlobalMinimum}
% \includegraphics[width=.45\columnwidth]{../graphics/challenges1} \\

% \end{frame}




% \begin{frame}{Learning Rate}
% \includegraphics[width=.95\columnwidth]{../graphics/BigLearningRate}
% \end{frame}

% \begin{frame}{Learning Rate in Training Process}
% \begin{figure}
% \includegraphics[width=.95\columnwidth]{../graphics/LearningRateEffect}
% \end{figure}
% \texttt{Image by Jeremy Jordan via https://www.jeremyjordan.me/nn-learning-rate/}
% \end{frame}

\begin{frame}{SGD with Learning Rate Schedule}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
\STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
\REPEAT
\STATE t=t+1
\STATE $\{\x_1 \ldots \x_m\}=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select $m$ samples} \end{tiny}
\STATE $\mathbf{g}_t = \frac{1}{m} \nabla_{\param} \sum_{i=1}^m \loss(f(\x_i,\param_{t-1}), y_i)$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
\STATE \alert{$\eta_t = $ SetScheduleMultiplier$(t)$} 
\STATE $\param_t=\param_{t-1}  - \eta_t \mathbf{g}_t$
\UNTIL stopping criterion is met
\RETURN optimized parameters $\param_t$
\end{algorithmic}
\caption{Pseudocode for SGD with Learning Rate Schedule}
\label{alg:schedule}
\end{algorithm}
The schedule can take various forms. 
\end{frame}

\begin{frame}{Schedules 1/3}
\begin{itemize}
\item One option is to successively reduce the learning rate over time.
\item Linear rate decay:
\begin{equation*}
	\eta_k = (1-\frac{k}{\tau})\eta_0 + \frac{k}{\tau} \eta_{\tau}
\end{equation*}
until iteration $\tau$. After this, the learning rate remains constant. 
\item The underlying idea is that in the beginning, the algorithm should explore the parameter space in order to identify a good region.
\item In order to further increase and to avoid bouncing between different slopes, we need to successively decrease the learning rate. 
\item This is the simplest schedule, and is widely used in practice. 
\end{itemize}
\end{frame}

\begin{frame}{Schedules 2/3}
\begin{figure}[htb]
  \centering
  \subfloat{\includegraphics[width=0.46\textwidth]{../graphics/CircularSchedule1.png}}\hspace{.3cm}
  \subfloat{\includegraphics[width=0.46\textwidth]{../graphics/CircularSchedule2.png}}
  \caption{Circular Schedules \cite{smith2017cyclical}}
\end{figure}
\begin{itemize}
	\item Circular schedules with or without decay can be used to move out sharp minima. 
	\item Sharp minima are suspected to lead to poor generalization (small changes in parameters lead to strongly increasing losses). 
	\item Increasing the learning rate can also allow for "more rapid traversal of saddle point plateaus.
\end{itemize}
\end{frame}

\begin{frame}{Schedules 3/3}
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{../graphics/cosine_annealing.png}
  \caption{Stochastic Gradient Descent with Warm Restarts (SGDR) \cite{loshchilov2016sgdr}}
\end{figure}
\begin{itemize}
	\item This strategy also aims at moving out of local minima, in particular sharp local minima. 
	\item An additional idea is to keep track of the models at different minima (end of the cycles).
	\item These models can then be averaged. We therefore get an ensemble of Neural Networks by only one training procedure \cite{huang2017snapshot}. 
\end{itemize}
\end{frame}

% \begin{frame}{SGD with Learning Rate Schedule}
% \begin{algorithm}[H]
% \begin{algorithmic}[1]
% \STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and a dataset $\X$
% \STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$, schedule multiplier $\eta_{t=0} \in \mathbb{R}$
% \REPEAT
% \STATE t=t+1
% \STATE $\X_t=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select a batch from data, whole data, only one, ... } \end{tiny}
% \STATE $\mathbf{g}_t = \nabla f_i(\X_t,\param_{t-1})$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
% \STATE \alert{$\eta_t = $ SetScheduleMultiplier$(t)$} \begin{tiny}{Can be fixed,decay, warm restarts, ...}\end{tiny}
% \STATE $\param_t=\param_{t-1}  - \alert{\eta_{t}} \eta \mathbf{g}_t$
% \UNTIL stopping criterion is met
% \RETURN optimized parameters $\param_i$
% \end{algorithmic}
% \caption{pseudocode for SGD with Learning Rate Schedule}
% \label{alg:seq}
% \end{algorithm}
% \end{frame}

% \begin{frame}{Visualizing the Loss Landscape of Neural Net}
% \begin{figure}
% \includegraphics[width=.5\columnwidth]{../graphics/VGG56Loss}
% \caption{Loss Function for VGG56 \cite{li2017visualizing}}
% \end{figure}
% \end{frame}


% \begin{frame}{SGD with Learning Rate Schedule}
% New problems.... new ideas!
% \begin{itemize}
% \item Fixing the Exponential Moving Decay \cite{dozat2016deep}
% \item Cyclical Learning Rates \cite{smith2017cyclical}
% \item Decay with restarts  \cite{loshchilov2016sgdr}
% %\begin{figure}
% %\includegraphics[width=.225\columnwidth]{../graphics/triangular}
% %\includegraphics[width=.225\columnwidth]{../graphics/triangular2}\\
% %\includegraphics[width=.225\columnwidth]{../graphics/LRRestarts}
% %\includegraphics[width=.225\columnwidth]{../graphics/Snapshot}
% %\caption{Schedules: Triangular  / Triangular with exponential decay / Exponential Decay with Restarts / Snapshot }
% %\end{figure}
% \item Snapshot Ensembles: Train 1, Get M for free \cite{huang2017snapshot}
% %\item Learning with Random Learning Rates \cite{Blier2018}
% \item Don't Decay the Learning Rate, Increase the batch size \cite{smith2018don}
% \end{itemize}
% \end{frame}

% \begin{frame}{Optimizer in Keras}
% \begin{figure}
% \includegraphics[width=.95\columnwidth]{../graphics/SnapshotCurve}
% \caption{\cite{huang2017snapshot}}
% \end{figure}
% \begin{figure}
% \includegraphics[width=.95\columnwidth]{../graphics/KerasSGD}
% \end{figure}
% %Practical Work 1:  Cyclic LR
% \end{frame}

\begin{frame}{SGD with momentum \cite{Qian99}}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
\STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
\REPEAT
\STATE t=t+1
\STATE $\{\x_1 \ldots \x_m\}=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select $m$ samples} \end{tiny}
\STATE $\mathbf{g}_t = \frac{1}{m} \nabla_{\param} \sum_{i=1}^m \loss(f(\x_i,\param_{t-1}), y_i)$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
\STATE $\eta_t = $ SetScheduleMultiplier$(t)$ 
\STATE \alert{$\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + \eta_{t} \mathbf{g}_t $}
\STATE $\param_i=\param_{t-1} - \alert{\mathbf{m}_{t}}$
\UNTIL stopping criterion is met
\RETURN optimized parameters $\param_t$
\end{algorithmic}
\caption{SGD with momentum}
\label{alg:momentum}
\end{algorithm}
\end{frame}


% \begin{frame}{SGD with momentum \cite{Qian99}}
% \begin{algorithm}[H]
% \begin{algorithmic}[1]
% \STATE \textbf{given} initial learning rate $\epsilon \in \mathbb{R}$, dataset $\X$, \alert{momentum factor $\beta_1 \in \mathbb{R}$}
% \STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$, first momentum factor $\mathbf{m}_{t=0}=0 \in \mathbb{R}^\nfeatures$, schedule multiplier $\eta_{t=0} \in \mathbb{R}$
% \REPEAT
% \STATE t=t+1
% \STATE $\X_t=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select a batch from data} \end{tiny}
% \STATE $\mathbf{g}_t = \nabla f_t(\X_t,\param_{t-1})$ \begin{tiny}\texttt{Return parameter gradient} \end{tiny}
% \STATE $\eta_t = $ SetScheduleMultiplier$(t)$ \begin{tiny}{Can be fixed,decay, warm restarts, ...}\end{tiny}
% \STATE \alert{$\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + \eta_{t} \eta  \mathbf{g}_t $}
% \STATE $\param_i=\param_{t-1} - \alert{\mathbf{m}_{t}}$
% \UNTIL stopping criterion is met
% \RETURN optimized parameters $\param_i$
% \end{algorithmic}
% \caption{pseudocode for stochastic gradient descent \alert{with Momentum} }
% \label{alg:seq}
% \end{algorithm}
% \end{frame}

\begin{frame}{Momentum}
\begin{figure}
\includegraphics[width=.95\columnwidth]{../graphics/Momentum}
\end{figure}
\end{frame}

\begin{frame}{Momentum}
\begin{itemize}
	\item Objective: accelerate optimization
	\item Mechanism: accumulation of an exponentially decaying moving average
	\item If the gradient always points to the same direction, it is amplified.
	\item In the case of abrupt changes in the gradient, the update direction is smoothened. 
\end{itemize}
\end{frame}


\begin{frame}{SGD with Nesterov's momentum \cite{Nesterov83}}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{given} initial learning rate $\eta \in \mathbb{R}$ and dataset $\X$
\STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$,
\REPEAT
\STATE t=t+1
\STATE $\{\x_1 \ldots \x_m\}=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select $m$ samples} \end{tiny}
\STATE $\mathbf{g}_t = \frac{1}{m} \nabla_{\param} \sum_{i=1}^m \loss(f(\x_i,\param_{t-1}\alert{+\mathbf{m}_{t-1}}), y_i)$  \begin{tiny}\texttt{Return parameter gradient}\end{tiny}
\STATE $\eta_t = $ SetScheduleMultiplier$(t)$ 
\STATE $\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + \eta_{t} \mathbf{g}_t $
\STATE $\param_i=\param_{t-1} - \mathbf{m}_{t}$
\UNTIL stopping criterion is met
\RETURN optimized parameters $\param_t$
\end{algorithmic}
\caption{SGD with Nesterov's momentum}
\label{alg:Nesterov}
\end{algorithm}
\end{frame}

% \begin{frame}{SGD with Nesterov's momentum \cite{Nesterov83}}
% \begin{algorithm}[H]
% \begin{algorithmic}[1]
% \STATE \textbf{given} initial learning rate $\epsilon \in \mathbb{R}$, dataset $\X$, momentum factor $\beta_1 \in \mathbb{R}$
% \STATE \textbf{initialize} time step $t=0$, parameter vector $\param_{t=0} \in \mathbb{R}^\nfeatures$, first momentum factor $\mathbf{m}_{t=0}=0 \in \mathbb{R}^\nfeatures$, schedule multiplier $\eta_{t=0} \in \mathbb{R}$
% \REPEAT
% \STATE t=t+1
% \STATE $\X_t=\texttt{SelectBatch} (\X)$ \begin{tiny}\texttt{Select a batch from data} \end{tiny}
% \STATE $\mathbf{g}_t = \nabla f_t(\X_t,\param_{t-1}\alert{+\mathbf{m}_{t-1}})$ \begin{tiny}\texttt{Return parameter gradient} \end{tiny}
% \STATE $\eta_t = $ SetScheduleMultiplier$(t)$ \begin{tiny}{Can be fixed,decay, warm restarts, ...}\end{tiny}
% \STATE $\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + \eta_{t} \eta  \mathbf{g}_t $
% \STATE $\param_i=\param_{t-1} - \mathbf{m}_{t}$
% \UNTIL stopping criterion is met
% \RETURN optimized parameters $\param_i$
% \end{algorithmic}
% \caption{pseudocode for stochastic gradient descent \alert{with Momentum} }
% \label{alg:seq}
% \end{algorithm}
% \end{frame}

\begin{frame}{Nesterov's Momentum}
\begin{figure}
\includegraphics[width=.8\columnwidth]{../graphics/Nesterov}
\end{figure}
\begin{itemize}
	\item The difference to standard momentum is where the gradient is evaluated. 
	\item This can lead to further acceleration. For the optimization of NN, this does not play a major role. 
\end{itemize}
\end{frame}


% \begin{frame}
% \begin{table}[htp]
% \caption{Summary: Some Optimizers I}
% \begin{center}
% \begin{tabular}{|c|c|} \hline
% Method & Update\\ \hline
% SGD  & $\param_{t+1}= \param_{t}-\eta\mathbf{g}(\param_{t})$ \\ \hline
% Momentum    & $\param_{t+1}= \param_{t}+\mathbf{m}_t$, $\mathbf{m}_{t}= \alert{\beta_1 \mathbf{m}_{t-1}} -\eta \mathbf{g}(\param_{t}) $ \\  \hline
% Nesterov Mom.  &  $\param_{t+1}= \param_{t}+\mathbf{m}_t$ , $\mathbf{m}_t= \beta_1 \mathbf{m}_{t-1} -\eta\mathbf{g}(\param_{t}+ \alert{\beta_1 \mathbf{m}_{t-1}}) $ \\ \hline
% \end{tabular}
% \end{center}
% \label{default}
% \end{table}%
% %where $G_{ii,t}$ is the sum of the squares of the gradients of $\param_i$ up to time step $i$ while $\epsilon$ is a smoothing term that avoids division by zero.
% %$\texttt{RMS}$ is the root mean squared error  (RMS)  of gradient for $\texttt{RMS}(g)_{t}$ or of parameter updates for $\texttt{RMS}(\partial \param)_{t}$ 
% \end{frame}


\begin{frame}{Motivation}
\begin{itemize}
	\item An additional difficulty: anisotropy.
	\item Some features evolve rapidly, others slowly. 
	\item For this reason, we might want to have different learning rates for different features.
\end{itemize}
\end{frame}

% \begin{frame}{Adagrad \cite{duchi2011adaptive}}
% \begin{itemize}
% 	\item We first calculate the accumulate squared gradient:
% 	\begin{equation*}
% 	r \leftarrow r + g \odot g
% 	\end{equation*}
% 	\item The update then becomes:
% 	\nabla \theta \leftarrow 

% 	\item The parameter update is defined as:
% 		\begin{equation}
% 			\param_{t+1}= \param_{t}- \frac{\eta}{\alert{\sqrt { G_{ii,t}+\epsilon  }}} \mathbf{g}(\param_{t})
% 		\end{equation}
% 	 	  where $G_{ii,t}$ is the sum of the squares of the gradients of $\param_i$ up to time step $t$ while $\epsilon$ is a smoothing term that avoids division by zero. We thus have
% 	 	  $G_{ii,t}=\sum_{j=1}^{t}  \mathbf{g}^2_{i}(\param_{j})$, where $\mathbf{g}_{i}$ denotes the i-th component of $\mathbf{g}.
% 	\item Each dimension is thus weighted by its gradient component: the learning rate is lower for features with a "high-gradient-past". 
% 	\item This allows slowly evolving features to get a higher learning rate. 
% %\alert{Note that only elements in the diagonal of $\mathbf{\Sigma}_{\param}^{-1}$ are estimated.}
% \end{frame}

\begin{frame}{Adagrad \cite{duchi2011adaptive}}
\begin{itemize}
	\item The parameter update is defined as:
		\begin{equation}
			\param_{t+1}= \param_{t}- \frac{\eta}{\alert{\sqrt { G_{ii,t}+\epsilon  }}} \mathbf{g}(\param_{t})
		\end{equation}
	 	  where $G_{ii,t}$ is the sum of the squares of the gradients of $\param_i$ up to time step $t$ while $\epsilon$ is a smoothing term that avoids division by zero. We thus have
	 	  $G_{ii,t}=\sum_{j=1}^{t}  \mathbf{g}^2_{i}(\param_{j})$, where $\mathbf{g}_{i}$ denotes the i-th component of $\mathbf{g}$.
	\item Each dimension is thus weighted by its gradient component: the learning rate is lower for features with a "high-gradient-past". 
	\item This allows slowly evolving features to get a higher learning rate. 
\end{itemize}
%\alert{Note that only elements in the diagonal of $\mathbf{\Sigma}_{\param}^{-1}$ are estimated.}
\end{frame}

\begin{frame}{RMSprop \cite{hinton2012neural}}
In RMSprop \cite{hinton2012neural} proposes a rule update model's parameter by
\begin{equation}
\param_{t+1}= \param_{t}- \frac{\eta}{\alert{\sqrt { \texttt{RMS}(G_{ii})_{t}+\epsilon  }}} \mathbf{g}(\param_{t})
\end{equation}
where  $\texttt{RMS}(G_{ii,t})$ is a momentum estimation of $\sqrt { G_{ii,t}+\epsilon}$, i.e., 
defining the sequence 
\begin{equation*}
E(\mathbf{g}^2)_{t}=\rho E(\mathbf{g}^2)_{t-1} + (1-\rho) \mathbf{g}^2_t,
\end{equation*}
and we define $\texttt{RMS}(G_{ii,t})=E(\mathbf{g}^2)_{t}+\epsilon$.

\end{frame}

\begin{frame}{RMSprop}
\begin{itemize}
\item RMSprop replaces the total sum (Adagrad) by an expontentially weighted moving average. 
\item This loss in memory is important in the presence of strong non-convexities: the landscape can change profoundly, and the extreme past is probably not useful in such a case. 
\end{itemize}
\end{frame}
	
% \begin{frame}{Adadelta \cite{zeiler2012adadelta}}
% However, the unit in the learning rate don't correspondent with unit in the denominator. Thus, Adadelta optimiser \cite{zeiler2012adadelta} update by:
% \begin{equation}
% \param_{t+1}= \param_{t}- \frac{\alert{ \texttt{RMS}(\partial\param)_{t-1}}}{\alert{\sqrt { \texttt{RMS}(G_{ii,t}+\epsilon})}} \mathbf{g}(\param_{t})
% \end{equation}
% here is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\param_i$ up to time step  $t$
% \end{frame}

% \begin{frame}
% \begin{table}[htp]
% \caption{Summary: Some Optimizers II}
% \begin{center}
% \begin{tabular}{|c|c|} \hline
% Method & Update\\ \hline
% SGD  & $\param_{t+1}= \param_{t}-\eta\mathbf{g}(\param_{t})$ \\ \hline
% Momentum    & $\param_{t+1}= \param_{t}+\mathbf{m}_t$, $\mathbf{m}_{t}= \alert{\beta_1 \mathbf{m}_{t-1}} -\eta \mathbf{g}(\param_{t}) $ \\  \hline
% Nesterov Mom.  &  $\param_{t+1}= \param_{t}+\mathbf{m}_t$ , $\mathbf{m}_t= \beta_1 \mathbf{m}_{t-1} -\eta\mathbf{g}(\param_{t}+ \alert{\beta_1 \mathbf{m}_{t-1}}) $\\ \hline
% Adagrad & $\param_{t+1}= \param_{t}- \frac{\eta}{\alert{\sqrt { G_{ii,t}+\epsilon  }}} \mathbf{g}(\param_{t})$  \\ \hline
% RMSprop & $\param_{t+1}= \param_{t}- \frac{\eta}{\alert{\sqrt { \texttt{RMS}(G_{ii,t})+\epsilon  }}} \mathbf{g}(\param_{t})$  \\ \hline
% Adadelta & $ \param_{t+1}= \param_{t}- \frac{\alert{ \texttt{RMS}(\partial\param)_{t-1}}}{\alert{\sqrt { \texttt{RMS}(G_{ii})_{t}+\epsilon  }}} \mathbf{g}(\param_{t})$ \\ \hline
% \end{tabular}
% \end{center}
% \label{default}
% \end{table}%
% %where $G_{ii,t}$ is the sum of the squares of the gradients of $\param_i$ up to time step $i$ while $\epsilon$ is a smoothing term that avoids division by zero.
% %$\texttt{RMS}$ is the root mean squared error  (RMS)  of gradient for $\texttt{RMS}(g)_{t}$ or of parameter updates for $\texttt{RMS}(\partial \param)_{t}$ 
% \end{frame}


\begin{frame}{ADAM \cite{kingma2014adam}}
\begin{itemize}
\item Adam ("Adaptive Moments") can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum.
\item Concretely, Adam first calculates a weighted sum of past gradient vectors, and then divides each entry of this vector by the sum of squares of its past (recent) values. 
\item Adam and RMSprop are the most popular algorithms for network optimization today. 
\end{itemize}
\end{frame}

%\begin{frame}
%\cite{reddi2018convergence}
%\end{frame}

%\begin{frame}{Momentum}
%The momentum algorithm accumulates the exponentially decaying moving average of past gradients (called as velocity) and uses it as the direction in which to take the next step. Momentum is given by mass times velocity, which is equal to velocity if we’re using unit mass. The momentum update is given by:
%\end{frame}

%\begin{frame}{Nesterov Momemtum}
%Thus, it might be better to compute the gradient from that point onward
%\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Difficulties in Deep Network Optimisation}

% \begin{frame}{Difficulties in Deep Network Optimisation}
% \begin{itemize}
% \item[A]{Local minima / Global minima}
% \item[B]{Saddle Point (Plateaus or Flat Regions)}
% \item[C]{Vanishing Gradient}
% \item[D]{Overfitting}
% \item[E]{Initialization issues}
% \end{itemize}
% \end{frame}


\begin{frame}{More challenges in optimization with SGD}
\begin{figure}[htb]
	\centering
	\includegraphics[width=.75\columnwidth]{../graphics/Global_local.pdf}
\end{figure}
\begin{itemize}
\item Local Minima (however, it is not sure how much they impact optimization in practice). 
\item Saddle Points, plateaus or flat regions.
\item Cliffs and exploding gradients: very large gradients also lead to large steps.
\item Poor correspondence between local and global structure (see Figure).
\item Strong anisotropy (ill conditioning of the Hessian matrix).
\item Vanishing gradients
\end{itemize}
\end{frame}



% \begin{frame}{Initialization Issues}
% \begin{enumerate}
% \item \textbf{Gaussian}: From Imagenet 2012, \cite{krizhevsky2012imagenet} recommends initialization with $\mathcal{N}(0,.01)$ and adding bias equal to one
% for some layers become very popular. It is not possible to train very deep network from scratch with it \cite{simonyan2014very}. The problem is caused by the
% activation (and/or) gradient magnitude in final layers \cite{he2016deep}.
% \item  \textbf{Glorot}:  \cite{glorot2010understanding}  proposed a formula for estimating the standard deviation on the basis of
% the number of input and output channels of the layers under assumption of no non-linearity between
% layers. Despite invalidity of the assumption, Glorot initialization works well.	
% \item \textbf{Orthogonal}: Saxe et al. \cite{saxe2013exact}  showed that orthonormal matrix initialization works much better
% for linear networks than Gaussian noise, which is only approximate orthogonal. It also work fornetworks with non-linearities.
% \item Recommended lecture: All you need is a good init  \cite{mishkin2015all}
% \end{enumerate}
% \end{frame}



\begin{frame}{Vanishing gradients}
\begin{itemize}
\item If the gradient of a node vanishes (becomes 0), the entry value is not updated. 
\item This means that there is no information that flows through the node backwards. 
\item In this case, the networks cannot learn anymore: the network becomes untrainable. 
\item This problem is particularly striking for very deep networks. 
\end{itemize}
\end{frame}


\begin{frame}{Fighting against Vanishing Gradient}
\begin{enumerate}
\item Use ReLu. They are much more robust to this problem, because a sigmoid maps a large input space to a $[0,1]$ output. If the input is large, the gradient becomes small. ReLus do not suffer from this problem. 
\item Use of residual networks.
\item Batch normalization.
\end{enumerate}
\end{frame}

\begin{frame}{Fighting against Vanishing Gradient: skip connections}
Skip connections or Shortcuts (Residual Networks): 
\begin{figure}
\includegraphics[width=.65 \columnwidth]{../graphics/shortcut}
\end{figure}
\begin{figure}
\includegraphics[width=.4\columnwidth]{../graphics/VGG56Loss}
\includegraphics[width=.45\columnwidth]{../graphics/Resnet56}
\caption{From \cite{li2017visualizing}}
\end{figure}
\end{frame}

% \begin{frame}{Fighting against Vanishing Gradient}
% \begin{enumerate}
% \item[4] Avoid "stuck states" induced by activation function: 
% \begin{figure}
% \includegraphics[width=.85 \columnwidth]{../graphics/nnet-error-functions2}
% \caption{Left: Three activation function Right: Derivative of activation function.}
% \end{figure}
% \end{enumerate}
% \end{frame}

% \begin{frame}{Fighting against Vanishing Gradient}
% \begin{enumerate}
% \item[5] Regularization: $L_2$ or $L_1$ norm applies "weight decay" in the cost function of the network. Note that for many activation function, when the activation value is small, that will be almost linear.
% \begin{figure}
% \includegraphics[width=.75 \columnwidth]{../graphics/KerasRegularizer}
% \end{figure}

% \end{enumerate}
% \end{frame}


\begin{frame}{Batch Normalization \cite{ioffe2015batch}  \cite{mishkin2015all}}
BatchNorm (BN) is a transformation applied before the activation in a given layer, i.e. if $x_i$ are the output of a layer for a mini-batch, the BN is defined by
\begin{eqnarray*}
\hat{x}_i = \frac{x_i-\mu_{B}}{\sqrt{\sigma^2_B}}, \quad \quad
BN_{\gamma,\beta}(x_i):= \gamma \hat{x}_i + \beta
\end{eqnarray*}
where $\mu_{B}$ and $\sigma^2_B$ are respectively the mini-batch mean and variance. $\gamma$ scale and $\beta$ shift a learned parameters.
\begin{itemize}
\item Moving values to zero (activation works better!) 
\item Large learning rates can scale the parameters which could amplify the gradients, thus leading to an explosion. In Batch normalization small changes in parameter to one layer do not get propagated to other layers. 
\item This makes it possible to use higher learning rates for the optimizers, which otherwise would not have been possible. 
\item It also makes gradient propagation in the network more stable.
\end{itemize}
\end{frame}


\begin{frame}{Batch Normalization}
\begin{figure}
\includegraphics[width=.75 \columnwidth]{../graphics/BatchNormalization}
\caption{Image from "Intuit and Implement: Batch Normalization"}
\end{figure}
\end{frame}
%\begin{itemize}
%\item[D]{Ill-conditioning of the Hessian Matrix}
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{How to fight against overfitting}


\begin{frame}{Supervised Learning (Machine Learning)}
\begin{itemize}
\item \textbf{Data}: $\nsize$ observations $(\x_i,y_i) \in \mathcal{X} \times \mathcal{Y}, i=1,\ldots,\nsize$, \alert{\textbf{i.i.d.}}
\item \textbf{Model}: $\texttt{Model}(\x):=\param^T\featmap (\x)$ of features  $\featmap (\x) \in \mathbb{R}^\nfeatures$ \alert{Prediction as linear mapping of features}
\item \textbf{Minimization of Regularized Empirical Risk}: We would like to find $\param^{*}$ the solution of:
\begin{eqnarray*}
\param^{*}:=\min_{\param \in \mathbb{R}^\nfeatures} \frac{1}{\nsize} \sum_{i=1}^{\nsize} \loss(y_i,\param^T\featmap (\x_i))&  + &\alpha \mathcal{R}(\param) \\
\texttt{Data fitting}& + &\texttt{regularizer}
\end{eqnarray*}
\end{itemize}
where $\loss(\cdot,\cdot)$ is called the  \emph{loss function}.
\end{frame}



\begin{frame}{Other loss functions, other models}
%\begin{center}\includegraphics[width=.2\columnwidth]{../graphics/Loss}\end{center}
\begin{enumerate}
\item Support Vector Machine (SVM): "Hinge" Loss 
\begin{equation}
\loss(y,\param^T\featmap(\x))=\max \{1-y\param^T\featmap(\x),0\}
\end{equation}
\item Logistic Regression: 
\begin{equation}
\loss(y,\param^T\featmap(\x))=\log (1+\exp (-y\param^T\featmap(\x)))
\end{equation}
\item Mean Squared Regression:
\begin{equation}
\loss(y,\param^T\featmap(\x))=\frac{1}{2}(y-\param^T\featmap(\x))^2
\end{equation}
\item Adaboost
\begin{equation}
\loss(y,\param^T\featmap(\x))=\exp^{ -(y-\param^T\featmap(\x))}
\end{equation}
\item Others ...
\end{enumerate}
\end{frame}

\begin{frame}{Minimizing Empirial Risk = Problems!}
\begin{itemize}
\item Empirical Risk: $\hat{f}(\param) :=\frac{1}{\nsize} \sum_{i=1}^{\nsize} \loss(y_i,\param^T\featmap (\x_i))$ \pause \\\alert{Loss in a training set}
\end{itemize}
\begin{figure}[htb]
\includegraphics[width=0.4\textwidth]{../graphics/polyfit_degree_11.png}
\caption{There are infinity minimizer of the empirical risk!}
\end{figure}
 \pause
\begin{itemize}
\item Expected Risk : $f(\param) := \mathbb{E}_{(\x,y)} \loss(y,\param^T\featmap (\x))$ \pause  \\\alert{Loss in a testing set}
\end{itemize}
There are infinity minimizers of the empirical risk, but most of them have a large expected risk (\alert{overfitting}).
\end{frame}


\begin{frame}{Bias/Variance Tradeoff}
Let $\hat{y}:=\texttt{Model}(\x)$ the prediction of a deterministic model evaluated at $\x$
\begin{eqnarray*}
\mathbb{E}_{(\x,y)}\left [ (y-\texttt{Model}(\x))^ 2\right ] = \\ Var\left[y \right] + Var \left[ \texttt{Model}(\x) \right] + (Bias [\texttt{Model}(\x)])^2
\end{eqnarray*}
\begin{figure}
\includegraphics[width=.45 \columnwidth]{../graphics/polyfit_degree_1}
\includegraphics[width=.45 \columnwidth]{../graphics/polyfit_degree_11}
\caption{Underfitting / Overfitting}
\end{figure}
Underfitting : Prediction with less variance but more bias
Overfitting : Prediction with more variance but less bias
\end{frame}

\begin{frame}{How to judge if a deep machine learning model is overfitting or not?}
\begin{columns}
\begin{column}{.5\textwidth}
\includegraphics[width=.95 \columnwidth]{../graphics/Overfitting}
\end{column}
\begin{column}{.5\textwidth}
\begin{itemize}
\item Training Set / Testing Set
\item Cross-Validation
\item $||\param||_p$ is large
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{1. Early Stopping / ReduceLROnPlateau / Learning Rate Scheduler}
\begin{figure}
\includegraphics[width=.75 \columnwidth]{../graphics/EarlyStopping}
\end{figure}
\end{frame}

\begin{frame}{1. Early Stopping / ReduceLROnPlateau / Learning Rate Scheduler}
\begin{figure}
\includegraphics[width=1 \columnwidth]{../graphics/CallBacksKeras}
\caption{Callbacks in Keras}
\end{figure}
\end{frame}

\begin{frame}{2. We need more data!}
\begin{figure}
\includegraphics[width=.45 \columnwidth]{../graphics/polyfit_degree_11}
\includegraphics[width=.45 \columnwidth]{../graphics/polyfit_degree_11_N60.png}
\caption{Same model complexity but more data}
\end{figure}
\begin{enumerate}
\item Additive Gaussian noise.
\item Data augmentation.
\item Adversarial Training (Not in this talk)
\end{enumerate}
\end{frame}

\begin{frame}{Expected Risk (again)}
The expected risk
\begin{equation*}
 \mathbb{E}_{(\x,y)}  \loss(y,\texttt{Model}) =  \int \loss(y,\texttt{Model} (\x)) dP(\x,y) := R(\texttt{Model} )
\end{equation*}
\pause
But the distribution P is \alert{unknown} in most practical situations. 
\pause
We usually have access to a set of training data $T=\{(\x_i,y_i) \}_{i=1}^{\nsize}$, where $(\x_i,y_i)\sim P$, for all $i=1,\ldots,\nsize$. Thus, we may approximate $P$ by the \emph{empirical distribution}:
\begin{equation*}
P_{\delta}(\x,y) = \frac{1}{\nsize}\sum_{i=1}^{\nsize}\delta(\x=\x_i,y=y_i)
\end{equation*}
\end{frame}

\begin{frame}{Empirical Risk (again)}
Using the empirical distribution $P_{\delta}$, we can now approximated the expected risk, by the called \emph{empirical risk}
\begin{equation}\label{RM}
R_{\delta}(\texttt{Model} ) = \int \loss(y,\texttt{Model} (\x)) dP_{\alert{\delta}}(\x,y) = \frac{1}{\nsize}\sum_{i=1}^{\nsize}\loss(\texttt{Model}(\x_i),y_i)
\end{equation} \\
Learning the function $f$ by minimizing \eqref{RM} is known as the Empirical Risk Minimization (ERM) principle \cite{vapnik98} (Vapnik, 1998). If the number of parameters are comparable to $\nsize$, one trivial way to minimize \eqref{RM} is to \alert{memorize} the whole set of training data (overfitting).
\end{frame}

\begin{frame}{Vicinal Risk Minimization (VRM)}
$P_{\delta}$ is only one of the possibility to approximate the true distribution $P$. \cite{chapelle2001vicinal} proposed to approximate $P$ by:
\begin{equation*}
P_{v}(\x,y) = \frac{1}{\nsize}\sum_{i=1}^{\nsize}v(\tilde{\x},\tilde{y}, |\x_i,y_i)
\end{equation*}
where $v$ is \emph{vicinity distribution} that measure the probability for a "virtual" pair $(\tilde{\x},\tilde{y})$ to be in the \emph{vicinity} of the training pair $(\x,y)$. \pause
\begin{enumerate}
\item[1]  Gaussian vicinities: $v(\tilde{\x},\tilde{y}, |\x_i,y_i)=\mathcal{N}(\tilde{\x}-\x,\sigma^2\mathbf{I})\delta(\tilde{y}=y)$
\end{enumerate}
\pause
\alert{which is equivalent to augmenting the training data with additive Gaussian noise}
\end{frame}

\begin{frame}{Keras Gaussian Layer}
\begin{figure}
\includegraphics[width=.95 \columnwidth]{../graphics/GaussianNoiseLayer}
\caption{Gaussian vicinities in Keras}
\end{figure}
\end{frame}

\begin{frame}{Why Data-augmentation?}
\begin{enumerate}
\item[2]  Data-augmentation based vicinities: $P_{agg}(\x,y) = \frac{1}{\nsize}\sum_{i=1}^{\nsize}\delta(\tilde{\x},y_i)$, where $\tilde{\x}$ is a random transformation applied $\x$
\end{enumerate}
\begin{figure}
\includegraphics[width=.99 \columnwidth]{../graphics/cat_data_augmentation}
\caption{Example of a set of image produce by random transformations (translations, rotations, zooming, ...)}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=.99 \columnwidth]{../graphics/DataGeneratorKeras}
\caption{Data Generator in Keras}
\end{figure}
\end{frame}

\begin{frame}{Regularization vs Overfitting}
\begin{enumerate}
\item "Weight Decay": Again?
\item Early Stopping
\item More data! : Data Augmentation
\item More data!  : Adversarial Examples
\item Summing-up: Dropout
\end{enumerate}
\end{frame}

\begin{frame}{How can we reduce the variance of ?}
\begin{itemize}
\item Remember: given $\nsize$ i.i.d. obsevations $\x_1,\x_2,\ldots,\x_\nsize$ each of them with variance equal to $\sigma^2$. 
\item What is the variance of $\bar{\x}=\frac{1}{\nsize} \sum_{i=1}^{\nsize} \x_i $? \pause $\frac{\sigma^2}{\nsize}$
\item \alert{Hint}: Train model on different training sets, and use the mean of predictions as final model of prediction.
\end{itemize}
\end{frame}

\begin{frame}{Dropout: \cite{srivastava2014dropout} }
\begin{figure}
\includegraphics[width=.45 \columnwidth]{../graphics/NetworkDropoutNO}
\includegraphics[width=.45 \columnwidth]{../graphics/NetworkDropoutYes}
\caption{Left: No Dropout Right: Dropout}
\end{figure}
\begin{itemize}
%\item Since dropout can be seen as a stochastic regularization technique
\item Avoid memorization!
\item Dropout forces to learn more robust features that are useful in conjunction with many different random subsets.
\item With $H$ hidden units, each of which can be dropped, we have $2^H$ possible models!
\end{itemize}
%Dropout: a simple way to prevent neural networks from overfitting. 2014
\end{frame}

\begin{frame}{Dropout}
\begin{figure}
\includegraphics[width=.85 \columnwidth]{../graphics/DropoutP}
\caption{Left: A unit at training time that is present with probability $p$ and is connected to units
in the next layer with weights $\mathbf{W}$. Right: At test time, the unit is always present and
the weights are multiplied by $p$. The output at test time is same as the expected output
at training time.}
\end{figure}
\end{frame}


\begin{frame}{Dropout}
\begin{figure}
\includegraphics[width=.99 \columnwidth]{../graphics/DropoutKeras}
\caption{Dropout in Keras. Layer with a dropout of $p=.5$}
\end{figure}
Practical Work 2:  Dropout on Fashion MNIST
\end{frame}
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\bibliography{slides_deep.bib}
\end{frame}

\end{document}
