\documentclass[handout,xcolor=pdftex,dvipsnames,table,mathserif]{beamer}
\input{../../setting.tex}
\usepackage{physics}
\usepackage{tikz}
\usetikzlibrary{fit,positioning}

%% \usepackage{animate}

\AtBeginSection[]{
  \begin{frame}{Contents}
  \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

\AtBeginSubsection[]{
  \begin{frame}{Contents}
  \tableofcontents[currentsection, subsectionstyle=show/shaded/hide]
  \end{frame}
}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{items}[square]

%% For image credits on image bottom right
\usepackage[absolute,overlay]{textpos}
\setbeamercolor{framesource}{fg=gray}
\setbeamerfont{framesource}{size=\tiny}
\newcommand{\source}[1]{\begin{textblock*}{4cm}(8.7cm,8.6cm)
    \begin{beamercolorbox}[ht=0.5cm,right]{framesource}
      \usebeamerfont{framesource}\usebeamercolor[fg]{framesource} Credits: {#1}
    \end{beamercolorbox}
\end{textblock*}}

\title{Introduction to Autoencoders, GANs and Adversarial Examples}
\author{Santiago VELASCO-FORERO \\ \href{http://cmm.ensmp.fr/~velasco/}{http://cmm.ensmp.fr/~velasco/}}
\date{MINES ParisTech\\
  PSL Research University\\
  Center for Mathematical Morphology
}
\titlegraphic{\includegraphics[height=1.7cm]{../graphics/logoemp}}

\useinnertheme{rounded}
\usecolortheme{rose}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{frame}
\titlepage
\end{frame}

\frame{
\frametitle{Contents}
\tableofcontents[]
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}%{Reminder of Machine Learning Problems}
%\begin{enumerate}
{Supervised Learning} Given a labeled dataset $(\X,\mathbf{Y})$, we would like to learn a mapping from data space to label space.
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 6mm, thick, draw =black!80, node distance = 26mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\x$] {\includegraphics[width=.15\columnwidth]{../graphics/Animal}};
  \node[main,fill = red!50] (xout) [right= of alpha,label=below: What animal is this?] {$\hat{\y} \in \mathbb{Z}$ };
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Classification};
  \end{tikzpicture}
\end{figure}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 6mm, thick, draw =black!80, node distance = 26mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\x$] {\includegraphics[width=.15\columnwidth]{../graphics/Winter}};
  \node[main,fill = red!50] (xout) [right= of alpha,label=below: How cold is it?] {$\hat{\y} \in \mathbb{R}$ };
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Regression};
  \end{tikzpicture}
\end{figure}
\end{frame}


\begin{frame}{Unsupervised Learning: Clustering}
Given an unlabeled dataset $(\X)$, we would like to learn:
\alert{How to group objects into similar categories?}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 9mm, thick, draw =black!80, node distance = 26mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\X$ ] {\includegraphics[width=.2\columnwidth]{../graphics/Shapes}};
  \node[main,fill = white!50] (xout) [right= of alpha,label=below: $\hat{\mathbf{Y}} \in \mathbb{Z}$] {\includegraphics[width=.2\columnwidth]{../graphics/ShapesClusters}};
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Clustering};
  \end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Unsupervised Learning: Anomaly detection}
Given an unlabeled dataset $(\X)$, we would like to learn:
\alert{How to identify observations differing significantly from the majority of data?}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 9mm, thick, draw =black!80, node distance = 30mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\X$ ] {\includegraphics[width=.2\columnwidth]{../graphics/Anomaly1}};
  \node[main,fill = white!50] (xout) [right= of alpha,label=below: $\hat{\mathbf{Y}} \in \{\mathbf{0,1}\}$] {\includegraphics[width=.2\columnwidth]{../graphics/Anomaly2}};
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Anomaly Detection};
  \end{tikzpicture}
\end{figure}
\end{frame}


\begin{frame}{Unsupervised learning: Generative Models}
Given an unlabeled dataset $(\X)$, we would like to learn:
\alert{How to generate a new observation from the same distribution (unknown) of dataset?}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 9mm, thick, draw =black!80, node distance = 30mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\X$ ] {\includegraphics[width=.2\columnwidth]{../graphics/Generative1}};
  \node[main,fill = white!50] (xout) [right= of alpha,label=below: $\mathbf{x}_{\texttt{new}}$] {\includegraphics[width=.2\columnwidth]{../graphics/Generative2}};
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Generative model};
  \end{tikzpicture}
\end{figure}
\end{frame}



\begin{frame}{Autoencoders}
Autoencoders are neural networks whose purpose is twofold:
\begin{enumerate}
\item To compress some input data by transforming it from the input domain to another space,
known as the \emph{latent space} (code).
\item To take this latent representation and transform it back to the original space, such that the output is \emph{similar} to the input.
\end{enumerate}
%\begin{figure}
%\includegraphics[height=2.4cm]{../graphics/SallowUnderComplete}
%\caption{Latent space}
%\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 36mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = blue!50] (alpha) [label=below: Original ] {$\x$};
  \node[main,fill = green!50] (z) [right=of alpha,label=below:Latent Space] { $z$};
  \node[main,fill = blue!50] (xout) [right= of z,label=below: Reconstructed ] {$\hat{\x}$ };
  \draw[thick,->] (alpha) -- (z) node[midway,sloped,above] {Encoder Network};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {Decoder Network};
  \end{tikzpicture}
\end{figure}

The loss function for a given input vector is usually the reconstruction error:
\[
\loss(\x) = \norm{ \x-\hat{\x}}
\]


\end{frame}


\section{Autoencoders}

\begin{frame}{Autoencoder}
\begin{itemize}
\item An \emph{autoencoder} is a neural network that is trained to attempt to copy its input to its output.
\item The network may be viewed as consisting of two parts: an
encoder function $z = f (\x)$ and a decoder that produces a reconstruction $r(\x) = g(f(\x))$.
\item The composition of $f$ and $g$ is called the \emph{reconstruction function}
\item If an autoencoder succeeds to learn $g(f (\x)) = \x$ everywhere, then it is not especially useful (overfitting).
\item The learning process consists in minimizing the loss function:
\begin{equation}
\loss ( \x , g(f(\x))),
\end{equation}
where $\loss$ is a loss function, such as mean squared error.
\end{itemize}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = blue!50] (alpha) [label=below: Original] {$\x$};
  \node[main,fill = green!50] (z) [right=of alpha,label=below:Latent Space] { $z$};
  \node[main,fill = blue!50] (xout) [right= of z,label=below: Reconstructed] {$\hat{\x}$ };
  \draw[thick,->] (alpha) -- (z) node[midway,sloped,above] {$f $};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$g$};
  \end{tikzpicture}
\end{figure}
\end{frame}


\begin{frame}{Over/Under complete autoencoders}

\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 6mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = blue!50] (alpha) [label=below: $\mathbb{R}^\nfeatures$ ] {$\x$};
  \node[main,fill = green!50, scale=2] (z) [right=of alpha,label=below: $\mathbb{R}^K$] { $z$};
  \node[main,fill = blue!50] (xout) [right= of z,label=below: $\mathbb{R}^\nfeatures$] {$\hat{\x}$ };
  \draw[thick,->] (alpha) -- (z) node[midway,sloped,above] {$f$};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$g$};
  \end{tikzpicture}
  \caption{$\nfeatures< K$: Overcomplete AE}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 6mm]
\tikzstyle{connect}=[-latex, thick]
\node[main, fill = blue!50] (alpha) [label=below: $\mathbb{R}^\nfeatures$ ] {$\x$};
  \node[main,fill = green!50, scale=0.5] (z) [right=of alpha,label=below: $\mathbb{R}^K$] { $z$};
  \node[main,fill = blue!50] (xout) [right= of z,label=below: $\mathbb{R}^\nfeatures$] {$\hat{\x}$ };
  \draw[thick,->] (alpha) -- (z) node[midway,sloped,above] {$f$};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$g$};
  \end{tikzpicture}
  \caption{$K < \nfeatures$: Undercomplete AE}
\end{figure}
\end{frame}
%\begin{figure}
%\includegraphics[height=2.4cm]{../graphics/SallowUnderComplete}
%\includegraphics[height=2.4cm]{../graphics/DeepUnderComplete}\\
%\caption{(a)Sallow Under Complete (b)Deep Under Complete}
%\end{figure}
%\begin{figure}
%\includegraphics[height=2.4cm]{../graphics/SallowOverComplete}
%\includegraphics[height=2.4cm]{../graphics/DeepOverComplete}
%\caption{ (c)Sallow Over Complete (d)Deep Over Complete}
%\end{figure}


\begin{frame}{Dimensional reduction methods}
When the encoder and decoder are linear and $\loss$ is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA (principal component analysis).
%% \begin{figure}
%% \includegraphics[width=.9\columnwidth]{../graphics/DimensionalityReduction}
%% \end{figure}
\end{frame}


\begin{frame}{Latent space intuition}
\begin{figure}
\includegraphics[width=\textwidth]{../graphics/LatentSpaceIntuition0}
\source{https://www.jeremyjordan.me/variational-autoencoders/}
\end{figure}
\end{frame}



%% \begin{frame}{Autoencoder vs. general data compression methods}
%% \begin{itemize}
%% \item Autoencoders are data-dependent
%% \item MP3 or JPEG compression algorithm make general assumptions about sounds/images, but not about
%% specific types of sounds/images.
%% %\item Autoencoders are lossy.
%% \item Autoencoders are learnt for a specific application
%% \end{itemize}
%% \end{frame}




%\begin{frame}{Motivation: Nonlinear dimensionality reduction}
%\begin{figure}
%\includegraphics[width=.8\columnwidth]{../graphics/ManifoldLearning}
%\caption{TODO: Manifold Learning}
%\end{figure}
%\end{frame}

\subsection{Type of Autoencoder}
\begin{frame}{Type of Autoencoders:}
\begin{enumerate}
\item Vanilla autoenconder
\item Regularized autoencoder (Sparse)
\item Denoising autoenconder
\item Contractive autoenconder
\item Variational autoenconder
%\item Stacked Autoencoders
\end{enumerate}
\end{frame}

\begin{frame}{Vanilla (Standard) Autoencoder}
\begin{figure}
\includegraphics[width=\columnwidth]{../graphics/StandardAutoencoder}
%\caption{Vanilla Autoencoder.  Poor initialization can lead to local minima.}
\end{figure}
%% \begin{itemize}
%% \item \cite{rumelhart1986learning}: Random Initialization and gradient descent shows bad performance.
%% \item \cite{bengio2007greedy} \cite{vincent2010stacked}: Stacking Autoencoders and tune with gradient descent shows good performance.
%% \end{itemize}
\end{frame}

%\begin{frame}{Autoencoder}
%\begin{figure}
%\begin{tikzpicture}
%\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 6mm]
%\tikzstyle{connect}=[-latex, thick]
%  \node[circle, fill = blue!50] (Data) [label=below: Real Data ] {$\x$};
%  \node[main,fill = green!50, scale=1] (z) [right=of Data,label=below: $\mathbb{R}^K$] { $z$};
%  \node[circle,fill = blue!50] (xout) [right= of z,label=below: Real Data ] {$\hat{\x}$ };
%  \draw[thick,->] (Data) -- (z) node[midway,sloped,above] {$f$};
%  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$g$};
%  \end{tikzpicture}
%\end{figure}
%Loss function in computing by assuming the correct prediction is the input. i.e,  $\loss(\hat{y},y)=\loss (f(g(\x)), \x)$
%  \end{frame}


\begin{frame}{Regularized Autoencoders}
A regularized autoencoder is simply an autoencoder whose training criterion involves a
regularity penalty $\Omega(f)$ on the code layer $f$, in addition to the reconstruction error:
\begin{equation}
\loss ( \x , g(f(\x)))+\Omega(f)
\end{equation}
\begin{itemize}
\item $L_1$: Cost function = Loss Function $+ \frac{\lambda}{2 m } \sum ||w||$
\item $L_2$:  Cost function = Loss Function $+ \frac{\lambda}{2 m } \sum ||w||^2$
\end{itemize}
\begin{figure}
\includegraphics[width=\columnwidth]{../graphics/regularizers}
\caption{Regularizers in Keras. Note: Kernel and bias regularizer are not the same.}
\end{figure}
\end{frame}

\begin{frame}{Sparse Autoencoders}
Regularization of the representation learned by the Auto-Encoders.
\begin{itemize}
\item Enforcing most code coefficients to be close to 0 (to be inactive).
\item Capturing a more robust representation of the manifold structure.
\end{itemize}
Common implementation
\begin{itemize}
\item Adding a sparsity regularizer loss to the autoencoder loss function.
\item Various sparsity regularizers.
%% \item Other existing methods:
\end{itemize}
\end{frame}


%\begin{frame}{Contractive Autoencoders}
%The \emph{contractive autoencoder} introduces a regularizer on the code $h = f(\X)$, encouraging the derivatives of $f$ to be as small as possible:
%\begin{equation}
%\Omega(h)=\lambda \left \lVert \frac{\partial f(\X)}{\partial \X} \right \lVert ^2_{F}
%\end{equation}
%%TODO
%\begin{figure}
%\includegraphics[width=.5\columnwidth]{../graphics/Contractive}
%\end{figure}
%\end{frame}

\begin{frame}{Denoising Autoencoders (DAE)\cite{vincent2008extracting}}
\begin{equation}
\loss ( \X , g(f(\tilde{\X}))),
\end{equation}
where $\tilde{\X}$ is a corrupted copy of $\X$ by some form of noise.

An over-complete autoencoder with high capacity can end up learning an
identity function where input=output. Add noise to avoid overfitting.

\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 6mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = blue!50] (alpha) [label=below: $\mathbb{R}^\nfeatures$ ] {$\x$};
  \node[main, fill = yellow!50] (Noise) [right=of alpha,label=below: Noise ] {$\epsilon$};
  \node[main,fill = green!50, scale=1] (z) [right=of Noise,label=below: $\mathbb{R}^K$] { $z$};
  \node[circle,fill = blue!50] (xout) [right= of z,label=below: $\mathbb{R}^\nfeatures$] {$\hat{\x}$ };
  \draw[thick,->] (alpha) -- (Noise) node[midway,sloped,above] {$+$};
   \draw[thick,->] (Noise) -- (z) node[midway,sloped,above] {$f$};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$g$};
  \end{tikzpicture}
\end{figure}
\end{frame}


%% \begin{frame}{Interpretation: Manifold Learning}
%% \begin{figure}
%% \includegraphics[width=.5\columnwidth]{../graphics/Manifold}
%% \end{figure}
%% \begin{itemize}
%% \item training data lies nearby a low-dimensional manifold.
%% \item a corrupted example is obtained by applying a perturbation of original example
%% \item The model should learn to \emph{project them back} to the manifold
%% \end{itemize}
%% \end{frame}


\begin{frame}{Modern Autoencoder}
\begin{itemize}
\item Modern autoencoders have generalized the idea of an encoder and a decoder
beyond deterministic functions to stochastic mappings
$p_{\texttt{encoder}}(z|\x)$ and $p_{\texttt{decoder}}(\x|z)$.
\end{itemize}
\end{frame}


\subsection{Variational Autoencoder}

\begin{frame}{Variational Autoencoder}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = blue!50] (alpha) [label=below:Original] {$\x$};
  \node[main,fill = green!50] (z) [right=of alpha,label=below: Latent] { $z$};
  \node[circle,fill = blue!50] (xout) [right=of z,label=below: Decoding] {$\x$ };
  \draw[thick,->] (alpha) -- (z) node[midway,sloped,above] {$p_{\theta}(z|\x)$};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$p_{\theta}(\x|z)$};
  \end{tikzpicture}
\end{figure}
\begin{itemize}
\item Training via maximum likelihood of $p(\x)$
\item Intractability: the true posterior density $p_{\theta}(z|\x)$ can be calculated %= p_{\theta}(\x|z)p_{\theta}(z)/p_{\theta}(\x)$ is intractable.
\item Solutions: a) MCMC (too costly) b) Approximate $p(z|\x)$ by means of $q(z|\x)=\mathcal{N}(z; \mu_{z}(x),\sigma_{z}(x))$
\end{itemize}
\end{frame}


\begin{frame}{Variational Autoencoder}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = blue!50] (alpha) [label=below:Original] {$\x$};
  \node[main,fill = yellow!50] (zsampling) [right=of alpha,label=below:Gaussian Modeling] { $\mu_{z}(\x),\sigma_{z}(\x)$};
  \node[main,fill = green!50] (z) [right=of zsampling,label=below:Latent] { $z$};
  \node[circle,fill = blue!50] (xout) [right= of z,label=below: Decoding] {$\x$ };
  \draw[thick,->] (alpha) -- (zsampling) node[midway,sloped,above] {$q_{\theta}(z|\x)$};
  \draw[thick,->,dashed] (zsampling) -- (z) node[midway,sloped,above] {Sampling};
  \draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$p_{\theta}(\x|z)$};
  \end{tikzpicture}
\end{figure}
\begin{itemize}
\item Gaussian modeling %(diagonal covariance matrix to avoid problems in high dimensionality)
\item Training via maximum likelihood of $p(\x)$
\item Learning the parameters $\theta$ via backpropagation?
%\item Loss function?
\end{itemize}
\end{frame}

\begin{frame}{Latent space intuition (variational case)}
\begin{figure}
\includegraphics[width=\columnwidth]{../graphics/LatentSpaceIntuition}
\source{https://www.jeremyjordan.me/variational-autoencoders/}
\end{figure}
\end{frame}

%% \begin{frame}{Training via maximum likelihood}
%% Assume we would like to compute the likelihood of an image $\x$ from the training set:
%% \begin{eqnarray*}
%% \mathcal{L}(\x) &=&\log(p(\x)) \\
%% &=& \sum_{z} q(z|\x) \log(p(\x)) \\
%% &=& \sum_{z} q(z|\x) \log(\frac{p(z,\x)}{p(z | \x)}) \\
%% &=& \sum_{z} q(z|\x) \log(\frac{p(z,\x) q(z|\x)}{q(z|\x) p(z | \x)}) \\
%% &=&  \sum_{z} q(z|\x) \log(\frac{p(z,\x) }{q(z|\x)}) + \sum_{z} q(z|\x) \log(\frac{q(z|\x) }{p(z|\x)}) \\
%% &=& \sum_{z} q(z|\x) \log(\frac{p(z,\x) }{q(z|\x)})+ \underbrace{D_{KL}(q(z|\x),p(z|\x))}_{\text{how good is the approximation.}}\\
%% &=& \mathcal{L}^{lvb}(\x)+D_{KL}(q(z|\x),p(z|\x)) \\
%% &\geq& \mathcal{L}^{lvb}(\x)
%% \end{eqnarray*}
%% \end{frame}


%% \begin{frame}
%% \begin{eqnarray*}
%%  \mathcal{L}(\x) \geq \mathcal{L}^{lvb}(\x) &=& \sum_{z} q(z|\x) \log(\frac{p(z,\x) }{q(z|\x)}) \\
%%  &=& \sum_{z} q(z|\x) \log(\frac{p(\x | z) p(z)}{q(z|\x)}) \\
%%  &=& \sum_{z} q(z|\x) \log(\frac{p(\x | z)}{q(z|\x)}) +  \sum_{z} q(z|\x) \log(\frac{p(z)}{q(z|\x)}) \\
%%  &=& \EV_{q(z|\x)} \log(p(\x | z)) -D_{KL}(q(z|\x),p(z))\\
%%   &=& \underbrace{\EV_{q(z|\x)} \log(p(\x | z))}_{\texttt{Expected Reconstruction}} - \underbrace{D_{KL}(q(z|\x),p(z))}_{\texttt{Regularization $\mathcal{N}(0,1)$}}\\
%% \end{eqnarray*}
%% \begin{itemize}
%% \item First term implies the use of many realization of sampling process (in practice we only have a few of samples per training example!)
%% \item Second term is simply a formula for diagonal multivariate Gaussian distribution.
%% \end{itemize}
%% \end{frame}

\begin{frame}{Reparametrization trick}
Backpropagation is not possible through random sampling!
\begin{figure}
\begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 4mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = gray!50] (x) [] {$\x$};
   \node[main, fill = gray!50] (theta) [above=of x] {$\theta=[\mu,\Sigma]$};
   \node[main, fill = yellow!50] (z) [right=of theta] {$z \sim q(z|\theta,\x)$};
   \node[main, fill = gray!50] (f) [right=of z] {$f$};
  %\node[main,fill = yellow!50] (zsampling) [right=of alpha,label=below:Gaussian Modeling] { $\mu_{z}(\x),\sigma_{z}(\x)$};
  %\node[main,fill = green!50] (z) [right=of zsampling,label=below:Latent] { $z$};
  %\node[main,fill = blue!50] (xout) [right= of z,label=below: Decoding] {$\x$ };
  \draw[thick,->] (x) -- (z) node[midway,sloped,above] {};
  \draw[thick,->] (theta) -- (z) node[midway,sloped,above] {};
  \draw[thick,->] (z) -- (f) node[midway,sloped,above] {};
  %\draw[thick,->,dashed] (zsampling) -- (z) node[midway,sloped,above] {Sampling};
  %\draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$p_{\theta}(\x|z)$};
  \end{tikzpicture}
  \caption{Original Formulation}
\end{figure}
\pause
\begin{figure}
\begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 4mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = gray!50] (x) [] {$\x$};
   \node[main, fill = gray!50] (theta) [above=of x] {$\theta=[\mu,\Sigma]$};
   \node[main, fill = yellow!50] (epsilon) [above=of theta] {$\epsilon \sim p(\epsilon)$};
   \node[main, fill = gray!50] (z) [right=of theta] {$z = g(\theta,\x,\epsilon)$};
   \node[main, fill = gray!50] (f) [right=of z] {$f$};
  %\node[main,fill = yellow!50] (zsampling) [right=of alpha,label=below:Gaussian Modeling] { $\mu_{z}(\x),\sigma_{z}(\x)$};
  %\node[main,fill = green!50] (z) [right=of zsampling,label=below:Latent] { $z$};
  %\node[main,fill = blue!50] (xout) [right= of z,label=below: Decoding] {$\x$ };
  \draw[thick,->] (x) -- (z) node[midway,sloped,above] {};
  \draw[thick,->] (theta) -- (z) node[midway,sloped,above] {};
   \draw[thick,->] (epsilon) -- (z) node[midway,sloped,above] {};
  \draw[thick,->] (z) -- (f) node[midway,sloped,above] {};
  % \draw[thick,->, draw=red!50] (z) -- (theta) node[midway,sloped,above] {$\frac{\partial \mu }{\partial z}$};
  %\draw[thick,->,dashed] (zsampling) -- (z) node[midway,sloped,above] {Sampling};
  %\draw[thick,->] (z) -- (xout) node[midway,sloped,above] {$p_{\theta}(\x|z)$};
  \end{tikzpicture}
  \caption{Reparametrization trick. Backpropagation}
\end{figure}
\end{frame}




%\section{Applications for Autoencoders}

\begin{frame}{Applications of autoencoders}
\begin{itemize}
\item Data denosing
\item Dimensionality Reduction
\item Database understanding
\item Generative models
\end{itemize}
\end{frame}


\begin{frame}{Dimensionality Reduction by AE}
\begin{figure}
\includegraphics[width=.75\columnwidth]{../graphics/VAEMNIST}
\caption{\textbf{Left}: AE, \textbf{Center}: Only KL in VAC, \textbf{Right}: VAE}
\end{figure}
\source{https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf}
\end{frame}

\begin{frame}{Example: latent space for MNIST}
\begin{figure}
\includegraphics[height=0.9\textheight]{../graphics/LatentMNIST}
\end{figure}
\end{frame}

%\begin{frame}{Galaxy Embedding}
%\begin{figure}
%\includegraphics[width=.5\columnwidth]{../graphics/GalaxyEmbedding}
%\caption{Jeffrey Regier et al, A deep generative model for astronomical images of galaxies}
%\end{figure}
%\end{frame}





%\begin{frame}{Applications for Autoencoders}
%\begin{enumerate}
%\item Information retrieval using deep auto-encoders (semantic hashing)
%\begin{figure}
%\includegraphics[width=.75\columnwidth]{../graphics/LatentMNIST}
%\end{figure}
%\end{enumerate}
%\end{frame}

%\begin{frame}{KL-divergence between two multivariate Gaussian distributions}
%\begin{eqnarray*}
%KL(\mathcal{N}(\mu_0,\Sigma_0)) || \mathcal{N}(\mu_1,\Sigma_1))=\\
% \frac{1}{2} \left ( \tr (\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^T \Sigma_1^{-1}(\mu_1-\mu_0)-k+\log \left ( \frac{\det \Sigma_1}{\det \Sigma_0}\right )\right )
%\end{eqnarray*}
%where $k$ is the dimensionality of the distributions.
%\end{frame}


\section{Generative Adversarial Networks}

\begin{frame}{Unsupervised learning: Generative Models}
Given an unlabeled dataset $(\X)$, we would like to learn:
\alert{How to generate a new observations from the same distribution (unknown) of dataset?}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 9mm, thick, draw =black!80, node distance = 30mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (alpha) [label=below: $\X$ ] {\includegraphics[width=.2\columnwidth]{../graphics/Generative1}};
  \node[main,fill = white!50] (xout) [right= of alpha,label=below: $\mathbf{x}_{\texttt{new}}$] {\includegraphics[width=.2\columnwidth]{../graphics/Generative2}};
  \draw[thick,->] (alpha) -- (xout) node[midway,sloped,above] {Generative model};
  \end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = yellow!50] (Noise) [label=below: Noise ] {$\epsilon$};
   \node[main,fill = green!50, scale=1] (Generator) [right=of Noise, label=below: $G(z)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Fake data] { $\hat{\x}$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake,label=below: $\mathbb{R}^\nfeatures$ ] {Real data $\x$};
   \node[main, fill = red!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{real}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$z$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {$P_z(z)$};
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
\end{figure}
%\pause
%The goal is to optimize the follow expression, $\min_{G} \max _{D} V(D,G)$.
%for a given loss function $\loss(\cdot,\cdot)$.\\
%\alert{We are looking the best generator for the best discriminator}.
\end{frame}

\begin{frame}{Generative Adversarial Networks (GANs)}
We require that the discriminator $D$ recognizes examples from the $P_{\texttt{real}}(\x)$ distribution,
\begin{equation*}
\mathbb{E}_{\x \sim P_{\texttt{real}}(\x)}[\log D(\x)]  \quad \quad \alert{\texttt{Decision over Real Data}}
\end{equation*} where $\mathbb{E}$ denotes the expectation. This term comes from the ``1" class  of the log-loss function. \\
Additionally, we would like to tricking the discriminator via a good generator $G$. Thus, the term comes from  ``0" class of the log-loss function:
\begin{equation*}
\mathbb{E}_{z \sim P_{z}(z)}[ \log (1-D(G(z)))]  \quad \quad  \alert{\texttt{Decision over Fake Data}}
\end{equation*}
\end{frame}

\begin{frame}{Generative Adversarial Networks (GANs)}
Questions: Optimal Discriminator? Optimal Generator?
We define:
\begin{equation*}
V(G,D):= \mathbb{E}_{\x \sim P_{\texttt{real}}(\x)}[\log D(\x)] + \mathbb{E}_{z \sim P_{z}(z)}[ \log (1-D(G(z)))]
\end{equation*}
What should be an optimal discriminator? \pause
\begin{equation*}
D^* = \argmax_{D} V(G,D)
\end{equation*}
Now, given this optimal discriminator $D^*$, what should be an optimal generator? \pause
\begin{equation*}
G^* = \argmin_{G} V(G,D^*)
\end{equation*}
This is called the \emph{minimax formulation}, since the generator and discriminator are playing a \emph{zero-sum game} against each other:
\begin{equation}
\min_{G} \max _{D} V(D,G)
\end{equation}
\end{frame}

\begin{frame}{Generative Adversarial Networks (GANs)}
\begin{eqnarray*}
&& \min_{G} \max _{D} V(D,G) := \\
&= &\mathbb{E}_{\x \sim P_{\texttt{real}}(\x)}[\log D(\x)]  + \mathbb{E}_{z \sim P_{z}(z)}[ \log (1-D(G(z)))]  \\
&&\texttt{by Radon-Nikodym Theorem} \\
&=& \mathbb{E}_{\x \sim P_{\texttt{real}}(\x)}[\log D(\x)]  + \mathbb{E}_{x \sim P_{\alert{{g}(\x)}}}[ \log (1-\alert{D(x)})]
\end{eqnarray*}
An optimal generator $G^{*}$ should give $P_{g(\x)}=P_{\texttt{real}}(\x)$. \\
Now, given a generator $G$, we can compute the optimal discriminator by:
\begin{eqnarray*}
\frac{\partial V(G,D)(\x)}{\partial D(\x)} &=& P_{\texttt{real}}(\x)\frac{1}{D(\x)} + P_{g}(\x)\frac{1}{1-D(\x)} \\
&=& \frac{P_{\texttt{real}}(\x)-(P_{\texttt{real}}(\x)+P_{\texttt{g}}(\x)) D(\x)}{D(\x)(1-D(\x))}
\end{eqnarray*}
Setting $\frac{\partial V(G,D)(\x)}{\partial D(\x)}=0$, we obtain $D^{*}(\x)=\frac{P_{\texttt{real}}(\x)}{P_{\texttt{real}(\x)+P_{g}(\x)}}$
\end{frame}

\begin{frame}{Global Optimal on Generative Adversarial Networks (GANs)}
\begin{eqnarray*}
V(G,D)= \mathbb{E}_{\x \sim P_{\texttt{real}}(\x)}[\log D(\x)]  + \mathbb{E}_{x \sim P_{{g}(\x)}}[ \log (1-D(\x))] \\
= \int_{\x} P_{\texttt{real}}(\x) \log D(\x) + P_{g(\x)}  \log (1-D(\x))  d\x
\end{eqnarray*}
We are interested in compute
$V(G^{*},D^{*})= \pause \alert{2 \log (1/2)}$
\end{frame}


\begin{frame}{Training GANs: Training the Discriminator:}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = white!50] (Noise) [label=below: Noise ] {$\epsilon$};
   \node[main,fill = white!50, scale=1] (Generator) [right=of Noise, label=below: $G(z)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Fake data] { $\hat{\x}$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake,label=below: $\mathbb{R}^\nfeatures$ ] {Real data $\x$};
   \node[main, fill = red!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{real}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$z$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {$P_z(z)$};
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
  \caption{Only Discriminator should be updated}
\end{figure}
\end{frame}

\begin{frame}{Training GANs: Training the Generator}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = yellow!50] (Noise) [label=below: Noise ] {$\epsilon$};
   \node[main,fill = green!50, scale=1] (Generator) [right=of Noise, label=below: $G(z)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Fake data] { $\hat{\x}$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake,label=below: $\mathbb{R}^\nfeatures$ ] {Real data $\x$};
   \node[main, fill = white!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{real}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$z$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {$P_z(z)$};
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
  \caption{Discriminator should be set to "non-trainable" weights}

\end{figure}
\end{frame}


\begin{frame}{How to Train a GAN?}
\alert{Bad news: GANs are hard to train}.
\textbf{ Tips and tricks to make GANs work}
\url{https://github.com/soumith/ganhacks}
\begin{itemize}
\item Use others loss functions.
\item Avoid Sparse Gradients: ReLU, MaxPool
\item Use Deep Convolutional Generative Adversarial Networks when you can. It works!
\item Use SGD for discriminator and ADAM for generator
\item Use Dropouts in G in both train and test phase
\item ...
\end{itemize}
\alert{``In theory, there is no difference between theory and practice. In practice, there is."}
\end{frame}

\begin{frame}{High-resolution GANs}
Progressive Growing of GANs for Improved Quality, Stability, and Variation \cite{karras2017progressive}
\url{https://www.youtube.com/watch?v=G06dEcZ-QTg}
\end{frame}

\begin{frame}{Applications of GANs: Earn money!}\pause
23-25 Oct, 2018:  Auction house "Christies" sold a portrait for 432,000 dollars that had been generated by a GANs
 \begin{figure}
\includegraphics[width=.95\columnwidth]{../graphics/Christie}
\end{figure}
\end{frame}

\begin{frame}{GANs to improve simulations}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = yellow!50] (Noise) [label=below: Simulation ] {$\mathbf{z}_{sim}$};
   \node[main,fill = green!50, scale=1] (Generator) [right=of Noise, label=below: $G(z)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Improved data] { $\hat{\x}$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake,label=below: $\mathbb{R}^\nfeatures$ ] {Real data $\x$};
   \node[main, fill = red!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{real}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$z$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {$P_z(z)$};
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
  \caption{In this case, \texttt{Generator} is called \texttt{Refiner}}
\end{figure}
\end{frame}

\begin{frame}{Application: Refine simulations}
 \begin{figure}
\includegraphics[width=.95\columnwidth]{../graphics/Refiner}
\caption{Learning from Simulated and Unsupervised Images through Adversarial Training \cite{shrivastava2017learning}}
\end{figure}
\end{frame}


\begin{frame}{Conditional Generative Adversarial Networks (cGANs)}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[main, fill = yellow!50] (Noise) [label=below: Noise ] {$\epsilon$};
  \node[main, fill = blue!50] (Label) [below=of Noise, label=below: Label ] {$y$};
   \node[main,fill = green!50, scale=1] (Generator) [right=of Noise, label=below: $G(z/y)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Fake data] { $\hat{\x}$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake,label=below: $\mathbb{R}^\nfeatures$ ] {Real data $\x$};
   \node[main, fill = red!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{real}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$z$};
   \draw[thick,->] (Label) -- (Generator) node[midway,sloped,above] {$y$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {};%$P_{z/y}(z/y)$
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Example: cGANs}
\begin{figure}
\includegraphics[width=.75\columnwidth]{../graphics/FACEAGING}
\caption{Face aging with conditional generative adversarial networks} \cite{antipov2017face}
\end{figure}
\end{frame}

\begin{frame}{Pix2Pix transformation}
\begin{figure}
\includegraphics[width=.75\columnwidth]{../graphics/PixToPix}
\caption{Example of Pix2Pix, a.k.a. image-to-image translation, \cite{isola2017image}}
\end{figure}
\end{frame}

\begin{frame}{GANs for Pix2Pix}
\begin{figure}
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, minimum size = 10mm, thick, draw =black!80, node distance = 9mm]
\tikzstyle{connect}=[-latex, thick]
  \node[circle, fill = blue!50] (Noise) {$\mathbf{y}_i$};
   \node[main,fill = green!50, scale=1] (Generator) [right=of Noise, label=below: $G(y)$] { \texttt{Generator}};
  \node[main,fill = green!50] (Fake) [right= of Generator,label=below: Translated data] { $\hat{\mathbf{y}}_i$ };
    \node[circle, fill = blue!50] (Data) [above=of Fake ] {$\x_i$};
   \node[main, fill = red!50] (Discriminator) [right=of Fake,label=below: $D(\x)$ ] {Discriminator};
  \node(Output) [text width=1cm] [right=of Discriminator]{1 real 0 fake};
  \draw[thick,->] (Data) -- (Discriminator) node[midway,sloped,above] {$P_{\texttt{1}}(\x)$};
   \draw[thick,->] (Noise) -- (Generator) node[midway,sloped,above] {$y$};
  \draw[thick,->] (Generator) -- (Fake) node[midway,sloped,above] {};
  \draw[thick,->] (Fake) -- (Discriminator) node[midway,sloped,above] {$$};
  \draw[thick,->] (Discriminator) -- (Output) node[midway,sloped,above] {$$};
  \end{tikzpicture}
  \caption{Training data are pairs of image: $(\x_i,\mathbf{y}_i)$ for $i \in 1,\ldots,\nsize$. In this case, \texttt{Generator} is called \texttt{Translator}}
\end{figure}
\end{frame}

\begin{frame}{Target Domain Transformation}
\begin{figure}
\includegraphics[width=.75\columnwidth]{../graphics/PairUnPair}
\caption{ \emph{Left}:  Image to Image translation, \emph{Right}: Source to Target domain transform}
\end{figure}
\end{frame}

\begin{frame}{Cycle-Consistent Adversarial Networks}
\begin{figure}
\includegraphics[width=.75\columnwidth]{../graphics/DomainTransformation}
\caption{Unpaired image-to-image translation \cite{zhu2017unpaired}}
\end{figure}
%https://github.com/LynnHo/CycleGAN-Tensorflow-PyTorch
\end{frame}

\begin{frame}{Cycle-Consistent Adversarial Networks}
We have to train two GANs for Pix2Pix transform:
\begin{itemize}
\item First: GANs to translate data from domain one $\X_1$ to domain two represented by $\X_2$, denoted by $V(G_1,D_1,\X_1,\X_2)$
\item Second: GANs to translate data from domain one $\X_2$ to domain two represented by $\X_1$, denoted by $V(G_2,D_2,\X_2,\X_1)$
\pause
\item Include: a Cycle consistency loss $V_{cyc}$ to force that the composition of two \texttt{Generators} to reconstruct input image, i.e $G_1(G_2(\X_2))\approx\X_2$ and  $G_2(G_1(\X_1))\approx\X_1$
\end{itemize}
In general the full objective is:
\begin{eqnarray*}
\loss(G_1,G_2,D_1,D_2)=V(G_1,D_1,\X_1,\X_2)+V(G_2,D_2,\X_2,\X_1)\\
+\lambda V_{cyc}(G_1,G_2)
\end{eqnarray*}
\end{frame}

\section{Adversarial Examples}
\begin{frame}{Adversarial Examples \cite{Adversarial}}
%For adversaries, an ideal adversarial attack is to construct a perturbed input $\x^{adv}$ with minimal distance to $\x$ to fool the DNN
%model, and yet its human user would still visually consider the adversarial input $\x^{adv}$ similar to or the same as the benign input $x$.
%Given a benign sample $\x$, an adversarial example $\x^{adv}$ is generated by adding a small perturbation to $\x$ (i.e. $\x^{adv}=\x+\epsilon$), so that $\x^{adv}$ is misclassified by the targeted classifier $g$.
\begin{itemize}
\item Given a ML model $f(\cdot)$ and \emph{a small perturbation $\delta$}, we call $\x^{adv}$ an adversarial example
if there exists $x$, an example drawn from the benign data distribution, such that $f(\x)\neq f(\x^{adv})$ and $||\x - \x^{adv}|| \leq \delta$.
\item An human user would still visually consider the adversarial input $\x^{adv}$ similar to or the same as the benign input $x$
\item Usually, we are interested in adversarial for benign sample $\x$, i.e., sample were the model gives a correct predictions.
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[width=.285\columnwidth]{../graphics/Adversarial0} \textbf{+}
\includegraphics[width=.285\columnwidth]{../graphics/Noise} \textbf{=}
\includegraphics[width=.285\columnwidth]{../graphics/Adversarial2}
\caption{$\x$ + $\epsilon$ =  $\x^{adv}$}
\end{figure}

\begin{figure}
\includegraphics[width=.45\columnwidth]{../graphics/ClassX}
\includegraphics[width=.45\columnwidth]{../graphics/ClassXadv}
\caption{VGG19 Prediction for: $\x$ and $\x^{adv}$}
\end{figure}
\end{frame}

%\begin{frame}
%Formal definition:
%(Adversarial example (Wang et al., 2016)). Given a ML model $f(\cdot)$ and \emph{a small perturbation $\delta$}, we call $\x^{adv}$ an adversarial example
%if there exists $x$, an example drawn from the benign data distribution, such that $f(\x)\neq f(\x^{adv})$ and $||\x - \x^{adv}|| \leq \delta$.
%This is, $\x^{adv}$ could be called a \emph{counter example}.
%\end{frame}

%\begin{frame}
%\begin{figure}
%\includegraphics[width=.55\columnwidth]{../graphics/Adversarial2}
%\end{figure}
%From Explaining and Harnessing Adversarial Examples by Goodfellow et al.
%\end{frame}

%\begin{frame}
%\begin{figure}
%\includegraphics[width=.35\columnwidth]{../graphics/Noise}
%\end{figure}
%From Explaining and Harnessing Adversarial Examples by Goodfellow et al.
%\end{frame}

\begin{frame}{Gradient based Methods}
\begin{itemize}
\item Fast Gradient Sign Method \cite{Adversarial}
Their perturbation can be expressed as
\begin{equation}
\eta =\epsilon \cdot sign(\nabla_{\x} \loss (\x,y_{\texttt{true}}))
\end{equation}
where $\epsilon$ is the magnitude of the perturbation.
The generated adversarial example $\x^{adv}$ is calculated as $\x^{adv}=\x+\eta$.\\
This perturbation can be computed by using back-propagation.
\item Iterative Gradient Sign Method \cite{kurakin2016adversarial}
$\x_{0}^{adv}=\x$, iteratively repeat $\x_{t+1}^{adv}=\texttt{CLIP} (\x_{t}^{adv} + \epsilon \cdot sign(\nabla_{\x_{t}^{adv}} \loss (\x_{t}^{adv},y_{\texttt{true}}))$)
\end{itemize}
\pause
\alert{To avoid overfitting! and obtain more robust model.... Adversarial Training = Data Augmentation with adversarial examples}
\end{frame}

%\begin{frame}{Iterative Gradient Sign Method \cite{kurakin2016adversarial}}
%$\x_{0}^{adv}=\x$, iteratively repeat $\x_{t+1}^{adv}=\texttt{CLIP} (\x_{t}^{adv} + \epsilon \cdot sign(\nabla_{\x_{t}^{adv}} \loss (\x_{t}^{adv},y_{\texttt{true}}))$)
%\end{frame}

\begin{frame}{Universal Perturbation}
\begin{figure}
\includegraphics[width=.45\columnwidth]{../graphics/UniversalPerturbation}
\caption{Universal adversarial perturbations \cite{moosavi2017universal}}
\end{figure}
\end{frame}

\begin{frame}{White box Attacks}
\begin{figure}
\includegraphics[width=.95\columnwidth]{../graphics/WhiteBoxAttack}
\caption{White Box Attacks \cite{akhtar2018threat}}
\end{figure}
\end{frame}


%% \begin{frame}{Job Offers: Master Internships}
%% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\bibliography{slides_deep.bib}
\end{frame}

\end{document}
