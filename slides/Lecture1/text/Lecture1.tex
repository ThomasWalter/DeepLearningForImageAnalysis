%\documentclass[handout,xcolor=pdftex,dvipsnames,table,mathserif]{beamer}
\documentclass[xcolor=pdftex,dvipsnames,table]{beamer}
\input{../../setting.tex}


\title{Deep Learning for Image Analysis - \\
	   Lecture 1: An introduction to Machine Learning}
\author{Thomas Walter, PhD}
\date{Centre for Computational Biology (CBIO) \\
	  MINES Paris-Tech, PSL Research University \\
	  Institut Curie, PSL Research University \\
	  INSERM U900}


%To include LOGO?
%\logo{\includegraphics[width=.1\columnwidth]{MinesLogo}}
\useinnertheme{rounded}
\usecolortheme{rose}

\usepackage{xcolor}
\definecolor{lightblue}{RGB}{0,200,255}


\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{items}[square]


\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Overview}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction: Artificial Intelligence and Machine Learning}
\frame{\frametitle{Overview}\tableofcontents[currentsection]}

\begin{frame}{Definition of Artificial Intelligence and Machine Learning}
%\begin{definition}
%	Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.
%\end{definition}
\begin{itemize}
	\item The definition of the term \textbf{intelligence} is highly controversial. Usually, one understands by intelligence the capacity of an individual to reason logically, to understand complexity, to learn more or less abstract concepts, to plan and to solve problems in varying conditions.
	\item \textbf{Artificial intelligence (AI)} is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.
	\item AI effect: \emph{"AI is whatever hasn't been done yet"}.
	\item In 1956, AI became a field of research. AI can be broken down into many subfields: knowledge representation, planning, natural language processing, object manipulation (robotics), machine learning, $\ldots$
	\item \textbf{Machine Learning} is concerned with the technology that enables computer programs to improve their performance at a certain task by experience.
\end{itemize}
\end{frame}

\begin{frame}{A short history of artificial intelligence}
\begin{itemize}
	\item The dream to create machines that can think and act has been present in literature and mythology since antiquity (e.g. the myth of \emph{"Talos"} or \emph{"Rossum's Universal Robots"} by Karel \v{C}apek, 1920).
	\item AI as a scientific discipline took its beginning with the publication of Alan Turing in 1950 (Turing test). The question \emph{"Can machines think?"} was asked scientifically.
	\item Other theoretical bases were developed by Wiener (cybernetics) and Shannon (information theory).
	\item The term \emph{"artificial intelligence"} was cornered in 1956, in the famous Dartmouth conference, where AI has been established as a field.
\end{itemize}
\end{frame}

\begin{frame}{A short history of machine learning}
\begin{figure}[htb]
  \centering
  \subfloat{\includegraphics[height=0.23\textheight]{../graphics/Mark_1_perceptron.jpeg}}\hspace{2cm}
  \subfloat{\includegraphics[height=0.23\textheight]{../graphics/perceptron.jpg}}
  \caption{The Mark I Perceptron}
\end{figure}
\vspace{-.5cm}
\begin{itemize}
	\item The theoretical foundations go back to the early 19th century (e.g. Bayesian theory and Least Squares).
	\item Even before machines came into play, there was a lot of interest in finding methods to derive rules from data (\emph{data fitting}). These methods are part of what we call Machine Learning today (linear regression, Bayesian theory, Logistic regression, Linear discriminant analysis, Markov chains).
	\item In 1958, Rosenblatt published his \emph{Perceptron} (basically a linear classifier), which is seen as the predecessor of Neural Networks.
\end{itemize}
\end{frame}

\begin{frame}{Machine Learning: basic definitions}
\begin{itemize}
	\item Machine Learning aims at predicting some output $y$ from an input (or measurement) $x$:
	\begin{equation}
	y = f(x)
	\end{equation}
	\item In this formulation, Machine Learning aims at finding (learning) $f$ from available data.
	\item The data that is used to learn $f$ is called \textbf{training set}.
	\item In this general formulation, there is no particular limitation as to the mathematical nature of $x$ and $y$. In many cases $x$ is a $\nfeatures$-dimensional vector and $y$ a categorical or continuous output variable, but there are other settings, where $x$ and / or $y$ are more complicated objects, such as images or graphs.
\end{itemize}
\end{frame}

\begin{frame}{Different settings in Machine Learning}
	\begin{table}
	\begin{tabular}{|l || c | c | }
		\hline
 		& Supervised & Unsupervised \\
		\hline \hline
		$y$ discrete & Classification & Clustering \\
		$y$ continuous & Regression & Dimensionality reduction\\
		\hline
	\end{tabular}
	\end{table}
\begin{itemize}
	\item In \textbf{supervised learning}, the training data contains both measurements $x_i$ and the corresponding output variables $y_i$. Together, they build the training set $T$:
	\begin{equation}
	T = \{(x_i, y_i)\}_{i=1, \ldots, \nsize}
	\end{equation}
	\item In \textbf{unsupervised learning}, there are no annotations $y_i$. We aim at inferring \textbf{patterns} from the data (clusters, latent variables).
\end{itemize}
\end{frame}

\begin{frame}{Classification vs. Clustering}
	 \begin{figure}[htb]
   		\centering
   		\subfloat{\includegraphics[height=0.3\textheight]{../graphics/Figure_supervised.pdf}} \hspace{1cm}
   		\subfloat{\includegraphics[height=0.3\textheight]{../graphics/Figure_unsupervised.pdf}}
   		\caption{Supervised and unsupervised learning}
	 \end{figure}
	\begin{itemize}
		\item In {\bf supervised learning}, we start from annotated data (here annotation is illustrated by the color), and we wish to learn a decision boundary that allows us to tell these classes apart.
		\item In {\bf unsupervised learning}, we start with a point clouds and we wish to identify classes directly from their distribution.
	\end{itemize}
\end{frame}


\begin{frame}{Training and prediction}
\begin{figure}[htb]
  \centering
  \subfloat[Training a classifier]{\includegraphics[width=0.6\textwidth]{../graphics/workflow_training.png}}\\
  \subfloat[Prediction]{\includegraphics[width=0.8\textwidth]{../graphics/workflow_prediction.png}}
\end{figure}
\begin{itemize}
\item Training: to learn a model from the training set. Depending on the method, this can take minutes, hours or days.
\item Prediction: to apply the learned classifier to new data. This is usually computationally efficient.
\end{itemize}
\end{frame}

\begin{frame}{Objects and features}
\begin{itemize}
\item Machine learning typically deals with objects outside the mathematical world (emails, images, genomes, cars, $\ldots$).
\item The first step is therefore to find a suitable representation of the objects.
\begin{itemize}
\item \textbf{feature engineering}: finding descriptors according to existing domain knowledge
\item \textbf{representation learning}: learning the descriptors together with the classifier
\end{itemize}
\item In many cases the objects can be represented by a $\nfeatures$-dimensional vector of features (or descriptors): $\x \in \mathbb{R}^{\nfeatures}$.
\item It can be convenient to map a feature vector to a higher dimensional space:
\begin{eqnarray}
\featmap : \mathbb{R}^{\nfeatures} &\rightarrow & \mathbb{R}^Q \\
\x &\rightarrow &\featmap (\x)
\end{eqnarray}
\end{itemize}
\end{frame}

\begin{frame}{Training set and design matrix}
\begin{itemize}
\item In the frequent case that objects can be described by a feature vector $\x \in \mathbb{R}^{\nfeatures}$, we can represent the training set $T = \{(x_i, y_i)\}_{i=1, \ldots, \nsize}$ by a $\nsize \times \nfeatures$ \textbf{design matrix} $\X$ and an output vector $y$.
\item In the design matrix, each row corresponds to one sample, each column corresponds to one feature:
\begin{table}
\begin{tabular}{|l || c | c | c | c |}
	\hline
		& feature 1 & feature 2 & $\ldots$ & feature $\nfeatures$ \\
	\hline \hline
	Sample 1 & 0.23 & 1.30 & $\ldots$ & 0.01 \\
	\hline
	Sample 2 & 0.42 & 1.15 & $\ldots$ & -0.23 \\
	\hline
	$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
	\hline
	Sample $\nsize$ & 0.31 & 1.53 & $\ldots$ & 0.33 \\
	\hline
\end{tabular}
\end{table}
\end{itemize}
\end{frame}


\begin{frame}{Example: classification of flowers}
\begin{figure}
  \centering
  \subfloat[Iris setosa]{\includegraphics[height=2.4cm]{../graphics/Iris_setosa.jpg}}\qquad
  \subfloat[Iris versicolor]{\includegraphics[height=2.4cm]{../graphics/Iris_versicolor.jpg}}\qquad
  \subfloat[Iris virginica]{\includegraphics[height=2.4cm]{../graphics/Iris_virginica.jpg}}\qquad
  \label{fig:Iris_data_set}
\end{figure}
\begin{itemize}
	\item One of the oldest data sets in machine learning is the Iris data set, that was collected by the statistician and biologist Ronald Fisher in 1936 \cite{Fisher1936}.
	\item There are 3 classes (different types of the Iris flower). For each class, there were 50 samples collected. For each sample, 4 characteristics were measured (lengths and widths of different parts of the plants).
	\item The task is thus to learn a rule to predict the type of the flower ($y$) from a 4-dimensional vector $x$ of geometric measurements.
\end{itemize}
\end{frame}

\begin{frame}{Example: classification of SPAM emails}
\begin{figure}[htb]
\includegraphics[width=0.9\textwidth]{../graphics/SPAM_mail.png}
\end{figure}
\begin{itemize}
	\item This is a binary classification problem: $y \in \{0,1\}$ (0: junk, 1: normal).
	\item The features can be constructed in the following way: for each email annotated by the user, the words are listed. An email is described as a vector of frequencies of these words.
	\item The system learns then a function that assigns to each vector of measured word frequencies the label $y$.
\end{itemize}
\end{frame}

\begin{frame}{Example: classification of drugs}
\begin{figure}[htb]
\includegraphics[width=0.7\textwidth]{../graphics/ml_example_drugs.pdf}
\end{figure}
\begin{itemize}
	\item Here, we want to classify molecules with respect to their efficiency against a disease (binary classification: a drug is efficient or not).
	\item An important question here is how to encode a molecule. One option is to define chemoinformatic features and obtain a vectorial representation of the molecule $\x \in \mathbb{R}^{\nfeatures}$.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Principles of Machine Learning algorithms}
\frame{\frametitle{Overview}\tableofcontents[currentsection]}

\begin{frame}{A simple example: polynomial curve fitting\footnote{Example adapted from \cite{Bishop2006}}}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/sample_from_sin.png}
\end{figure}
\begin{itemize}
	\item From a set of measured points $(x_i, y_i)$ (red), we would like to build a model to predict the value $y$ for any given $x$.
	\item The true function is $g(x)=\sin (x)$ (displayed in blue).
	\item The measurements $y_i$ are noisy outputs of that function, i.e.
	\begin{equation}
	y_i = \sin (x_i) + \epsilon \; , \;\;\; \;\;\; \epsilon \sim \mathcal{N}(0,0.2)
	\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{A simple example: polynomial curve fitting}
\begin{itemize}
	\item We use the following polynomial model:
	\begin{eqnarray}
	f(x) &=& a_0 + a_1 x + a_2 x^2 + \ldots + a_m x^m \nonumber \\
	&=& \param^T \featmap (x)
	\end{eqnarray}
	\item Parameter vector: $\param = (a_0, a_1, \ldots, a_m)^T$
	\item Here, the initial measurement $x$ is a scalar. In our model, we map $x$ to a higher dimensional space:
	\begin{eqnarray}
		\featmap : \mathbb{R}^{\nfeatures} &\rightarrow & \mathbb{R}^Q \nonumber \\
		x &\rightarrow & \featmap (x) = (1, x, x^2, \ldots, x^m)^T
	\end{eqnarray}
	\item The model is linear in the parameters $\theta$ and linear in $\featmap$, but for $m>1$, the model is not linear in $x$.
\end{itemize}
\end{frame}

\begin{frame}{A simple example: polynomial curve fitting}
\begin{itemize}
	\item One classical approach is to minimize the least squared error between measured and predicted values:
	\begin{eqnarray}
		\min_{\param} \loss(\param) &=& \min_{\param} \sum_{i=1}^N (y_i - f(x_i))^2 \nonumber \\
		&=& \min_{\param} \sum_{i=1}^N (y_i - \param^T \featmap (x_i))^2
	\end{eqnarray}
	\item This can be achieved by setting the gradient with respect to $\param$ to zero:
	\begin{equation}
		\nabla_{\param} \loss = (\frac{\partial \loss}{\partial a_0}, \frac{\partial \loss}{\partial a_1}, \ldots, \frac{\partial \loss}{\partial a_m} )^T = 0
	\end{equation}
	\item Unlike for most optimization problems in this course, this leads to an analytical solution for $\param$. This is known as \textbf{linear regression}. For more details, we refer to \cite{Hastie2009}.
\end{itemize}
\end{frame}

\begin{frame}{Overfitting and underfitting}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/polyfit_degree_1.png}
\end{figure}
For $m=1$, the model is linear in its inputs. The solution is not capable of modeling the measured data points; we get a poor approximation of the original function. The family of functions we have used was not complex enough to model the true data distribution. We also speak of \textbf{underfitting}.
\end{frame}

\begin{frame}{Overfitting and underfitting}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/polyfit_degree_3.png}
\end{figure}
For $m=3$, we obtain a solution that seems to be quite right: it is sufficiently complex to model the true data distribution, but not too complex to model the small variations which are due to noise.
\end{frame}

\begin{frame}{Overfitting and underfitting}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/polyfit_degree_11.png}
\end{figure}
For $m=11$, we obtain a solution that has zero error (the function passes through every point of the training set). But the coefficients with large absolute values that cancel each other precisely on the training points lead to a highly unstable function. We speak of \textbf{overfitting} and \textbf{poor generalization}.
\end{frame}

\begin{frame}{Overfitting and underfitting}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/polyfit_degree_11_N60.png}
\end{figure}
One way of reducing overfitting is to increase the number of samples. Even if the function is complex, it cannot be “too wild”, as it has to find a compromise between many training samples. This however implies the annotation (or measurement) of more samples.
\end{frame}

\begin{frame}{Overfitting and underfitting}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/ridge_regression_11_10.png}
\end{figure}
Another way of preventing overfitting without increasing the number of samples, is to add a penalization term in the optimization procedure. This is also known as \textbf{regularization}:
\begin{equation}
	\loss = \sum_{i=1}^N (y_i - \param^T \featmap (x_i))^2 + \lambda \| \param \|^2
\end{equation}
\end{frame}

\begin{frame}{Generalization: training and test error}
\begin{figure}[htb]
\includegraphics[width=0.6\textwidth]{../graphics/Training_and_test_error.png}
\end{figure}
\begin{itemize}
\item Supervised Learning aims at finding a function $f$ that predicts an output value $y$ from a measurement $x$ for unseen data, i.e. for data that has not been used to find $f$.
\item Machine Learning is much concerned with avoiding $f$ to \textbf{memorize} the training set, i.e. to perform well on a training set but poorly a test set.
\item An important paradigm is that we must never evaluate the performance of our machine learning method on the data that has been used to train it.
\end{itemize}
\end{frame}

\begin{frame}{Generalization: strategies}
\begin{itemize}
\item Many ML algorithms can be written as an optimization problem:
\begin{equation*}
\param ^{\ast} = \argmin_{\param} \loss (\param) + \mathcal{R}(\param)
\end{equation*}
Minimizing the loss $\loss (\param)$ aims at finding the rule to reproduce the annotations in the training set, minimizing the regularization term $\mathcal{R}(\param)$ aims at avoiding the model to adapt too much to the training data, leading to simpler models. We have seen the $L_2$ norm, but there are many other options for $\mathcal{R}$.
\item Other regularization strategies include:
\begin{itemize}
\item Model averaging (ensemble methods)
\item Artificial or actual increase of training data
\item Adversarial training
\end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supervised Learning: Example algorithms}

% \begin{frame}{Example: classification of cells}
% \begin{figure}[htb]
% \includegraphics[width=0.8\textwidth]{../graphics/ComputationalPhenotyping.pdf}
% \end{figure}

% \begin{itemize}
% 	\item In this example, we wish to classify cell images.
% 	\item For this, cells are first segmented and then described by a vector $x$ of features describing shape and texture.
% 	\item We are given a training set $T = \{(x_i, y_i)\}_{i=1, \ldots, \nsize}$, i.e. for each cell in the training set, we know its morphological class.
% 	\item The task is to infer a function $f$ from the training set that allows to assign one of these classes to new unseen objects.
% \end{itemize}
% \end{frame}

\begin{frame}{Example: classification of cells}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/ComputationalPhenotyping.pdf}
\end{figure}

\begin{itemize}
	\item In this example, we wish to classify cell images.
	\item For this, cells are first segmented and then represented by a vector $x \in \mathbb{R}^P$ of features describing shape and texture for each segmented object.
	%\item We define a set of morphological, biologically meaningful classes.
	\item We are given a training set $T = \{(x_i, y_i)\}_{i=1, \ldots, \nsize}$, i.e. for each cell in the training set $x_i \in \mathbb{R}^P$, we know its morphological class $y_i$.
	\item The task is to infer a function $f$ from the training set that allows to assign one of these classes to new unseen objects.
\end{itemize}
\end{frame}

%\subsection{Nearest Neighbor classification}
\subsection{Random Forests}
\begin{frame}[plain,c]
\begin{center}
\Huge Random Forests
\end{center}
\end{frame}

\begin{frame}{Random Forest: Decision trees}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/CellClassification_RF.pdf}
\end{figure}

\begin{itemize}
	\item Intuitive approach: applying a series of "rules" to the training data to recover the classes $\omega_i$ (e.g. "is the cell bright?", "is the cell elongated?", ... )
	\item Each rule (or decision) divides the set of objects into two subsets.
	\item The whole classifier is thus represented by a decision tree, i.e. a hierarchically organized set of binary decisions.
\end{itemize}
\end{frame}

\begin{frame}{Random Forest: Decision trees}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/RF2.pdf}
\end{figure}
\begin{itemize}
	\item The decision tree corresponds to a partition of the feature space.
	\item The corresponding decision boundaries can be very complex and adapt to the training set.
	\item Classification of a new object: application of the binary decision rules and assignment of the leaf label.
\end{itemize}
\end{frame}

\begin{frame}{Random Forest: Decision trees}
\begin{itemize}
\item Such decision trees can be built from data in an optimal way, by repeated division of the feature space.
\item At every step, we split the data set into two, according to one feature.
\item The feature and the threshold are chosen automatically in such a way that the resulting groups have best "purity".
\item This can be achieved by minimizing the GINI impurity. For $K$ classes, GINI impurity is defined as:
\begin{eqnarray*}
GI(R_m) &=& \sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk}) \\
GI(s) &=& \sum_{R_m}=\frac{|R_m|}{N}GI(R_m)
\end{eqnarray*}
where $R_m$ are the sets resulting from the split $s$ and $\hat{p}_{mk}$ is the probability of a sample in $R_m$ to belong to class $k$.
\end{itemize}
\end{frame}

\begin{frame}{Random Forest: GINI impurity}
\begin{figure}[htb]
\includegraphics[width=0.7\textwidth]{../graphics/RF_GINI.pdf}
\end{figure}
Here, we obtain for the GINI impurity:
\begin{eqnarray*}
GI(R_1) &=&  \frac{1}{10} \cdot \frac{9}{10} + \frac{5}{10} \cdot \frac{5}{10} + \frac{4}{10} \cdot \frac{6}{10} = 0.58 \\
GI(R_2) &=&  \frac{0}{7}\cdot \frac{7}{7} + \frac{4}{7} \cdot \frac{3}{7} + \frac{3}{7} \cdot \frac{4}{7} = 0.49 \\
GI(s) &=&  \frac{10}{17}\cdot R_1 + \frac{7}{17}\cdot R_2 = 0.54
\end{eqnarray*}
\end{frame}

\begin{frame}{Random Forests}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/Forest.pdf}
\end{figure}
\begin{itemize}
	\item Decision trees can approximate very complicated decision boundaries, but they tend to fit to much to the data (overfitting).
	\item Random forests: set of decision trees, each learned on a different (randomly drawn) portion of the data and with different (randomly selected) features.
	\item Each tree gives a classification result.
	\item The final result is obtained by a majority vote.
\end{itemize}
\end{frame}

\begin{frame}{Random Forests}
\begin{itemize}
	\item Because each tree is slightly different from the others, each tree learns a slightly different aspect of the training data.
	\item A classification method that exists in averaging the results of several classifiers (often weak classifiers), is called {\bf ensemble method}.
	\item The strategy of averaging is a form of regularization.
	\item In practice, model averaging is very popular in the deep learning field. Often, one averages simply the output of several networks to obtain a more robust classifier.
\end{itemize}
\end{frame}

\subsection{Linear Discriminant Analysis (LDA)}
\begin{frame}[plain,c]
\begin{center}
\Huge Linear Discriminant Analysis
\end{center}
\end{frame}


\begin{frame}{The Bayes rule of classification 1/3}
\begin{block}{Product and sum rules of probabilities}
	Let $X$ and $Y$ be random variables, and let $P(X)$ and $P(Y)$ be their marginal probabilities, $P(X,Y)$ their joint probability and $P(X\,|\,Y)$ the conditional probability of $X$ given $Y$.
	\begin{itemize}
		\item Product rule:
		\begin{equation}
			P(X,Y) = P(X\,|\,Y)P(Y)
		\end{equation}
		\item Sum rule:
		\begin{equation}
			P(X) = \sum_Y P(X,Y)P(Y)
		\end{equation}
	\end{itemize}
\end{block}
From the product rule and $P(X,Y)=P(Y,X)$ we get immediately the Bayes theorem:
\begin{equation}
	P(Y \, | \, X) = \frac{P(X \, | \, Y)P(Y)}{P(X)}
\end{equation}
\end{frame}

\begin{frame}{The Bayes rule of classification 2/3}

We assume that the measured features are continuous variables. Almost identical rules hold in this case, but we need to replace probabilities by probability densities.

\begin{block}{Bayes rule of classification}
	Let $x \in \mathbb{R}^P$ with a probability density $p(x)$ and $Y \in \{\omega_1, \ldots, \omega_K\}$ a discrete set of class labels. The posterior probability $P(Y=\omega_k\,|\,x)$ can be written as:
	\begin{equation}
		P(Y=\omega_k \, | \, x) = \frac{p(x \, | \, Y=\omega_k) P(Y=\omega_k)}{p(x)}
	\end{equation}
	The Bayes rule of classification: \emph{choose the class that maximizes the posterior probability} $P(Y=\omega_k|x)$:
	\begin{equation}
		\omega^{\ast}(x) = \arg\max_k P(Y=\omega_k \, | \, x) = \arg\max_k p(x\,|\,Y=\omega_k)P(Y=\omega_k)
	\end{equation}
\end{block}

\end{frame}


\begin{frame}{The Bayes rule of classification 3/3}

\begin{itemize}
\item The Bayes rule of classification simply states that the best class to choose is the one with highest probability given the observation.
\item Importantly, we can calculate this posterior probability from the class dependent feature distributions and the class probabilities.
\item This rule is important because it gives the best possible classifier, given that the feature distributions $p(x\,|\,Y=\omega_k)$ and the class probabilities $P(Y=\omega_k)$ are known.
\item However, this is normally not the case: these distributions need to be estimated from the training data.

\end{itemize}

\end{frame}

\begin{frame}{The Bayes rule for classification in the binary case}

For simplicity, we assume the case of binary classification, i.e. $\Omega = \{\omega_1, \omega_2\}$. The Bayes rule for classification states that we should choose $\omega_1$ if:

\begin{equation*}
\frac{p(x|\omega_1)P(\omega_1)}{p(x|\omega_2)P(\omega_2)} > 1
\end{equation*}
\\
We can apply the $log$ to both sides of the equation. We obtain the following expression for the Bayes rule of classification:
\begin{equation}\label{equ:lda:condition}
\log{ \frac{p(x|\omega_1)P(\omega_1)}{p(x|\omega_2)P(\omega_2)} } > 0
\end{equation}

\end{frame}

\begin{frame}{Normality assumption}
\begin{figure}[htb]
\includegraphics[width=0.8\textwidth]{../graphics/LDA1.pdf}
\end{figure}
We assume that features are normally distributed for each of the classes:
\begin{equation}\label{equ:lda:normal}
p(x|\omega_k)=\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}
\end{equation}
\end{frame}

\begin{frame}{Linear Discriminant Analysis (LDA)}
Plugging equation (\ref{equ:lda:normal}) into (\ref{equ:lda:condition}) leads to {\bf Quadratic discriminant analysis (QDA)}:
\begin{eqnarray*}
&& \log{\frac{P(\omega_1)}{P(\omega_2)}} +
\frac{1}{2}\log{\frac{|\Sigma_2|}{|\Sigma_1|}}
- \frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) \nonumber \\
&+& \frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) > 0
\end{eqnarray*}

% \begin{equation*}
% \log{\frac{P(\omega_1)}{P(\omega_2)}} +
% \frac{1}{2}\log{\frac{|\Sigma_2|}{|\Sigma_1|}}
% - \frac{1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1)
% + \frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) > 0
% \end{equation*}

If we further assume that the covariance matrices of the classes are equal, i.e. $\Sigma_1 = \Sigma_2 = \Sigma$), we obtain {\bf Linear Discriminant Analysis, LDA}:
\begin{equation*}
\log{\frac{P(\omega_1)}{P(\omega_2)}} + x^T\Sigma^{-1}(\mu_1-\mu_2) - \frac{1}{2}(\mu_1-\mu_2)^T\Sigma^{-1}(\mu_1+\mu_2) > 0
\end{equation*}
\end{frame}

\begin{frame}{Comparison LDA vs. closest mean 1/2}
\begin{itemize}
	\item Closest Mean Classifier:
	\begin{itemize}
		\item Calculate the mean value for each of the classes (from the training set).
		\item Rule: for a new vector $x \in \mathbb{R}^P$ assign the class with the closest mean, i.e. choose class $\omega_1$ if :
		\begin{eqnarray*}
		\|x - \mu_1\| &<& \|x - \mu_2\| \\
		(x-\mu_1)^T(x-\mu_1) - (x-\mu_2)^T(x-\mu_2) &<& 0 \\
		x^T(\mu_2 - \mu_1) + \frac{1}{2}(\|\mu_1\|^2 - \|\mu_2\|^2) &<& 0 \\
		x^T(\mu_1 - \mu_2) + b &>& 0
		\end{eqnarray*}
	\end{itemize}
	\item We see that the only term that depends on $x$ is linear and corresponds to a projection of $x$ onto the difference vector of the two class means $\mu_1$ and $\mu_2$.
\end{itemize}
\end{frame}

\begin{frame}{Comparison LDA vs. closest mean 2/2}
	\begin{figure}[htb]
		\includegraphics[width=0.8\textwidth]{../graphics/LDA_projection.pdf}
	\end{figure}
	\begin{itemize}
		\item The expression for LDA classification is:
		\begin{equation*}
			x^T\Sigma^{-1}(\mu_1-\mu_2) + b > 0
		\end{equation*}
		\item We thus see that the only difference is that we take the covariance matrix $\Sigma$ into account.
		\item The normal vector of the separating hyperplane is thus not $(\mu_1-\mu_2)$ but $\Sigma^{-1}(\mu_1-\mu_2)$.
	\end{itemize}
\end{frame}

\subsection{Support Vector Machines (SVM) and kernel methods}
\begin{frame}[plain,c]
\begin{center}
\Huge Support Vector Machines
\end{center}
\end{frame}

\begin{frame}{Support Vector Machines: principle}
	\begin{figure}[htb]
		\includegraphics[width=0.8\textwidth]{../graphics/SVM1.pdf}
	\end{figure}
	\begin{itemize}
		\item Optimal placement of a linear decision boundary.
		\item Intuitive approach: place a "ribbon" instead of a single line, i.e. two parallel lines separated by a distance $d$.
		\item Maximization of the width $d$ in order to push the separating hyperplane away from the two classes.
	\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines: linearly separable training set}
	\begin{figure}[htb]
		\includegraphics[width=0.7\textwidth]{../graphics/SVM2.pdf}
	\end{figure}
	\begin{itemize}
		\item Equation of a linear decision boundary $h: w^Tx + b = 0$.
		\item $w$: normal vector of the separating hyperplane.
		\item $w^Tx + b  > 0$: $x$ is on the same side as the normal vector $w$ points to.
		\item $w^Tx + b  < 0$: $x$ is on the opposite side.
	\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines: linearly separable training set}
	\begin{figure}[htb]
		\includegraphics[width=0.7\textwidth]{../graphics/SVM2.pdf}
	\end{figure}
	\begin{itemize}
		\item With the encoding $y\in\{-1,+1\}$, a sample is correctly classified if $y_i(w^Tx_i + b) > 0$.
		\item The distance between the two parallel lines $w^Tx + b = \pm 1$ is given by
		\begin{equation}
			d = \frac{2}{\|w\|}
		\end{equation}
		\item Maximizing of $d$ is equivalent to minimizing $\|w\|^2$.
	\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines: linearly separable training set}
	\begin{figure}[htb]
		\includegraphics[width=0.7\textwidth]{../graphics/SVM2.pdf}
	\end{figure}
	\begin{itemize}
		\item With this, the problem can be written as a {\bf convex optimization problem under constraints}:
		\begin{eqnarray*}
			\mbox{minimize} & & \|w\|^2 \\
			\mbox{subject to} & & y_i(w^Tx_i + b) \geq 1 \quad i = 1, \ldots, N
		\end{eqnarray*}
		\item Convexity implies that there is no local minimum besides the global minimum.
	\end{itemize}
\end{frame}


\begin{frame}{SVM for not linearly separable data}
	\begin{figure}[htb]
		\includegraphics[width=0.7\textwidth]{../graphics/SVM3.pdf}
	\end{figure}
	\begin{itemize}
		\item In reality, complex data is rarely linearly separable.
		\item In this case, we need to find a compromise between error term $\sum_{i=1}^{N}\xi_i$ and regularization $\|w\|^2$.
		%\item Normal vector $w$: vector of feature weights.
		%\begin{equation*}
		%	f(x) = w^Tx + b = \sum_{k=1}^{P}w^{(k)}x^{(k)} + b
		%\end{equation*}
	\end{itemize}
\end{frame}

\begin{frame}{SVM for not linearly separable data}
	\begin{itemize}
		\item SVM correspond to the following constrained optimization problem:
		\begin{eqnarray*}
			\min_{w,\xi} & & \|w\|^2 + C \sum_{i=1}^{N}\xi_i\\
			\mbox{subject to} & & y_i(w^Tx_i + b) \geq 1 - \xi_i \quad i = 1, \ldots, N \\
			& & \xi_i \geq 0 \quad i = 1, \ldots, N
		\end{eqnarray*}
		\item This problem can be efficiently solved with the Lagrange formalism (not shown).
		\item We note that this corresponds to the formulation we have seen earlier:
		\begin{equation*}
			\param ^{\ast} = \argmin_{\param} \loss (\param) + \mathcal{R}(\param)
		\end{equation*}
	\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines: Kernel trick}
	\begin{figure}[htb]
		\includegraphics[width=0.7\textwidth]{../graphics/KernelTrick.pdf}
	\end{figure}
	\begin{itemize}
		\item Limitation: linear decision boundaries
		\item Solution: transform the features in a higher dimensional space.
		\item SVM: in-built mechanism to do such transformations efficiently.
	\end{itemize}
\end{frame}


\begin{frame}{How to set hyperparameters}
	\begin{itemize}
		\item Training a classifier: finding automatically a large number of feature weights (SVM, LDA) or other parameters (RF) from annotated data.
		\item Hyperparameters: a small number of parameters that are set to control the training process, such as the number of trees, or the regularization parameter $\lambda$ ($C$ in SVM).
		\item The question is: how can we find good hyperparameters?
		\item Strategy: grid search. We choose the set of hyperparameters that perform best (i.e. that lead to the classifier with the best performance).
		\item Problem: how can we measure the performance?
	\end{itemize}
\end{frame}

\begin{frame}{Assessment of performance}
	\begin{itemize}
		\item<1-> First idea: we train a classifier on the training set $T$ and calculate the accuracy.
		\item<2-> Problem: Training error is a poor approximation of the test error.
		\begin{figure}[htb]
			\includegraphics[width=0.6\textwidth]{../graphics/Training_and_test_error.png}
		\end{figure}
		\item<3-> We need to estimate the test error (error on unseen samples)!
	\end{itemize}
\end{frame}

\begin{frame}{How to set hyperparameters: estimation of the test error}
	\begin{itemize}
		\item<1-> Second idea: to split the training set in two subsets $T_1$, $T_2$. We train on $T_1$, we test on $T_2$.
		\item<2-> Problem: annotated data is often expensive, and we would like to have the largest possible sets for training and testing.
		\item<3-> Third idea: Cross Validation. We split the data into $K$ folds. We train on $K-1$ folds, and we test on the remaining fold. We iterate until each fold has been used once for testing.
		\begin{figure}[htb]
			\includegraphics[width=0.65\textwidth]{../graphics/CV1.pdf}
		\end{figure}
		\item<4-> This provides us with an estimation of the accuracy for unseen data (test error).
		\item<5-> Hyperparameter selection: we can now choose the hyperparameter with the best accuracy.
	\end{itemize}
\end{frame}

\begin{frame}{How to set hyperparameters: grid search}
	\begin{figure}[htb]
		\includegraphics[width=0.6\textwidth]{../graphics/CV2.pdf}
	\end{figure}
	\begin{itemize}
		\item If there are several parameters, we can perform this for every combination of values (grid search).
		\item Alternatively, we can use random hyperparameter values.
		\item Problem: time-consuming.
		\item Ad hoc strategies to reduce the computation time: coarse-to-fine strategy.
	\end{itemize}
\end{frame}

\begin{frame}{Performance evaluation with optimized hyperparameters}
	\begin{itemize}
		\item<1-> If we optimize the hyperparameters in this way, the final performance might be over-optimistic (we have chosen the hyperparameters that give best performance for the test set, i.e. we have used the test set to set the parameters).
		\item<2-> Solution: nested cross validation.
		\begin{figure}[htb]
			\includegraphics[width=0.6\textwidth]{../graphics/CV3.pdf}
		\end{figure}
		\item<3-> Nested cross validation is extremely time consuming.
	\end{itemize}
\end{frame}

\begin{frame}{Performance evaluation in deep learning}
	\begin{itemize}
		\item In the context of deep learning with long training times, we often do not use cross validation.
		\item Split the training set into 3:
		\begin{itemize}
			\item {\bf Training set}: used to obtain the classifier for a given set of hyperparameters.
			\item {\bf Validation set}: used to find good hyperparameters.
			\item {\bf Test set}: used to evaluate performance.
		\end{itemize}
	\end{itemize}
\end{frame}


\section{Conclusion}
\begin{frame}{Conclusion}
	\begin{itemize}
		\item Supervised Machine Learning is concerned with inferring a function $f$ from a set of annotated data, allowing to predict an output variable $y$ from an input variable $x$.
		\item Different views:
			\begin{itemize}
				\item Probabilistic view: we maximize the posterior probability.
				\item Discriminant view: we optimize the separation of classes.
			\end{itemize}
		\item Ultimately, we with to minimize the error, but we need to regularize the predicting function.
		\item Forms of regularization we have seen so far:
			\begin{itemize}
				\item Regularization by model averaging (e.g. RF)
				\item Minimizing a global loss function containing an error and a regularization term:
				\begin{equation*}
					\param ^{\ast} = \argmin_{\param} \loss (\param) + \mathcal{R}(\param)
				\end{equation*}
			\end{itemize}
		\item All the methods, we have seen so far work on a fixed representation of the objects (often a vector $x \in\mathbb{R}^P$).
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\bibliography{slides_deep.bib}
\end{frame}


\end{document}
