@article{ImageNet:2015,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}


@inproceedings{Iandola:2017,
  title = {{{SqueezeNet}}: {{AlexNet}}-Level Accuracy with 50x Fewer Parameters and {$<$}0.{{5MB}} Model Size},
  booktitle = {{{ICLR}}},
  author = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  year = {2017},
  pages = {1--13},
  archivePrefix = {arXiv},
  eprint = {1602.07360v4},
  eprinttype = {arxiv}
}


@article{Sullivan2018a,
  title = {Deep Learning Is Combined with Massive-Scale Citizen Science to Improve Large-Scale Image Classification},
  author = {Sullivan, Devin P and Winsnes, Casper F and {\AA}kesson, Lovisa and Hjelmare, Martin and Wiking, Mikaela and Schutten, Rutger and Campbell, Linzi and Leifsson, Hjalti and Rhodes, Scott and Nordgren, Andie and Smith, Kevin and Revaz, Bernard and Finnbogason, Bergur and Szantner, Attila and Lundberg, Emma},
  year = {2018},
  month = oct,
  volume = {36},
  pages = {820--828},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.4225},
  file = {/Users/twalter/Zotero/storage/58NJHL7P/Sullivan et al. - 2018 - Deep learning is combined with massive-scale citiz.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {9}
}


@article{courtiolClassificationDiseaseLocalization2017,
  title = {Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly Supervised Approach},
  author = {Courtiol, Pierre and Tramel, Eric W and Sanselme, Marc and Wainrib, Gilles},
  year = {2017},
  pages = {1--13},
  archivePrefix = {arXiv},
  eprint = {1802.02212v1},
  eprinttype = {arxiv},
  journal = {CoRR}
}




@article{Ganin2016,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  year = {2016},
  volume = {17},
  pages = {1--35},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2015/08/013},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
  archivePrefix = {arXiv},
  eprint = {1505.07818},
  eprinttype = {arxiv},
  file = {/Users/twalter/Zotero/storage/W3GUQBE3/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf;/Users/twalter/Zotero/storage/QWY5UK6C/1505.html},
  isbn = {15324435},
  journal = {Journal of Machine Learning Research},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  number = {17}
}



@inproceedings{Durand2016,
  title = {{{WELDON}}: {{Weakly Supervised Learning}} of {{Deep Convolutional Neural Networks}}},
  shorttitle = {{{WELDON}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Durand, Thibaut and Thome, Nicolas and Cord, Matthieu},
  year = {2016},
  month = jun,
  pages = {4743--4752},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.513},
  abstract = {In this paper, we introduce a novel framework for WEakly supervised Learning of Deep cOnvolutional neural Networks (WELDON). Our method is dedicated to automatically selecting relevant image regions from weak annotations, e.g. global image labels, and encompasses the following contributions. Firstly, WELDON leverages recent improvements on the Multiple Instance Learning paradigm, i.e. negative evidence scoring and top instance selection. Secondly, the deep CNN is trained to optimize Average Precision, and fine-tuned on the target dataset with efficient computations due to convolutional feature sharing. A thorough experimental validation shows that WELDON outperforms state-of-the-art results on six different datasets.},
  file = {/Users/twalter/Zotero/storage/EBUK7YVI/Durand et al. - 2016 - WELDON Weakly Supervised Learning of Deep Convolu.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}



@article{Ouyang2018,
  title = {Deep Learning Massively Accelerates Super-Resolution Localization Microscopy},
  author = {Ouyang, Wei and Aristov, Andrey and Lelek, Micka{\"e}l and Hao, Xian and Zimmer, Christophe},
  year = {2018},
  month = apr,
  volume = {36},
  pages = {460},
  publisher = {{Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.}},
  journal = {Nature Biotechnology}
}



@article{Zhou2018,
  title = {A Brief Introduction to Weakly Supervised Learning},
  author = {Zhou, Zhi-Hua},
  year = {2018},
  month = jan,
  volume = {5},
  pages = {44--53},
  issn = {2095-5138, 2053-714X},
  doi = {10.1093/nsr/nwx106},
  abstract = {Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.},
  file = {/Users/twalter/Zotero/storage/8NJE2JVQ/Zhou - 2018 - A brief introduction to weakly supervised learning.pdf},
  journal = {National Science Review},
  language = {en},
  number = {1}
}



@inproceedings{Oquab2015,
  title = {Is Object Localization for Free? - {{Weakly}}-Supervised Learning with Convolutional Neural Networks},
  shorttitle = {Is Object Localization for Free?},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  year = {2015},
  month = jun,
  pages = {685--694},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298668},
  abstract = {Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classification that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classification and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We find that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.},
  file = {/Users/twalter/Zotero/storage/XBXNA44N/Oquab et al. - 2015 - Is object localization for free - Weakly-supervis.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}




@inproceedings{Boyd2020,
  title = {Experimentally-{{Generated Ground Truth}} for {{Detecting Cell Types}} in an {{Image}}-{{Based Immunotherapy Screen}}},
  booktitle = {2020 {{IEEE}} 17th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {{J. Boyd} and {Z. Gouveia} and {F. Perez} and {T. Walter}},
  year = {2020},
  month = apr,
  pages = {886--890},
  doi = {10.1109/ISBI45749.2020.9098696},
  abstract = {Chimeric antigen receptor is an immunotherapy whereby T lymphocytes are engineered to selectively attack cancer cells. Image-based screens of CAR-T cells, combining phase contrast and fluorescence microscopy, suffer from the gradual quenching of the fluorescent signal, making the reliable monitoring of cell populations across time-lapse imagery difficult. We propose to leverage the available fluorescent markers as an experimentally-generated ground truth, without recourse to manual annotation. With some simple image processing, we are able to segment and assign cell type classes automatically. This ground truth is sufficient to train a neural object detection system from the phase contrast signal alone, potentially eliminating the need for the cumbersome fluorescent markers. This approach will underpin the development of cheap and robust microscope-based protocols to quantify CAR-T activity against tumor cells in vitro.},
  isbn = {1945-8452},
  keywords = {Agriculture,Computer architecture,deep learning,High Content Screening,Microprocessors,Microscopy,object detection,Object detection,phase contrast microscopy,Sociology,Statistics}
}



@article{Christiansen2018,
  title = {In {{Silico Labeling}}: {{Predicting Fluorescent Labels}} in {{Unlabeled Images}}},
  author = {Christiansen, Eric M. and Yang, Samuel J. and Ando, D. Michael and Javaherian, Ashkan and Skibinski, Gaia and Lipnick, Scott and Mount, Elliot and O'Neil, Alison and Shah, Kevan and Lee, Alicia K. and Goyal, Piyush and Fedus, William and Poplin, Ryan and Esteva, Andre and Berndl, Marc and Rubin, Lee L. and Nelson, Philip and Finkbeiner, Steven},
  year = {2018},
  volume = {173},
  pages = {792-803.e19},
  publisher = {{Elsevier Inc.}},
  issn = {10974172},
  doi = {10.1016/j.cell.2018.03.040},
  abstract = {Microscopy is a central method in life sciences. Many popular methods, such as antibody labeling, are used to add physical fluorescent labels to specific cellular constituents. However, these approaches have significant drawbacks, including inconsistency; limitations in the number of simultaneous labels because of spectral overlap; and necessary perturbations of the experiment, such as fixing the cells, to generate the measurement. Here, we show that a computational machine-learning approach, which we call ``in silico labeling'' (ISL), reliably predicts some fluorescent labels from transmitted-light images of unlabeled fixed or live biological samples. ISL predicts a range of labels, such as those for nuclei, cell type (e.g., neural), and cell state (e.g., cell death). Because prediction happens in silico, the method is consistent, is not limited by spectral overlap, and does not disturb the experiment. ISL generates biological measurements that would otherwise be problematic or impossible to acquire. In silico labeling, a machine-learning approach, reliably infers fluorescent measurements from transmitted-light images of unlabeled fixed or live biological samples.},
  archivePrefix = {arXiv},
  eprint = {cond-mat/0411586},
  eprinttype = {arxiv},
  file = {/Users/twalter/Zotero/storage/JD9VHTUL/Christiansen et al. - 2018 - In Silico Labeling Predicting Fluorescent Labels in Unlabeled Images.pdf},
  isbn = {8415683111},
  journal = {Cell},
  keywords = {cancer,computer vision,deep learning,machine learning,microscopy,neuroscience,stem cells},
  number = {3},
  pmid = {29656897}
}




@article{Ouyang2019a,
  title = {Analysis of the {{Human Protein Atlas Image Classification}} Competition},
  author = {Ouyang, Wei and Winsnes, Casper F. and Hjelmare, Martin and Cesnik, Anthony J. and {\AA}kesson, Lovisa and Xu, Hao and Sullivan, Devin P. and Dai, Shubin and Lan, Jun and Jinmo, Park and Galib, Shaikat M. and Henkel, Christof and Hwang, Kevin and Poplavskiy, Dmytro and Tunguz, Bojan and Wolfinger, Russel D. and Gu, Yinzheng and Li, Chuanpeng and Xie, Jinbin and Buslov, Dmitry and Fironov, Sergei and Kiselev, Alexander and Panchenko, Dmytro and Cao, Xuan and Wei, Runmin and Wu, Yuanhao and Zhu, Xun and Tseng, Kuan-Lun and Gao, Zhifeng and Ju, Cheng and Yi, Xiaohan and Zheng, Hongdong and Kappel, Constantin and Lundberg, Emma},
  year = {2019},
  month = dec,
  volume = {16},
  pages = {1254--1261},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0658-6},
  file = {/Users/twalter/Zotero/storage/SSADH7GW/Ouyang et al. - 2019 - Analysis of the Human Protein Atlas Image Classifi.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {12}
}



