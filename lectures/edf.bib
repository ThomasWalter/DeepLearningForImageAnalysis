
@article{sikora_low_1995,
	title = {Low complexity shape-adaptive {DCT} for coding of arbitrarily shaped image segments},
	volume = {7},
	journal = {Signal Processing : Image Communication},
	author = {Sikora, T.},
	year = {1995},
	pages = {381--395},
}

@book{bartlett_face_2001,
	title = {Face image analysis by unsupervised learning},
	publisher = {Kluwer academic publishers},
	author = {Bartlett, Marian Stewart},
	year = {2001},
}

@inproceedings{v._morard_region_2011,
	address = {Etats-Unis},
	title = {Region growing structuring elements and new operators based on their shape},
	booktitle = {Signal and {Image} {Processing} ({SIP} 2011)},
	publisher = {ACTA Press},
	author = {{V. Morard} and {E. Decencière} and {P. Dokladal}},
	year = {2011},
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	urldate = {2015-10-22},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	month = oct,
	year = {2011},
	pages = {2825−2830},
	file = {Scikit-learn\: Machine Learning in Python:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/NNHRQR3D/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@inproceedings{ohtake_ridge-valley_2004,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '04},
	title = {Ridge-valley {Lines} on {Meshes} via {Implicit} {Surface} {Fitting}},
	url = {http://doi.acm.org/10.1145/1186562.1015768},
	doi = {10.1145/1186562.1015768},
	abstract = {We propose a simple and effective method for detecting view-and scale-independent ridge-valley lines defined via first- and second-order curvature derivatives on shapes approximated by dense triangle meshes. A high-quality estimation of high-order surface derivatives is achieved by combining multi-level implicit surface fitting and finite difference approximations. We demonstrate that the ridges and valleys are geometrically and perceptually salient surface features, and, therefore, can be potentially used for shape recognition, coding, and quality evaluation purposes.},
	urldate = {2014-12-04},
	booktitle = {{ACM} {SIGGRAPH} 2004 {Papers}},
	publisher = {ACM},
	author = {Ohtake, Yutaka and Belyaev, Alexander and Seidel, Hans-Peter},
	year = {2004},
	keywords = {curvature extrema, implicit surface fitting, ridges},
	pages = {609--612},
}

@article{mainsah_effects_1994,
	title = {The effects of quantization on {3D} topography characterization},
	number = {5},
	journal = {Meas. Sci. Technol.},
	author = {Mainsah, E. and Stout, K. J and Sullivan, P. J},
	year = {1994},
	pages = {172--181},
}

@inproceedings{stawiaski_region_2007,
	address = {Saint Etienne, France},
	title = {Region merging via graph-cuts},
	booktitle = {The 12th {International} {Congress} for {Stereology} ({ICS} {XII})},
	author = {Stawiaski, Jean and Decencière, Etienne},
	year = {2007},
}

@phdthesis{aubert_proprietes_1999,
	title = {Propriétés optiques des surfaces rugueuses aléatoires},
	school = {Ecole Nationale Supérieure des Mines de Paris},
	author = {Aubert, A.},
	year = {1999},
}

@techreport{muralikrishnan_valley_2000,
	type = {Status report},
	title = {Valley suppression algorithms},
	url = {http://www.coe.uncc.edu/ bmuralik/academic/docs/valley-dec-2000.pdf},
	number = {N-05/95/MM},
	institution = {Center for Precision Metrology, UNC Charlote},
	author = {Muralikrishnan, B. and Raja, J.},
	year = {2000},
}

@incollection{matheron_splines_1981,
	title = {Splines and kriging : their formal equivalence},
	booktitle = {Down-to-earht statistics : solutions looking for geological problems},
	publisher = {Syracuse university. Geology contributions},
	author = {Matheron, G.},
	editor = {Merrian, D. F},
	year = {1981},
	pages = {77--95},
}

@article{boulanger_motifs_1992,
	title = {The motifs method: an interesting complement to {ISO} parameters for some functional problems},
	volume = {32},
	number = {1/2},
	journal = {International Journal of Machine Tools and Manufacture},
	author = {Boulanger, J.},
	year = {1992},
	pages = {203--209},
}

@techreport{matheron_schema_1968,
	title = {Schéma booléen séquentiel de partition aléatoire},
	number = {N-83},
	institution = {Centre de Morphologie Mathématique, Ecole des Mines de Paris},
	author = {Matheron, G.},
	year = {1968},
}

@article{matheron_intrinsic_1973,
	title = {The intrinsic random functions, and their applications},
	volume = {5},
	journal = {Advances in Applied Probability},
	author = {Matheron, Georges},
	year = {1973},
	pages = {439--468},
}

@article{yokoi_topological_1973,
	title = {Topological properties in digital binary pictures},
	volume = {4},
	journal = {Systems Comput. Controls},
	author = {Yokoi, S. and Toriwaki, J. and Fukumura, T.},
	year = {1973},
	pages = {32--40},
}

@article{schunck_image_1986,
	title = {The image flow constraint equation},
	volume = {35},
	number = {1},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Schunck, B. G},
	year = {1986},
	pages = {20--46},
}

@inproceedings{yoshizawa_fast_2005,
	address = {New York, NY, USA},
	series = {{SPM} '05},
	title = {Fast and {Robust} {Detection} of {Crest} {Lines} on {Meshes}},
	isbn = {1-59593-015-9},
	url = {http://doi.acm.org/10.1145/1060244.1060270},
	doi = {10.1145/1060244.1060270},
	abstract = {We propose a fast and robust method for detecting crest lines on surfaces approximated by dense triangle meshes. The crest lines, salient surface features defined via first- and second-order curvature derivatives, are widely used for shape matching and interrogation purposes. Their practical extraction is difficult because it requires good estimation of high-order surface derivatives. Our approach to the crest line detection is based on estimating the curvature tensor and curvature derivatives via local polynomial fitting.Since the crest lines are not defined in the surface regions where the surface focal set (caustic) degenerates, we introduce a new thresholding scheme which exploits interesting relationships between curvature extrema, the so-called MVS functional of Moreton and Sequin, and Dupin cyclides,An application of the crest lines to adaptive mesh simplification is also considered.},
	urldate = {2014-12-04},
	booktitle = {Proceedings of the 2005 {ACM} {Symposium} on {Solid} and {Physical} {Modeling}},
	publisher = {ACM},
	author = {Yoshizawa, Shin and Belyaev, Alexander and Seidel, Hans-Peter},
	year = {2005},
	pages = {227--232},
}

@article{stout_characterization_1982,
	title = {The characterization of internal combustion engine bores},
	volume = {83},
	journal = {Wear},
	author = {Stout, K. J},
	year = {1982},
	pages = {311--326},
}

@inproceedings{zhang_application_2011,
	title = {Application of the {Morphological} {Ultimate} {Opening} to the {Detection} of {Microaneurysms} on {Eye} {Fundus} {Images} from {Clinical} {Databases}},
	booktitle = {{ICS}'13},
	author = {Zhang, Xiwei and Thibault, Guillaume and Decencière, Etienne},
	year = {2011},
}

@article{levoy_efficient_1990,
	title = {Efficient {Ray} {Tracing} of {Volume} {Data}},
	volume = {9},
	number = {3},
	journal = {ACM Transactions on Graphics},
	author = {Levoy, Marc},
	month = jul,
	year = {1990},
	pages = {245--261},
}

@article{dong_reference_1995,
	title = {Reference planes for the assessment of surface roughness in three dimensions},
	volume = {35},
	number = {2},
	journal = {International Journal of Machine Tools and Manufacture},
	author = {Dong, W. P and Mainsah, E. and Stout, K. J},
	year = {1995},
	pages = {263--271},
}

@misc{matheron_surpotentes_1982,
	title = {Surpotentes et sous-potentes},
	publisher = {Ecole des Mines de Paris},
	author = {Matheron, Georges},
	year = {1982},
}

@article{stawiaski_region_2008,
	title = {Region merging via graph cuts},
	volume = {27},
	number = {1},
	journal = {Image Analysis and Stereology},
	author = {Stawiaski, J. and Decencière, E.},
	year = {2008},
	pages = {39--46},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1023/A%3A1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2015-10-22},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computing Methodologies, Language Translation and Linguistics, Simulation and Modeling, classification, ensemble, regression},
	pages = {5--32},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/JNW4PZBJ/A1010933404324.html:text/html},
}

@phdthesis{pardas_segmentacion_1994,
	title = {Segmentación morfológica de secuencias de imágenes: aplicación a la codificación},
	school = {Universitat Politècnica de Catalunya},
	author = {Pardàs, M.},
	month = dec,
	year = {1994},
}

@article{caciu_numerical_2005,
	title = {Numerical analysis of a {3D} hydrodynamic contact},
	volume = {51},
	number = {12},
	journal = {International Journal for numerical methods in fluids},
	author = {Caciu, Costin A and Decencière, Etienne},
	year = {2005},
	pages = {1355--1377},
}

@inproceedings{louppe_understanding_2013,
	title = {Understanding variable importances in forests of randomized trees},
	url = {http://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	urldate = {2013-12-17},
	author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
	year = {2013},
	pages = {431--439},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/VHV3C66X/4928-understanding-variable-importances-in-forests-of-randomized-trees.html:text/html},
}

@inproceedings{schulze_properties_1991,
	address = {San Jose, CA, USA},
	title = {Some properties of the two-dimensional pseudomedian filter},
	url = {http://link.aip.org/link/?PSI/1451/48/1&Agg=doi},
	doi = {10.1117/12.44315},
	urldate = {2011-04-11},
	booktitle = {Proceedings of {SPIE}},
	author = {Schulze, Mark A. and Pearce, John A.},
	year = {1991},
	pages = {48--57},
}

@article{schmitt_strong_1994,
	title = {Strong and weak convex hulls in non-{Euclidean} metric: theory and application},
	volume = {15},
	issn = {0167-8655},
	shorttitle = {Strong and weak convex hulls in non-{Euclidean} metric},
	url = {http://www.sciencedirect.com/science/article/pii/0167865594901570},
	doi = {10.1016/0167-8655(94)90157-0},
	abstract = {The notion of convexity is usually defined in the plane supplied with the Euclidean metric. This paper examines what remains if we equip the plane with a distance induced by a norm which is not necessarily the Euclidean one. The basic properties of the geodesic arcs according to these non-Euclidean metrics are stated. In some cases there exists more than one geodesic arc between two points. The two associated notions of convexity, both strong and weak, are the presented. The relationships between the notion of weak convex hull and the limit of closings of increasing size are stated. Finally an application in binary image pattern recognition is described.},
	number = {9},
	urldate = {2014-11-21},
	journal = {Pattern Recognition Letters},
	author = {Schmitt, Michel and Mattioli, Juliette},
	month = sep,
	year = {1994},
	keywords = {Convexity, Geodesic arcs, Morphological closing, Weakly convex hull, mathematical morphology},
	pages = {943--947},
	file = {ScienceDirect Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/PF5HJHAS/0167865594901570.html:text/html},
}

@inproceedings{kunt_second-generation_1985,
	title = {Second-generation image coding techniques},
	volume = {73},
	booktitle = {Proceedings of the {IEEE}},
	author = {Kunt, M. and Ikonomopoulos, A. and Kocher, M.},
	year = {1985},
	pages = {549--574},
}

@misc{maisonneuve_sur_1982,
	title = {Sur le partage des eaux},
	author = {Maisonneuve, Francis},
	year = {1982},
	note = {Int. Rep CGMM},
}

@book{matheron_elements_1967,
	title = {Éléments pour une théorie des milieux poreux},
	publisher = {Masson, Paris},
	author = {Matheron, G.},
	year = {1967},
}

@article{hoover_experimental_1996,
	title = {An experimental comparison of range image segmentation algorithms},
	volume = {18},
	issn = {0162-8828},
	doi = {10.1109/34.506791},
	abstract = {A methodology for evaluating range image segmentation algorithms is proposed. This methodology involves (1) a common set of 40 laser range finder images and 40 structured light scanner images that have manually specified ground truth and (2) a set of defined performance metrics for instances of correctly segmented, missed, and noise regions, over- and under-segmentation, and accuracy of the recovered geometry. A tool is used to objectively compare a machine generated segmentation against the specified ground truth. Four research groups have contributed to evaluate their own algorithm for segmenting a range image into planar patches.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hoover, A. and Jean-Baptiste, G. and Jiang, X. and Flynn, P.J. and Bunke, H. and Goldgof, D.B. and Bowyer, K. and Eggert, D.W. and Fitzgibbon, A. and Fisher, R.B.},
	month = jul,
	year = {1996},
	keywords = {Artificial Intelligence, Geometrical optics, Geometry, Image segmentation, Laser noise, Measurement standards, Pixel, Shape, Testing, computer vision, image classification, laser range finder images, laser ranging, over-segmentation, performance metrics, planar patches, range image segmentation algorithms, recovered geometry, structured light scanner images, under-segmentation},
	pages = {673--689},
	file = {IEEE Xplore Abstract Record:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/8PT8NRZA/abs_all.html:text/html},
}

@article{setos_fox_1996,
	title = {The {Fox} {Movietone} news preservation project: an introduction},
	volume = {105},
	number = {9},
	journal = {SMPTE Journal},
	author = {Setos, A. G},
	year = {1996},
	pages = {532--536},
}

@article{tholath_three-dimensional_1999,
	title = {Three-dimensional filtering of engineering surfaces using envelope system},
	volume = {23},
	journal = {Precision engineering},
	author = {Tholath, J. and Radhakrishnan, V.},
	year = {1999},
	pages = {221--228},
}

@book{matheron_random_1975,
	title = {Random sets and integral geometry},
	publisher = {Wiley, New York},
	author = {Matheron, G.},
	year = {1975},
}

@book{bowden_friction_1950,
	title = {The friction and lubrication of solids, part {I}},
	publisher = {Clarendon, Oxford},
	author = {Bowden, F. P and Tabor, D.},
	year = {1950},
}

@article{stout_surface_1984,
	title = {Surface topography of cylinder bores - the relationship between manufacture, characterization and function},
	volume = {95},
	journal = {Wear},
	author = {Stout, K. J},
	year = {1984},
	pages = {111--125},
}

@book{bowden_friction_1964,
	title = {The friction and lubrication of solids, part {II}},
	publisher = {Clarendon, Oxford},
	author = {Bowden, F. P and Tabor, D.},
	year = {1964},
}

@article{decenciere_feedback_2014,
	title = {{FEEDBACK} {ON} {A} {PUBLICLY} {DISTRIBUTED} {IMAGE} {DATABASE}: {THE} {MESSIDOR} {DATABASE}},
	volume = {33},
	copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
	issn = {1854-5165},
	shorttitle = {{FEEDBACK} {ON} {A} {PUBLICLY} {DISTRIBUTED} {IMAGE} {DATABASE}},
	url = {http://www.ias-iss.org/ojs/IAS/article/view/1155},
	doi = {10.5566/ias.1155},
	abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
	language = {en},
	number = {3},
	urldate = {2014-11-20},
	journal = {Image Analysis \& Stereology},
	author = {Decencière, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, Béatrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, Béatrice and Klein, Jean-Claude},
	month = aug,
	year = {2014},
	keywords = {Diabetic Retinopathy, Image Processing, Messidor, image database},
	pages = {231--234},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/CI7Q85XF/1155.html:text/html},
}

@article{lantuejoul_geodesic_1984,
	title = {Geodesic methods in quantitative image analysis},
	volume = {17},
	number = {2},
	journal = {Pattern Recognition},
	author = {Lantuéjoul, Christian and Maisonneuve, F.},
	year = {1984},
	pages = {177--187},
}

@article{edmonds_theoretical_1972,
	title = {Theoretical improvements in algorithmic efficiency for network flow problems},
	volume = {19},
	number = {2},
	journal = {J. ACM},
	author = {Edmonds, J. and Karp, R. M},
	year = {1972},
	pages = {248--264},
}

@article{sacerdotti_mathematical_2000,
	title = {Mathematical {Modelling} of three-dimensional surface topography in autobody manufacture - {The} {State} of the {Art}},
	volume = {214},
	journal = {Proceedings of the Institution of Mechanical Engineers, PartB},
	author = {Sacerdotti, F. and Griffiths, B. J and Benati, F. and Butler, C.},
	year = {2000},
	pages = {811--819},
}

@techreport{decenciere_topological_2004,
	type = {Rapport {Pocket} {Multimedia}},
	title = {Topological sampling of binary images},
	number = {N 01/05/MM},
	institution = {Ecole des Mines de Paris, Centre de Morphologie Mathématique},
	author = {Decencière, Etienne and Bilodeau, Michel},
	month = dec,
	year = {2004},
}

@article{belongie_shape_2002,
	title = {Shape matching and object recognition using shape contexts},
	volume = {24},
	number = {24},
	journal = {i3epami},
	author = {Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
	month = apr,
	year = {2002},
	pages = {509--522},
}

@article{hoffman_segmentation_1987,
	title = {Segmentation and {Classification} of {Range} {Images}},
	volume = {PAMI-9},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.1987.4767955},
	abstract = {The recognition of objects in three-dimensional space is a desirable capability of a computer vision system. Range images, which directly measure 3-D surface coordinates of a scene, are well suited for this task. In this paper we report a procedure to detect connected planar, convex, and concave surfaces of 3-D objects. This is accomplished in three stages. The first stage segments the range image into “surface patches” by a square error criterion clustering algorithm using surface points and associated surface normals. The second stage classifies these patches as planar, convex, or concave based on a non-parametric statistical test for trend, curvature values, and eigenvalue analysis. In the final stage, boundaries between adjacent surface patches are classified as crease or noncrease edges, and this information is used to merge compatible patches to produce reasonable faces of the object(s). This procedure has been successfully applied to a large number of real and synthetic images, four of which we present in this paper.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hoffman, R. and Jain, Anil K.},
	month = sep,
	year = {1987},
	keywords = {Clustering algorithms, Eigenvalues and eigenfunctions, Face detection, Image segmentation, Layout, Reflectivity, Surface fitting, Testing, clustering, computer vision, curvature, decision tree, eigenvalues, nonparametric statistical tests, object detection, range images, surface normals, surface patches},
	pages = {608--620},
	file = {IEEE Xplore Abstract Record:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/RWTSVKHD/abs_all.html:text/html},
}

@article{caciu_parametric_nodate,
	title = {Parametric optimisation of periodic textured surfaces for friction reduction in combustion engines},
	journal = {STLE Tribology Transactions (accepted Feb. 2008)},
	author = {Caciu, Costin A and Decencière, Etienne and Jeulin, Dominique},
}

@phdthesis{lerallut_modelisation_2006,
	title = {Modélisation et {Interprétation} d'{Images} à l'{Aide} de {Graphes}},
	school = {Ecole des Mines de Paris, Paris},
	author = {Lerallut, Romain},
	year = {2006},
}

@phdthesis{casas_pla_image_1996,
	title = {Image compression based on perceptual coding techniques},
	school = {Universitat Politècnica de Catalunya},
	author = {Casas Pla, J. R},
	month = mar,
	year = {1996},
}

@article{von_weingraber_suitability_1957,
	title = {Suitability of the envelope line as a reference standard for measuring roughness},
	volume = {11},
	journal = {Microtecnic},
	author = {von Weingraber, H.},
	year = {1957},
	pages = {6--17},
}

@book{matheron_traite_1963,
	title = {Traité de géostatistique appliquée, tome {II}},
	publisher = {Editions Techniques, Paris},
	author = {Matheron, Georges},
	year = {1963},
}

@article{caciu_numerical_nodate,
	title = {Numerical analysis of the consequences of rugosity modifications in {3D} hydrodynamic contacts},
	journal = {STLE Tribology Transactions (accepted Feb. 2008)},
	author = {Caciu, Costin A and Decencière, Etienne and Jeulin, Dominique},
}

@book{henry_sismique_1997,
	title = {Sismique réflexion : principes et développements},
	publisher = {Technip},
	author = {Henry, G.},
	year = {1997},
}

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
	volume = {65},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	shorttitle = {The perceptron},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {*Brain, *Cognition, *Memory, Nervous System},
	pages = {386--408},
}

@inproceedings{lecun_procedure_1985,
	title = {Une procedure d'apprentissage pour reseau a seuil asymmetrique ({A} learning scheme for asymmetric threshold networks)},
	url = {https://nyu.pure.elsevier.com/en/publications/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l},
	language = {English (US)},
	urldate = {2016-06-06},
	booktitle = {proceedings of {Cognitiva} 85},
	author = {LeCun, Yann},
	year = {1985},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/3Z77FDMX/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l.html:text/html},
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2016-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Snapshort:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/945EI2D3/4824-imagenet-classification-w.html:text/html},
}

@inproceedings{nguyen_deep_2015,
	title = {Deep neural networks are easily fooled: {High} confidence predictions for unrecognizable images},
	shorttitle = {Deep neural networks are easily fooled},
	doi = {10.1109/CVPR.2015.7298640},
	abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call “fooling images” (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Nguyen, A. and Yosinski, J. and Clune, J.},
	month = jun,
	year = {2015},
	keywords = {Biomedical imaging, computer vision, image classification, neural nets, convolution, evolutionary computation, object recognition, DNNs, ImageNet datasets, MNIST datasets, convolutional neural networks, deep neural networks, evolutionary algorithms, fooling images, gradient ascent, image labeling, pattern-recognition tasks, recognizable objects, unrecognizable images, visual classification problems, Keyboards, Volcanoes},
	pages = {427--436},
	file = {IEEE Xplore Abstract Record:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/HMXF785X/abs_all.html:text/html},
}

@article{arganda-carreras_crowdsourcing_2015,
	title = {Crowdsourcing the creation of image segmentation algorithms for connectomics},
	volume = {9},
	issn = {1662-5129},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4633678/},
	doi = {10.3389/fnana.2015.00142},
	abstract = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic (EM) images of the brain. Participants submitted boundary maps predicted for a test set of images, and were scored based on their agreement with a consensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This “deep learning” approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from cooperation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test dataset. Retrospective evaluation of the challenge scoring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
	urldate = {2016-06-30},
	journal = {Front Neuroanat},
	author = {Arganda-Carreras, Ignacio and Turaga, Srinivas C. and Berger, Daniel R. and Cireşan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, Jürgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M. and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D. and Bas, Erhan and Uzunbas, Mustafa G. and Cardona, Albert and Schindelin, Johannes and Seung, H. Sebastian},
	month = nov,
	year = {2015},
	pmid = {26594156},
	pmcid = {PMC4633678},
}

@inproceedings{mehta_selective_2014,
	title = {Selective quantifiable facial assessment of aging},
	doi = {10.1109/PAHCE.2014.6849637},
	abstract = {Imaging represents a technique of choice in evaluation of the visual effects of aging and associated with that occurrence of wrinkles. In particular, skin wrinkles typically occur due to aging processes, including loss of body mass, sun damage, smoking, squinting and other factors. They represent a clear and easily accessible indicator of changes. As such, one can also acquire useful information about the aging process in skin by analyzing the wrinkles. To this end, we utilize a combination of numerical techniques, including color quantization, image segmentation, and various edge detection algorithms in order to perform automated wrinkle counting and wrinkle density calculations. As a more appropriate alternative to chronological age, such a methodology allows us to come up with quantifiable measures for skin aging, which may be used for performing statistics and extracting general patterns associated with physiological aging of the skin, as well as extending such numerical techniques for other biomedical applications in which distinct topological features contain important information about biological processes. Different subjects were used to test the techniques and extract aging patterns by examining the skin immediately underneath the lower eyelid as our region of interest. Numerically processed photographs, included counting the number of wrinkles meeting predefined threshold conditions, and calculating the corresponding wrinkle density for a given subject and particular conditions. Applicability and practicality of different edge detection methods were also a part of the studies as demonstrated.},
	booktitle = {Health {Care} {Exchanges} ({PAHCE}), 2014 {Pan} {American}},
	author = {Mehta, G. and Druzgalski, C.},
	month = apr,
	year = {2014},
	keywords = {Aging, Edge detection, Feature extraction, Gray-scale, Image color analysis, Image edge detection, Image segmentation, Skin Aging, Statistics, biomedical optical imaging, image colour analysis, medical image processing, skin, numerical analysis, biomedical applications, chronological age, color quantization, edge detection algorithms, edge detection methods, facial assessment, numerical techniques, pattern extraction, photographs, skin wrinkle counting, skin wrinkle density calculations, topological features, visual effect evaluation, Cameras, Physiology, Skin wrinkles},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/UXIASNCQ/abs_all.html:text/html},
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2016-09-12},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/QSXCC3ZF/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html:text/html},
}

@incollection{ronneberger_u-net:_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	copyright = {©2015 Springer International Publishing Switzerland},
	shorttitle = {U-{Net}},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	language = {en},
	number = {9351},
	urldate = {2016-09-12},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	month = oct,
	year = {2015},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Graphics, Image Processing and Computer Vision, Pattern recognition, Imaging / Radiology, Health Informatics},
	pages = {234--241},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/BBUWJFJV/978-3-319-24574-4_28.html:text/html},
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. We show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/},
	urldate = {2016-09-12},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1511.00561 PDF:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/C2EXEX9G/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/ZC76VUF8/1511.html:text/html},
}

@article{everingham_pascal_2014,
	title = {The {Pascal} {Visual} {Object} {Classes} {Challenge}: {A} {Retrospective}},
	volume = {111},
	issn = {0920-5691, 1573-1405},
	shorttitle = {The {Pascal} {Visual} {Object} {Classes} {Challenge}},
	url = {http://link.springer.com/article/10.1007/s11263-014-0733-5},
	doi = {10.1007/s11263-014-0733-5},
	abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community’s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
	language = {en},
	number = {1},
	urldate = {2016-09-27},
	journal = {Int J Comput Vis},
	author = {Everingham, Mark and Eslami, S. M. Ali and Gool, Luc Van and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2014},
	pages = {98--136},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/RX7JR9Z4/s11263-014-0733-5.html:text/html},
}

@incollection{ganin_n^4-fields:_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {N{\textasciicircum}4-{Fields}: {Neural} {Network} {Nearest} {Neighbor} {Fields} for {Image} {Transforms}},
	copyright = {©2015 Springer International Publishing Switzerland},
	isbn = {978-3-319-16807-4 978-3-319-16808-1},
	shorttitle = {N{\textasciicircum}4-{Fields}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-16808-1_36},
	abstract = {We propose a new architecture for difficult image processing operations, such as natural edge detection or thin object segmentation. The architecture is based on a simple combination of convolutional neural networks with the nearest neighbor search. We focus our attention on the situations when the desired image transformation is too hard for a neural network to learn explicitly. We show that in such situations the use of the nearest neighbor search on top of the network output allows to improve the results considerably and to account for the underfitting effect during the neural network training. The approach is validated on three challenging benchmarks, where the performance of the proposed architecture matches or exceeds the state-of-the-art.},
	language = {en},
	number = {9004},
	urldate = {2016-09-28},
	booktitle = {Computer {Vision} -- {ACCV} 2014},
	publisher = {Springer International Publishing},
	author = {Ganin, Yaroslav and Lempitsky, Victor},
	editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
	month = nov,
	year = {2014},
	doi = {10.1007/978-3-319-16808-1_36},
	keywords = {Artificial Intelligence (incl. Robotics), Image Processing and Computer Vision, Pattern recognition, Health Informatics, Information Systems Applications (incl. Internet), Information Storage and Retrieval},
	pages = {536--551},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/U7PJD35G/978-3-319-16808-1_36.html:text/html},
}

@incollection{zilly_boosting_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Boosting {Convolutional} {Filters} with {Entropy} {Sampling} for {Optic} {Cup} and {Disc} {Image} {Segmentation} from {Fundus} {Images}},
	copyright = {©2015 Springer International Publishing Switzerland},
	isbn = {978-3-319-24887-5 978-3-319-24888-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-24888-2_17},
	abstract = {We propose a novel convolutional neural network (CNN) based method for optic cup and disc segmentation. To reduce computational complexity, an entropy based sampling technique is introduced that gives superior results over uniform sampling. Filters are learned over several layers with the output of previous layers serving as the input to the next layer. A softmax logistic regression classifier is subsequently trained on the output of all learned filters. In several error metrics, the proposed algorithm outperforms existing methods on the public DRISHTI-GS data set.},
	language = {en},
	number = {9352},
	urldate = {2016-09-28},
	booktitle = {Machine {Learning} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Zilly, Julian G. and Buhmann, Joachim M. and Mahapatra, Dwarikanath},
	editor = {Zhou, Luping and Wang, Li and Wang, Qian and Shi, Yinghuan},
	month = oct,
	year = {2015},
	doi = {10.1007/978-3-319-24888-2_17},
	keywords = {Artificial Intelligence (incl. Robotics), Image Processing and Computer Vision, Pattern recognition, Health Informatics, Data Mining and Knowledge Discovery},
	pages = {136--143},
	file = {Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/DEHKRC84/978-3-319-24888-2_17.html:text/html},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	shorttitle = {Deep learning in neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \&amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2016-11-24},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {unsupervised learning, deep learning, evolutionary computation, Supervised learning, Reinforcement learning},
	pages = {85--117},
}

@article{fukushima_neural_1979,
	title = {Neural {Network} {Model} for a {Mechanism} of {Pattern} {Recognition} {Unaffected} by {Shift} in {Position}- {Neocognitron}},
	volume = {62},
	number = {10},
	journal = {ELECTRON. \& COMMUN. JAPAN},
	author = {Fukushima, Kunihiko},
	year = {1979},
	pages = {11--18},
}

@article{farabet_learning_2013,
	title = {Learning {Hierarchical} {Features} for {Scene} {Labeling}},
	volume = {35},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2012.231},
	abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Farabet, C. and Couprie, C. and Najman, L. and LeCun, Y.},
	month = aug,
	year = {2013},
	keywords = {Context, Feature extraction, Image edge detection, Image segmentation, Labeling, image classification, image texture, trees (mathematics), deep learning, image labeling, Transforms, shape recognition, Barcelona dataset, SIFT flow dataset, Stanford background dataset, contextual information capturing, dense feature vector extraction, hierarchical feature learning, image pixel labeling, multiple size region encoding, multiscale convolutional network, near-record accuracy, object category, scene labeling, segmentation components, segmentation tree, shape information capturing, texture information capturing, Accuracy, Vectors, Convolutional networks, scene parsing},
	pages = {1915--1929},
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2016-11-29},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_fully_2016,
	title = {Fully {Convolutional} {Instance}-aware {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1611.07709},
	abstract = {We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at {\textbackslash}url\{https://github.com/daijifeng001/TA-FCN\}.},
	urldate = {2016-11-29},
	journal = {arXiv:1611.07709 [cs]},
	author = {Li, Yi and Qi, Haozhi and Dai, Jifeng and Ji, Xiangyang and Wei, Yichen},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.07709 PDF:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/EMPAUK9Z/Li et al. - 2016 - Fully Convolutional Instance-aware Semantic Segmen.pdf:application/pdf;arXiv.org Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/IH4NWAJC/1611.html:text/html},
}

@inproceedings{ciresan_committee_2011,
	title = {A committee of neural networks for traffic sign classification},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6033458},
	urldate = {2017-01-03},
	booktitle = {Neural {Networks} ({IJCNN}), {The} 2011 {International} {Joint} {Conference} on},
	publisher = {IEEE},
	author = {Cireşan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, Jürgen},
	year = {2011},
	pages = {1918--1921},
}

@article{fukushima_neocognitron:_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/article/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
	language = {en},
	number = {4},
	urldate = {2017-01-04},
	journal = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	urldate = {2017-01-04},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@inproceedings{ciresan_mitosis_2013,
	title = {Mitosis {Detection} in {Breast} {Cancer} {Histology} {Images} with {Deep} {Neural} {Networks}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-40763-5_51},
	doi = {10.1007/978-3-642-40763-5_51},
	abstract = {We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.},
	language = {en},
	urldate = {2017-01-06},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2013},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Cireşan, Dan C. and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, Jürgen},
	month = sep,
	year = {2013},
	pages = {411--418},
}

@article{simonyan_very_2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	url = {http://arxiv.org/abs/1409.1556},
	urldate = {2017-01-09},
	journal = {arXiv preprint arXiv:1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	file = {[PDF] arxiv.org:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/DIXNC3WH/Simonyan et Zisserman - 2014 - Very deep convolutional networks for large-scale i.pdf:application/pdf;Snapshot:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/9GFJC2W9/1409.html:text/html},
}

@inproceedings{pang_cell_2010,
	title = {Cell {Nucleus} {Segmentation} in {Color} {Histopathological} {Imagery} {Using} {Convolutional} {Networks}},
	doi = {10.1109/CCPR.2010.5659313},
	abstract = {Recent studies have shown that convolutional networks can achieve a great deal of success in high-level vision problems such as objection recognition. In this paper, convolutional networks are used to solve a typical low-level image processing task, image segmentation. Here, the convolutional networks are trained using gradient descent techniques to solve the problem of segmenting the cell nuclei from the background in the histopathology images. Using a dataset with 58 H\&E stained breast cancer biopsy images, we find that the convolutional networks, with 3 hidden layers and 8 feature maps per hidden layer, provide superior performance to other pixel classification methods including FLDA and SVM. We also show two important properties of the convolutional networks as a segmentation method. First, as a machine learning approach, the convolution networks encode enough high-level domain-specific knowledge into the final segmentation strategy by learning the training data. Second, the convolutional networks can use appropriate amount of context information in segmenting by optimizing the weights of the filters in the networks through the learning process. In the end of this paper, several possible directions for future research are also proposed.},
	booktitle = {2010 {Chinese} {Conference} on {Pattern} {Recognition} ({CCPR})},
	author = {Pang, B. and Zhang, Y. and Chen, Q. and Gao, Z. and Peng, Q. and You, X.},
	month = oct,
	year = {2010},
	keywords = {Image color analysis, Image segmentation, Labeling, Pixel, image classification, image colour analysis, medical image processing, learning (artificial intelligence), object recognition, Convolutional networks, Computer architecture, support vector machines, FLDA, SVM, breast cancer biopsy images, cell nucleus segmentation, color histopathological imagery, feature maps per hidden layer, gradient descent techniques, high level domain specific knowledge, high level vision problem, histopathology images, machine learning approach, objection recognition, pixel classification methods, typical low level image processing task, Machine learning, Microprocessors},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/home/decencie/.mozilla/firefox/dv1q8ktg.default/zotero/storage/UF9B5NWP/5659313.html:text/html},
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	urldate = {2017-03-23},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{girshick_rich_2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
	urldate = {2017-03-23},
	booktitle = {Proceedings {CVPR}},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
}

@article{fukushima_neocognitron_2003,
	title = {Neocognitron for handwritten digit recognition},
	volume = {51},
	issn = {0925-2312, 1872-8286},
	language = {английский},
	journal = {Neurocomputing},
	author = {Fukushima, K.},
	year = {2003},
	keywords = {Handwritten Digit, Multi-Layered Network, Neocognitron, Neural Network Model, Visual Pattern Recognition},
	pages = {161--180},
}

@article{hu_learning_2017,
	title = {Learning to {Segment} {Every} {Thing}},
	url = {http://arxiv.org/abs/1711.10370},
	abstract = {Existing methods for object instance segmentation require all training instances to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to {\textasciitilde}100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models over a large set of categories for which all have box annotations, but only a small fraction have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We carefully evaluate our proposed approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.},
	urldate = {2017-12-05},
	journal = {arXiv:1711.10370 [cs]},
	author = {Hu, Ronghang and Dollár, Piotr and He, Kaiming and Darrell, Trevor and Girshick, Ross},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10370},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{redmon_yolo9000:_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2018-01-08},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_ssd:_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2018-01-08},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
}

@article{schlegl_unsupervised_2017,
	title = {Unsupervised {Anomaly} {Detection} with {Generative} {Adversarial} {Networks} to {Guide} {Marker} {Discovery}},
	url = {http://arxiv.org/abs/1703.05921},
	abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
	urldate = {2018-01-09},
	journal = {arXiv:1703.05921 [cs]},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05921},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
}

@inproceedings{hradis_convolutional_2015,
	title = {Convolutional neural networks for direct text deblurring},
	volume = {10},
	booktitle = {Proceedings of {BMVC}},
	author = {Hradiš, Michal and Kotera, Jan and Zemcík, Pavel and Šroubek, Filip},
	year = {2015},
}

@article{morard_parsimonious_2014,
	title = {Parsimonious {Path} {Openings} and {Closings}},
	volume = {23},
	issn = {1057-7149},
	doi = {10.1109/TIP.2014.2303647},
	abstract = {Path openings and closings are morphological tools used to preserve long, thin, and tortuous structures in gray level images. They explore all paths from a defined class, and filter them with a length criterion. However, most paths are redundant, making the process generally slow. Parsimonious path openings and closings are introduced in this paper to solve this problem. These operators only consider a subset of the paths considered by classical path openings, thus achieving a substantial speed-up, while obtaining similar results. In addition, a recently introduced 1D opening algorithm is applied along each selected path. Its complexity is linear with respect to the number of pixels, independent of the size of the opening. Furthermore, it is fast for any input data accuracy (integer or floating point) and works in stream. Parsimonious path openings are also extended to incomplete paths, i.e., paths containing gaps. Noise-corrupted paths can thus be processed with the same approach and complexity. These parsimonious operators achieve a several orders of magnitude speed-up. Examples are shown for incomplete path openings, where computing times are brought from minutes to tens of milliseconds, while obtaining similar results.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Morard, V. and Dokládal, P. and Decencière, E.},
	month = apr,
	year = {2014},
	keywords = {Complexity theory, Morphology, Noise, Robustness, filtering theory, mathematical morphology, Electronic mail, 1D opening algorithm, classical path openings, computing times, gray level images, incomplete path openings, incomplete paths, length criterion, long-structure preservation, magnitude speed-up, morphological tools, noise-corrupted paths, parsimonious operators, parsimonious path closings, parsimonious path openings, path filtering, substantial speed-up, thin-structure preservation, tortuous structure preservation, Heuristic algorithms, Timing, Path operators, complete and incomplete paths, curvilinear structures, image processing},
	pages = {1543--1555},
}

@article{bejnordi_diagnostic_2017,
	title = {Diagnostic {Assessment} of {Deep} {Learning} {Algorithms} for {Detection} of {Lymph} {Node} {Metastases} in {Women} {With} {Breast} {Cancer}},
	volume = {318},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2665774},
	doi = {10.1001/jama.2017.14585},
	abstract = {{\textless}h3{\textgreater}Importance{\textless}/h3{\textgreater}{\textless}p{\textgreater}Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin–stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists’ diagnoses in a diagnostic setting.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Design, Setting, and Participants{\textless}/h3{\textgreater}{\textless}p{\textgreater}Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Exposures{\textless}/h3{\textgreater}{\textless}p{\textgreater}Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Main Outcomes and Measures{\textless}/h3{\textgreater}{\textless}p{\textgreater}The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4\% [95\% CI, 64.3\%-80.4\%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95\% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884];\textit{P} \&lt; .001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95\% CI, 0.927-0.998] for the pathologist WOTC).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions and Relevance{\textless}/h3{\textgreater}{\textless}p{\textgreater}In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.{\textless}/p{\textgreater}},
	language = {en},
	number = {22},
	urldate = {2018-08-06},
	journal = {JAMA},
	author = {Bejnordi, Babak Ehteshami and Veta, Mitko and Diest, Paul Johannes van and Ginneken, Bram van and Karssemeijer, Nico and Litjens, Geert and Laak, Jeroen A. W. M. van der and Hermsen, Meyke and Manson, Quirine F. and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and Dijk, Marcory CRF van and Bult, Peter and Beca, Francisco and Beck, Andrew H. and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang-Jing and Heng, Pheng-Ann and Haß, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and Öner, Mustafa Ümit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee-Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Phoulady, Hady Ahmady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Venâncio, Rui},
	month = dec,
	year = {2017},
	pages = {2199--2210},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2018-10-05},
	journal = {Bulletin of Mathematical Biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	number = {2},
	urldate = {2018-10-08},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	month = jan,
	year = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
}

@article{cybenko_approximations_1989,
	title = {Approximations by superpositions of a sigmoidal function},
	volume = {2},
	url = {https://ci.nii.ac.jp/naid/10008983330/},
	urldate = {2018-10-08},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	year = {1989},
	pages = {183--192},
}

@inproceedings{werbos_applications_1982,
	series = {Lecture {Notes} in {Control} and {Information} {Sciences}},
	title = {Applications of advances in nonlinear sensitivity analysis},
	isbn = {978-3-540-39459-4},
	abstract = {The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems“ from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.},
	language = {en},
	booktitle = {System {Modeling} and {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Werbos, Paul J.},
	editor = {Drenick, R. F. and Kozin, F.},
	year = {1982},
	keywords = {Deterministic Optimization, Energy Information Administration, Evaluation Team, Sensitivity Analysis Method, Stochastic Optimization},
	pages = {762--770},
}

@inproceedings{pal_capsdemm:_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CapsDeMM}: {Capsule} {Network} for {Detection} of {Munro}’s {Microabscess} in {Skin} {Biopsy} {Images}},
	isbn = {978-3-030-00934-2},
	shorttitle = {{CapsDeMM}},
	abstract = {This paper presents an approach for automatic detection of Munro’s Microabscess in stratum corneum (SC) of human skin biopsy in order to realize a machine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils in presence of nucleated cells is solved using the recent advances of deep learning algorithms. Separation of SC layer, extraction of patches from the layer followed by classification of patches with respect to presence or absence of neutrophils form the basis of the overall approach which is effected through an integration of a U-Net based segmentation network and a capsule network for classification. The novel design of the present capsule net leads to a drastic reduction in the number of parameters without any noticeable compromise in the overall performance. The research further addresses the challenge of dealing with Mega-pixel images (in 10X) vis-à-vis Giga-pixel ones (in 40X). The promising result coming out of an experiment on a dataset consisting of 273 real-life images shows that a practical system is possible based on the present research. The implementation of our system is available at https://github.com/Anabik/CapsDeMM.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Pal, Anabik and Chaturvedi, Akshay and Garain, Utpal and Chandra, Aditi and Chatterjee, Raghunath and Senapati, Swapan},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	keywords = {Evaluation, Convolutional neural network, Biopsy image, Capsule network, Dataset, Munro’s microabscess, Neutrophil, Psoriasis histopathology, Segmentation, Stratum corneum, Super-pixel},
	pages = {389--397},
}

@inproceedings{masci_learning_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Learning} {Framework} for {Morphological} {Operators} {Using} {Counter}–{Harmonic} {Mean}},
	isbn = {978-3-642-38294-9},
	abstract = {We present a novel framework for learning morphological operators using counter-harmonic mean. It combines concepts from morphology and convolutional neural networks. A thorough experimental validation analyzes basic morphological operators dilation and erosion, opening and closing, as well as the much more complex top-hat transform, for which we report a real-world application from the steel industry. Using online learning and stochastic gradient descent, our system learns both the structuring element and the composition of operators. It scales well to large datasets and online settings.},
	language = {en},
	booktitle = {Mathematical {Morphology} and {Its} {Applications} to {Signal} and {Image} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Masci, Jonathan and Angulo, Jesús and Schmidhuber, Jürgen},
	editor = {Hendriks, Cris L. Luengo and Borgefors, Gunilla and Strand, Robin},
	year = {2013},
	keywords = {mathematical morphology, convolutional networks, machine learning, online learning},
	pages = {329--340},
}

@incollection{lecun_efficient_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {BackProp}},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	language = {en},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer},
	author = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	editor = {Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {1998},
	keywords = {Handwritten Digit, Conjugate Gradient, Gradient Descent, Neural Information Processing System, Newton Algorithm},
	pages = {9--50},
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2018-10-17},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	language = {en},
	urldate = {2018-10-17},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	pages = {249--256},
}

@incollection{ciresan_deep_2012,
	title = {Deep {Neural} {Networks} {Segment} {Neuronal} {Membranes} in {Electron} {Microscopy} {Images}},
	url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
	urldate = {2018-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Cireşan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, Jürgen},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2843--2851},
}

@inproceedings{chellapilla_high_2006,
	title = {High {Performance} {Convolutional} {Neural} {Networks} for {Document} {Processing}},
	url = {https://hal.inria.fr/inria-00112631/document},
	abstract = {Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X−3.0X speedup. The GPU implementation is even faster and produces a 3.1X−4.1X speedup.},
	language = {en},
	urldate = {2018-10-28},
	booktitle = {Tenth {International} {Workshop} on {Frontiers} in {Handwriting} {Recognition}},
	publisher = {Suvisoft},
	author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
	month = oct,
	year = {2006},
}

@article{ciresan_deep_2010,
	title = {Deep, {Big}, {Simple} {Neural} {Nets} for {Handwritten} {Digit} {Recognition}},
	volume = {22},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00052},
	doi = {10.1162/NECO_a_00052},
	abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.},
	number = {12},
	urldate = {2018-10-28},
	journal = {Neural Computation},
	author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jürgen},
	month = sep,
	year = {2010},
	pages = {3207--3220},
}

@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2018-10-28},
	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-10-28},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
	language = {en},
	number = {2},
	urldate = {2018-10-29},
	journal = {Int J Comput Vis},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	keywords = {Benchmark, Database, Object detection, Object recognition},
	pages = {303--338},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Feature extraction, Pattern recognition, convolution, Machine learning, Neural networks, 2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, Optical character recognition software, Optical computing, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
}

@inproceedings{simard_efficient_1993,
	title = {Efficient pattern recognition using a new transformation distance},
	booktitle = {Advances in neural information processing systems},
	author = {Simard, Patrice and LeCun, Yann and Denker, John S},
	year = {1993},
	pages = {50--58},
}

@article{deniz_segmentation_2018,
	title = {Segmentation of the {Proximal} {Femur} from {MR} {Images} using {Deep} {Convolutional} {Neural} {Networks}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-34817-6},
	doi = {10.1038/s41598-018-34817-6},
	abstract = {Magnetic resonance imaging (MRI) has been proposed as a complimentary method to measure bone quality and assess fracture risk. However, manual segmentation of MR images of bone is time-consuming, limiting the use of MRI measurements in the clinical practice. The purpose of this paper is to present an automatic proximal femur segmentation method that is based on deep convolutional neural networks (CNNs). This study had institutional review board approval and written informed consent was obtained from all subjects. A dataset of volumetric structural MR images of the proximal femur from 86 subjects were manually-segmented by an expert. We performed experiments by training two different CNN architectures with multiple number of initial feature maps, layers and dilation rates, and tested their segmentation performance against the gold standard of manual segmentations using four-fold cross-validation. Automatic segmentation of the proximal femur using CNNs achieved a high dice similarity score of 0.95 ± 0.02 with precision = 0.95 ± 0.02, and recall = 0.95 ± 0.03. The high segmentation accuracy provided by CNNs has the potential to help bring the use of structural MRI measurements of bone quality into clinical practice for management of osteoporosis.},
	language = {En},
	number = {1},
	urldate = {2018-11-08},
	journal = {Scientific Reports},
	author = {Deniz, Cem M. and Xiang, Siyuan and Hallyburton, R. Spencer and Welbeck, Arakua and Babb, James S. and Honig, Stephen and Cho, Kyunghyun and Chang, Gregory},
	month = nov,
	year = {2018},
	pages = {16485},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	urldate = {2018-11-08},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_exudate_2014,
	title = {Exudate detection in color retinal images for mass screening of diabetic retinopathy},
	volume = {18},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841514000693},
	doi = {10.1016/j.media.2014.05.004},
	abstract = {The automatic detection of exudates in color eye fundus images is an important task in applications such as diabetic retinopathy screening. The presented work has been undertaken in the framework of the TeleOphta project, whose main objective is to automatically detect normal exams in a tele-ophthalmology network, thus reducing the burden on the readers. A new clinical database, e-ophtha EX, containing precisely manually contoured exudates, is introduced. As opposed to previously available databases, e-ophtha EX is very heterogeneous. It contains images gathered within the OPHDIAT telemedicine network for diabetic retinopathy screening. Image definition, quality, as well as patients condition or the retinograph used for the acquisition, for example, are subject to important changes between different examinations. The proposed exudate detection method has been designed for this complex situation. We propose new preprocessing methods, which perform not only normalization and denoising tasks, but also detect reflections and artifacts in the image. A new candidates segmentation method, based on mathematical morphology, is proposed. These candidates are characterized using classical features, but also novel contextual features. Finally, a random forest algorithm is used to detect the exudates among the candidates. The method has been validated on the e-ophtha EX database, obtaining an AUC of 0.95. It has been also validated on other databases, obtaining an AUC between 0.93 and 0.95, outperforming state-of-the-art methods.},
	number = {7},
	urldate = {2018-11-18},
	journal = {Medical Image Analysis},
	author = {Zhang, Xiwei and Thibault, Guillaume and Decencière, Etienne and Marcotegui, Beatriz and Laÿ, Bruno and Danno, Ronan and Cazuguel, Guy and Quellec, Gwénolé and Lamard, Mathieu and Massin, Pascale and Chabouis, Agnès and Victor, Zeynep and Erginay, Ali},
	month = oct,
	year = {2014},
	keywords = {Exudates segmentation, Diabetic retinopathy screening, e-Ophtha EX database, Mathematical morphology},
	pages = {1026--1043},
}

@article{luc_semantic_2016,
	title = {Semantic {Segmentation} using {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.08408},
	abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
	urldate = {2018-11-18},
	journal = {arXiv:1611.08408 [cs]},
	author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08408},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lerme_fully_2016,
	series = {Special {Issue} on {ICPR} 2014 {Awarded} {Papers}},
	title = {A fully automatic method for segmenting retinal artery walls in adaptive optics images},
	volume = {72},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865515003621},
	doi = {10.1016/j.patrec.2015.10.011},
	abstract = {Adaptive optics imaging of the retina has recently proven its capability to image micrometric structures such as blood vessels, involved in common ocular diseases. In this paper, we propose an approach for automatically segmenting the walls of retinal arteries in the images acquired with this technology. The walls are modeled as four curves approximately parallel to a previously detected reference line located near the vessel center (axial reflection). These curves are first initialized using a tracking procedure and then more accurately positioned using an active contour model embedding a parallelism constraint. We consider both healthy and pathological subjects in the same framework and show that the proposed method applies in all cases. Extensive experiments are also proposed, by analyzing the robustness of the axial reflections detection, the influence of the tracking parameters as well as the performance of the tracking and the active contour model. Noticeably, the results show a good robustness for detecting axial reflections and a moderate influence of the tracking parameters. Compared to a naive initialization, the active contour model coupled with the tracking also offers faster convergence and better accuracy while keeping an overall error smaller or very near the inter-physicians error.},
	urldate = {2018-11-27},
	journal = {Pattern Recognition Letters},
	author = {Lermé, Nicolas and Rossant, Florence and Bloch, Isabelle and Paques, Michel and Koch, Edouard and Benesty, Jonathan},
	month = mar,
	year = {2016},
	keywords = {Mathematical morphology, Active contour model, Adaptive optics, Retina imaging},
	pages = {72--81},
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	urldate = {2018-11-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	pages = {2672--2680},
}

@inproceedings{zhao_uniqueness-driven_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Uniqueness-{Driven} {Saliency} {Analysis} for {Automated} {Lesion} {Detection} with {Applications} to {Retinal} {Diseases}},
	isbn = {978-3-030-00934-2},
	abstract = {Saliency is important in medical image analysis in terms of detection and segmentation tasks. We propose a new method to extract uniqueness-driven saliency based on the uniqueness of intensity and spatial distributions within the images. The main novelty of this new saliency feature is that it is powerful in the detection of different types of lesions in different types of images without the need of tuning parameters for different problems. To evaluate its effectiveness, we have applied our method to the detection lesions of retinal images. Four different types of lesions: exudate, hemorrhage, microaneurysms and leakage from 7 independent public retinal image datasets of diabetic retinopathy and malarial retinopathy, were studied and the experimental results show that the proposed method is superior to the state-of-the-art methods.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Zhao, Yitian and Zheng, Yalin and Zhao, Yifan and Liu, Yonghuai and Chen, Zhili and Liu, Peng and Liu, Jiang},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	keywords = {Retinopathy, Computer aided-diagnosis, Saliency, Uniqueness},
	pages = {109--118},
}

@techreport{ronse_regular_1990,
	address = {Brussels, Belgium},
	type = {Working document},
	title = {Regular open or closed sets},
	number = {WD59},
	institution = {Philips Research Lab.},
	author = {Ronse, Christian},
	year = {1990},
}

@article{chollet_xception:_2017,
	title = {Xception: {Deep} learning with depthwise separable convolutions},
	shorttitle = {Xception},
	journal = {arXiv preprint},
	author = {Chollet, François},
	year = {2017},
	pages = {1610--02357},
}

@article{howard_mobilenets:_2017,
	title = {Mobilenets: {Efficient} convolutional neural networks for mobile vision applications},
	shorttitle = {Mobilenets},
	journal = {arXiv preprint arXiv:1704.04861},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	year = {2017},
}

@inproceedings{rastegari_xnor-net:_2016,
	title = {Xnor-net: {Imagenet} classification using binary convolutional neural networks},
	shorttitle = {Xnor-net},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	year = {2016},
	pages = {525--542},
}

@inproceedings{davidson_theory_1990,
	title = {Theory of morphological neural networks},
	volume = {1215},
	booktitle = {Digital {Optical} {Computing} {II}},
	publisher = {International Society for Optics and Photonics},
	author = {Davidson, Jennifer L. and Ritter, Gerhard X.},
	year = {1990},
	pages = {378--389},
}

@inproceedings{ritter_introduction_1996,
	title = {An introduction to morphological neural networks},
	volume = {4},
	booktitle = {Pattern {Recognition}, 1996., {Proceedings} of the 13th {International} {Conference} on},
	publisher = {IEEE},
	author = {Ritter, Gerhard X. and Sussner, Peter},
	year = {1996},
	pages = {709--717},
}

@inproceedings{wilson_morphological_1989,
	title = {Morphological networks},
	volume = {1199},
	booktitle = {Visual {Communications} and {Image} {Processing} {IV}},
	publisher = {International Society for Optics and Photonics},
	author = {Wilson, Stephen S.},
	year = {1989},
	pages = {483--496},
}

@article{ritter_lattice_2003,
	title = {Lattice algebra approach to single-neuron computation},
	volume = {14},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Ritter, Gerhard X. and Urcid, Gonzalo},
	year = {2003},
	pages = {282--295},
}

@article{sussner_morphological_2011,
	title = {Morphological perceptrons with competitive learning: {Lattice}-theoretical framework and constructive learning algorithm},
	volume = {181},
	shorttitle = {Morphological perceptrons with competitive learning},
	number = {10},
	journal = {Information Sciences},
	author = {Sussner, Peter and Esmi, Estevão Laureano},
	year = {2011},
	pages = {1929--1950},
}

@inproceedings{pessoa_morphological/rank_1996,
	title = {Morphological/rank neural networks and their adaptive optimal design for image processing},
	volume = {6},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 1996. {ICASSP}-96. {Conference} {Proceedings}., 1996 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Pessoa, Lucio FC and Maragos, Petros},
	year = {1996},
	pages = {3398--3401},
}

@article{pessoa_neural_2000,
	title = {Neural networks with hybrid morphological/rank/linear nodes: a unifying framework with applications to handwritten character recognition},
	volume = {33},
	shorttitle = {Neural networks with hybrid morphological/rank/linear nodes},
	number = {6},
	journal = {Pattern Recognition},
	author = {Pessoa, Lucio FC and Maragos, Petros},
	year = {2000},
	pages = {945--960},
}

@article{mahmood_deep_2018,
	title = {Deep {Adversarial} {Training} for {Multi}-{Organ} {Nuclei} {Segmentation} in {Histopathology} {Images}},
	url = {http://arxiv.org/abs/1810.00236},
	abstract = {Nuclei segmentation is a fundamental task that is critical for various computational pathology applications including nuclei morphology analysis, cell type classification, and cancer grading. Conventional vision-based methods for nuclei segmentation struggle in challenging cases and deep learning approaches have proven to be more robust and generalizable. However, CNNs require large amounts of labeled histopathology data. Moreover, conventional CNN-based approaches lack structured prediction capabilities which are required to distinguish overlapping and clumped nuclei. Here, we present an approach to nuclei segmentation that overcomes these challenges by utilizing a conditional generative adversarial network (cGAN) trained with synthetic and real data. We generate a large dataset of H\&E training images with perfect nuclei segmentation labels using an unpaired GAN framework. This synthetic data along with real histopathology data from six different organs are used to train a conditional GAN with spectral normalization and gradient penalty for nuclei segmentation. This adversarial regression framework enforces higher order consistency when compared to conventional CNN models. We demonstrate that this nuclei segmentation approach generalizes across different organs, sites, patients and disease states, and outperforms conventional approaches, especially in isolating individual and overlapping nuclei.},
	urldate = {2019-01-11},
	journal = {arXiv:1810.00236 [cs]},
	author = {Mahmood, Faisal and Borders, Daniel and Chen, Richard and McKay, Gregory N. and Salimian, Kevan J. and Baras, Alexander and Durr, Nicholas J.},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hou_unsupervised_2017,
	title = {Unsupervised {Histopathology} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1712.05021},
	abstract = {Hematoxylin and Eosin stained histopathology image analysis is essential for the diagnosis and study of complicated diseases such as cancer. Existing state-of-the-art approaches demand extensive amount of supervised training data from trained pathologists. In this work we synthesize in an unsupervised manner, large histopathology image datasets, suitable for supervised training tasks. We propose a unified pipeline that: a) generates a set of initial synthetic histopathology images with paired information about the nuclei such as segmentation masks; b) refines the initial synthetic images through a Generative Adversarial Network (GAN) to reference styles; c) trains a task-specific CNN and boosts the performance of the task-specific CNN with on-the-fly generated adversarial examples. Our main contribution is that the synthetic images are not only realistic, but also representative (in reference styles) and relatively challenging for training task-specific CNNs. We test our method for nucleus segmentation using images from four cancer types. When no supervised data exists for a cancer type, our method without supervision cost significantly outperforms supervised methods which perform across-cancer generalization. Even when supervised data exists for all cancer types, our approach without supervision cost performs better than supervised methods.},
	urldate = {2019-01-11},
	journal = {arXiv:1712.05021 [cs]},
	author = {Hou, Le and Agarwal, Ayush and Samaras, Dimitris and Kurc, Tahsin M. and Gupta, Rajarsi R. and Saltz, Joel H.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{januszewski_flood-filling_2016,
	title = {Flood-{Filling} {Networks}},
	url = {http://arxiv.org/abs/1611.00421},
	abstract = {State-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations: a boundary detection process that uses local image information to classify image locations as boundaries between objects, followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments. Prior work has varied the complexity and approach employed in these two steps, including the incorporation of multi-layer neural networks to perform boundary prediction, and the use of global optimizations during pixel clustering. We propose a unified and end-to-end trainable machine learning approach, flood-filling networks, in which a recurrent 3d convolutional network directly produces individual segments from a raw image. The proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes. We demonstrate the approach on a challenging 3d image segmentation task, connectomic reconstruction from volume electron microscopy data, on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods. The proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end.},
	urldate = {2019-03-05},
	journal = {arXiv:1611.00421 [cs]},
	author = {Januszewski, Michał and Maitin-Shepard, Jeremy and Li, Peter and Kornfeld, Jörgen and Denk, Winfried and Jain, Viren},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00421},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{javaid_semantic_2019,
	title = {Semantic segmentation of computed tomography for radiotherapy with deep learning: compensating insufficient annotation quality using contour augmentation},
	volume = {10949},
	shorttitle = {Semantic segmentation of computed tomography for radiotherapy with deep learning},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10949/109492P/Semantic-segmentation-of-computed-tomography-for-radiotherapy-with-deep-learning/10.1117/12.2512461.short},
	doi = {10.1117/12.2512461},
	abstract = {In radiotherapy treatment planning, manual annotation of organs-at-risk and target volumes is a difficult and time-consuming task, prone to intra and inter-observer variabilities. Deep learning networks (DLNs) are gaining worldwide attention to automate such annotative tasks because of their ability to capture data hierarchy. However, for better performance DLNs require large number of data samples whereas annotated medical data is scarce. To remedy this, data augmentation is used to increase the training data for DLNs that enables robust learning by incorporating spatial/translational invariance into the training phase. Importantly, performance of DLNs is highly dependent on the ground truth (GT) quality: if manual annotation is not accurate enough, the network cannot learn better than the annotated example. This highlights the need to compensate for possibly insufficient GT quality using augmentation, i.e., by providing more GTs per image, in order to improve performance of DLNs. In this work, small random alterations were applied to GT and each altered GT was considered as an additional annotation. Contour augmentation was used to train a dilated U-Net in multiple GTs per image setting, which was tested on a pelvic CT dataset acquired from 67 patients to segment bladder and rectum in a multi-class segmentation setting. By using contour augmentation (coupled with data augmentation), the network learnt better than with data augmentation only, as it was able to correct slightly offset contours in GT. The segmentation results produced were quantified using spatial overlap, distance-based and probabilistic measures. The Dice score for bladder and rectum are reported as 0.88\&plusmn;0.19 and 0.89\&plusmn;0.04, whereas the average symmetric surface distance are 0.22 \&plusmn; 0.09 mm and 0.09 \&plusmn; 0.05 mm, respectively.},
	urldate = {2019-03-18},
	booktitle = {Medical {Imaging} 2019: {Image} {Processing}},
	publisher = {International Society for Optics and Photonics},
	author = {Javaid, Umair and Dasnoy, Damien and Lee, John A.},
	month = mar,
	year = {2019},
	pages = {109492P},
}

@article{drozdzal_importance_2016,
	title = {The {Importance} of {Skip} {Connections} in {Biomedical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1608.04117},
	abstract = {In this paper, we study the influence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient flow confirms that for a very deep FCN it is beneficial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing.},
	urldate = {2019-03-22},
	journal = {arXiv:1608.04117 [cs]},
	author = {Drozdzal, Michal and Vorontsov, Eugene and Chartrand, Gabriel and Kadoury, Samuel and Pal, Chris},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.04117},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jegou_one_2016,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	shorttitle = {The {One} {Hundred} {Layers} {Tiramisu}},
	url = {http://arxiv.org/abs/1611.09326},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	urldate = {2019-03-22},
	journal = {arXiv:1611.09326 [cs]},
	author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.09326},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{long_fully_2014,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	urldate = {2019-03-22},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{milletari_v-net:_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	shorttitle = {V-{Net}},
	url = {http://arxiv.org/abs/1606.04797},
	abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	urldate = {2019-03-22},
	journal = {arXiv:1606.04797 [cs]},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{diaz-pinto_cnns_2019,
	title = {{CNNs} for automatic glaucoma assessment using fundus images: an extensive validation},
	volume = {18},
	issn = {1475-925X},
	shorttitle = {{CNNs} for automatic glaucoma assessment using fundus images},
	url = {https://doi.org/10.1186/s12938-019-0649-y},
	doi = {10.1186/s12938-019-0649-y},
	abstract = {BackgroundMost current algorithms for automatic glaucoma assessment using fundus images rely on handcrafted features based on segmentation, which are affected by the performance of the chosen segmentation method and the extracted features. Among other characteristics, convolutional neural networks (CNNs) are known because of their ability to learn highly discriminative features from raw pixel intensities.MethodsIn this paper, we employed five different ImageNet-trained models (VGG16, VGG19, InceptionV3, ResNet50 and Xception) for automatic glaucoma assessment using fundus images. Results from an extensive validation using cross-validation and cross-testing strategies were compared with previous works in the literature.ResultsUsing five public databases (1707 images), an average AUC of 0.9605 with a 95\% confidence interval of 95.92–97.07\%, an average specificity of 0.8580 and an average sensitivity of 0.9346 were obtained after using the Xception architecture, significantly improving the performance of other state-of-the-art works. Moreover, a new clinical database, ACRIMA, has been made publicly available, containing 705 labelled images. It is composed of 396 glaucomatous images and 309 normal images, which means, the largest public database for glaucoma diagnosis. The high specificity and sensitivity obtained from the proposed approach are supported by an extensive validation using not only the cross-validation strategy but also the cross-testing validation on, to the best of the authors’ knowledge, all publicly available glaucoma-labelled databases.ConclusionsThese results suggest that using ImageNet-trained models is a robust alternative for automatic glaucoma screening system. All images, CNN weights and software used to fine-tune and test the five CNNs are publicly available, which could be used as a testbed for further comparisons.},
	language = {en},
	number = {1},
	urldate = {2019-03-25},
	journal = {BioMed Eng OnLine},
	author = {Diaz-Pinto, Andres and Morales, Sandra and Naranjo, Valery and Köhler, Thomas and Mossi, Jose M. and Navea, Amparo},
	month = mar,
	year = {2019},
	keywords = {Glaucoma, ACRIMA database, CNN, Fine-tuning, Fundus images},
	pages = {29},
}

@article{heijmans_algebraic_1990,
	title = {The algebraic basis of mathematical morphology {I}. {Dilations} and erosions},
	volume = {50},
	issn = {0734-189X},
	url = {http://www.sciencedirect.com/science/article/pii/0734189X9090148O},
	doi = {10.1016/0734-189X(90)90148-O},
	abstract = {Mathematical morphology is a theory of image transformations and functionals deriving its tools from set theory and integral geometry. This paper deals with a general algebraic approach which both reveals the mathematical structure of morphological operations and unifies several examples into one framework. The main assumption is that the object space is a complete lattice and that the transformations of interest are invariant under a given abelian group of automorphisms on that lattice. It turns out that the basic operations called dilation and erosion are adjoints of each other in a very specific lattice sense and can be completely characterized if the automorphism group is assumed to be transitive on a sup-generating subset of the complete lattice. The abstract theory is illustrated by a large variety of examples and applications.},
	number = {3},
	urldate = {2019-04-25},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Heijmans, H. J. A. M and Ronse, C},
	month = jun,
	year = {1990},
	pages = {245--295},
}

@article{ronse_algebraic_1991,
	title = {The algebraic basis of mathematical morphology: {II}. {Openings} and closings},
	volume = {54},
	issn = {1049-9660},
	shorttitle = {The algebraic basis of mathematical morphology},
	url = {http://www.sciencedirect.com/science/article/pii/1049966091900762},
	doi = {10.1016/1049-9660(91)90076-2},
	abstract = {This paper is the sequel to a previous paper (Part I) where we introduced and investigated an abstract algebraic framework for mathematical morphology. The main assumption is that the object space is a complete lattice. Of interest are all (increasing) operators which are invariant under a given abelian group of automorphisms on the lattice. In Part I we were mainly concerned with the basic operations dilation and erosion. In this paper we concentrate on openings and closings, which are very special classes of idempotent operators. Much attention is given to specific methods for building openings and closings in an economical way; in particular we study annular openings and inf-overfilters. We also consider the possibility of generating new openings by iteration of anti-extensive operators. Some examples illustrate the abstract theory.},
	number = {1},
	urldate = {2019-04-25},
	journal = {CVGIP: Image Understanding},
	author = {Ronse, C. and Heijmans, H. J. A. M.},
	month = jul,
	year = {1991},
	pages = {74--97},
}

@inproceedings{carrera_detecting_2015,
	title = {Detecting anomalous structures by convolutional sparse models},
	doi = {10.1109/IJCNN.2015.7280790},
	abstract = {We address the problem of detecting anomalies in images, specifically that of detecting regions characterized by structures that do not conform those of normal images. In the proposed approach we exploit convolutional sparse models to learn a dictionary of filters from a training set of normal images. These filters capture the structure of normal images and are leveraged to quantitatively assess whether regions of a test image are normal or anomalous. Each test image is at first encoded with respect to the learned dictionary, yielding sparse coefficient maps, and then analyzed by computing indicator vectors that assess the conformance of local image regions with the learned filters. Anomalies are then detected by identifying outliers in these indicators. Our experiments demonstrate that a convolutional sparse model provides better anomaly-detection performance than an equivalent method based on standard patch-based sparsity. Most importantly, our results highlight that monitoring the local group sparsity, namely the spread of nonzero coefficients across different maps, is essential for detecting anomalous regions.},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Carrera, D. and Boracchi, G. and Foi, A. and Wohlberg, B.},
	month = jul,
	year = {2015},
	keywords = {filtering theory, convolution, image processing, anomalous structure, anomaly detection, Anomaly Detection, anomaly-detection performance, computing indicator vector, convolutional sparse model, Convolutional Sparse Models, Deconvolutional Networks, learned dictionary, learned filter, local image region, nonzero coefficient, normal image, sparse coefficient map, standard patch-based sparsity},
	pages = {1--8},
}

@article{pimentel_review_2014,
	title = {A review of novelty detection},
	volume = {99},
	issn = {0165-1684},
	url = {http://www.sciencedirect.com/science/article/pii/S016516841300515X},
	doi = {10.1016/j.sigpro.2013.12.026},
	abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as “one-class classification”, in which a model is constructed to describe “normal” training data. The novelty detection approach is typically used when the quantity of available “abnormal” data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that “normality” may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
	urldate = {2019-05-17},
	journal = {Signal Processing},
	author = {Pimentel, Marco A. F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
	month = jun,
	year = {2014},
	keywords = {Machine learning, Novelty detection, One-class classification},
	pages = {215--249},
}

@article{markou_novelty_2003,
	title = {Novelty detection: a review—part 1: statistical approaches},
	volume = {83},
	issn = {0165-1684},
	shorttitle = {Novelty detection},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168403002020},
	doi = {10.1016/j.sigpro.2003.07.018},
	abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.},
	number = {12},
	urldate = {2019-05-17},
	journal = {Signal Processing},
	author = {Markou, Markos and Singh, Sameer},
	month = dec,
	year = {2003},
	keywords = {Hidden Markov models, Clustering, Gaussian mixture models, KNN, Novelty detection review, Parzen density estimation, Statistical approaches, String matching},
	pages = {2481--2497},
}

@article{markou_novelty_2003-1,
	title = {Novelty detection: a review—part 2:: neural network based approaches},
	volume = {83},
	issn = {0165-1684},
	shorttitle = {Novelty detection},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168403002032},
	doi = {10.1016/j.sigpro.2003.07.019},
	abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network-based approaches for novelty detection. Statistical approaches are covered in Part 1 paper.},
	number = {12},
	urldate = {2019-05-17},
	journal = {Signal Processing},
	author = {Markou, Markos and Singh, Sameer},
	month = dec,
	year = {2003},
	keywords = {Neural networks, Novelty detection, ART, MLP, Network-based approaches, RBF},
	pages = {2499--2521},
}

@article{chandola_anomaly_2009,
	title = {Anomaly {Detection}: {A} {Survey}},
	volume = {41},
	issn = {0360-0300},
	shorttitle = {Anomaly {Detection}},
	url = {http://doi.acm.org/10.1145/1541880.1541882},
	doi = {10.1145/1541880.1541882},
	abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
	number = {3},
	urldate = {2019-05-17},
	journal = {ACM Comput. Surv.},
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	month = jul,
	year = {2009},
	keywords = {Anomaly detection, outlier detection},
	pages = {15:1--15:58},
}

@inproceedings{mondal_morphological_2019,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Morphological {Networks} for {Image} {De}-raining},
	isbn = {978-3-030-14085-4},
	abstract = {Mathematical morphological methods have successfully been applied to filter out (emphasize or remove) different structures of an image. However, it is argued that these methods could be suitable for the task only if the type and order of the filter(s) as well as the shape and size of operator kernel are designed properly. Thus the existing filtering operators are problem (instance) specific and are designed by the domain experts. In this work we propose a morphological network that emulates classical morphological filtering consisting of a series of erosion and dilation operators with trainable structuring elements. We evaluate the proposed network for image de-raining task where the SSIM and mean absolute error (MAE) loss corresponding to predicted and ground-truth clean image is back-propagated through the network to train the structuring elements. We observe that a single morphological network can de-rain an image with any arbitrary shaped rain-droplets and achieves similar performance with the contemporary CNNs for this task with a fraction of trainable parameters (network size). The proposed morphological network (MorphoN) is not designed specifically for de-raining and can readily be applied to similar filtering/noise cleaning tasks. The source code can be found here https://github.com/ranjanZ/2D-Morphological-Network.},
	language = {en},
	booktitle = {Discrete {Geometry} for {Computer} {Imagery}},
	publisher = {Springer International Publishing},
	author = {Mondal, Ranjan and Purkait, Pulak and Santra, Sanchayan and Chanda, Bhabatosh},
	editor = {Couprie, Michel and Cousty, Jean and Kenmochi, Yukiko and Mustafa, Nabil},
	year = {2019},
	keywords = {Mathematical morphology, Image filtering, Morphological network, Optimization},
	pages = {262--275},
}

@inproceedings{liu_intriguing_2018,
	title = {An intriguing failing of convolutional neural networks and the coordconv solution},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
	year = {2018},
	pages = {9605--9616},
}

@inproceedings{lee_roomnet:_2017,
	title = {{RoomNet}: {End}-{To}-{End} {Room} {Layout} {Estimation}},
	shorttitle = {{RoomNet}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Lee_RoomNet_End-To-End_Room_ICCV_2017_paper.html},
	urldate = {2019-06-13},
	author = {Lee, Chen-Yu and Badrinarayanan, Vijay and Malisiewicz, Tomasz and Rabinovich, Andrew},
	year = {2017},
	pages = {4865--4874},
}

@inproceedings{ganaye_semi-supervised_2018,
	title = {Semi-supervised learning for segmentation under semantic constraint},
	booktitle = {International {Conference} on {Medical} {Image} {Computing} and {Computer}-{Assisted} {Intervention}},
	publisher = {Springer},
	author = {Ganaye, Pierre-Antoine and Sdika, Michaël and Benoit-Cattin, Hugues},
	year = {2018},
	pages = {595--602},
}

@inproceedings{ganaye_towards_2018,
	title = {Towards integrating spatial localization in convolutional neural networks for brain image segmentation},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Ganaye, Pierre-Antoine and Sdika, Michaël and Benoit-Cattin, Hugues},
	year = {2018},
	pages = {621--625},
}

@inproceedings{basharat_learning_2008,
	title = {Learning object motion patterns for anomaly detection and improved object detection},
	doi = {10.1109/CVPR.2008.4587510},
	abstract = {We present a novel framework for learning patterns of motion and sizes of objects in static camera surveillance. The proposed method provides a new higher-level layer to the traditional surveillance pipeline for anomalous event detection and scene model feedback. Pixel level probability density functions (pdfs) of appearance have been used for background modelling in the past, but modelling pixel level pdfs of object speed and size from the tracks is novel. Each pdf is modelled as a multivariate Gaussian mixture model (GMM) of the motion (destination location \& transition time) and the size (width \& height) parameters of the objects at that location. Output of the tracking module is used to perform unsupervised EM-based learning of every GMM. We have successfully used the proposed scene model to detect local as well as global anomalies in object tracks. We also show the use of this scene model to improve object detection through pixel-level parameter feedback of the minimum object size and background learning rate. Most object path modelling approaches first cluster the tracks into major paths in the scene, which can be a source of error. We avoid this by building local pdfs that capture a variety of tracks which are passing through them. Qualitative and quantitative analysis of actual surveillance videos proved the effectiveness of the proposed approach.},
	booktitle = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Basharat, Arslan and Gritai, Alexei and Shah, Mubarak},
	month = jun,
	year = {2008},
	note = {ISSN: 1063-6919},
	keywords = {Layout, object detection, Cameras, Videos, Object detection, anomaly detection, actual surveillance videos, background modelling, Event detection, Feedback, Gaussian processes, image motion analysis, image resolution, Motion detection, multivariate Gaussian mixture model, object motion patterns, object path modelling, Pipelines, pixel level probability density functions, pixel-level parameter feedback, Probability density function, scene model feedback, static camera surveillance, Surveillance, video signal processing, video surveillance},
	pages = {1--8},
}

@article{zenati_efficient_2018,
	title = {Efficient {GAN}-{Based} {Anomaly} {Detection}},
	volume = {abs/1802.06222},
	url = {http://arxiv.org/abs/1802.06222},
	urldate = {2019-12-04},
	journal = {CoRR},
	author = {Zenati, Houssam and Foo, Chuan Sheng and Lecouat, Bruno and Manek, Gaurav and Chandrasekhar, Vijay Ramaseshan},
	year = {2018},
}

@inproceedings{akcay_ganomaly:_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{GANomaly}: {Semi}-supervised {Anomaly} {Detection} via {Adversarial} {Training}},
	isbn = {978-3-030-20893-6},
	shorttitle = {{GANomaly}},
	doi = {10.1007/978-3-030-20893-6_39},
	abstract = {Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution—an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2018},
	publisher = {Springer International Publishing},
	author = {Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P.},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	keywords = {Anomaly detection, Generative Adversarial Networks, Semi-supervised learning, X-ray security imagery},
	pages = {622--637},
}

@article{reinhard_color_2001,
	title = {Color transfer between images},
	volume = {21},
	issn = {02721716},
	url = {http://ieeexplore.ieee.org/document/946629/},
	doi = {10.1109/38.946629},
	number = {4},
	urldate = {2020-01-27},
	journal = {IEEE Comput. Grap. Appl.},
	author = {Reinhard, E. and Adhikhmin, M. and Gooch, B. and Shirley, P.},
	month = aug,
	year = {2001},
	pages = {34--41},
}

@inproceedings{xiao_new_2019,
	title = {A {New} {Color} {Augmentation} {Method} for {Deep} {Learning} {Segmentation} of {Histological} {Images}},
	doi = {10.1109/ISBI.2019.8759591},
	abstract = {This paper addresses the problem of labeled data insufficiency in neural network training for semantic segmentation of color-stained histological images acquired via Whole Slide Imaging. It proposes an efficient image augmentation method to alleviate the demand for a large amount of labeled data and improve the network's generalization capacity. Typical image augmentation in bioimaging involves geometric transformation. Here, we propose a new image augmentation technique by combining the structure of one image with the color appearance of another image to construct augmented images on-the-fly for each training iteration. We show that it improves performance in the segmentation of histological images of human skin, and also offers better results when combined with geometric transformation.},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	author = {Xiao, Yang and Decencière, Etienne and Velasco-Forero, Santiago and Burdin, Hélène and Bornschlögl, Thomas and Bernerd, Françoise and Warrick, Emilie and Baldeweck, Thérèse},
	month = apr,
	year = {2019},
	note = {ISSN: 1945-7928},
	keywords = {Databases, Image color analysis, Image segmentation, human skin, image colour analysis, medical image processing, segmentation, skin, learning (artificial intelligence), neural nets, Training, deep learning, Deep learning, image segmentation, Agriculture, biological tissues, color appearance, color augmentation, color transfer, color-stained histological images, color-stained slide, deep learning segmentation, Fontana Masson, geometric transformation, geometry, histopathology, image augmentation, neural network training, semantic segmentation, Skin, transforms, whole slide imaging},
	pages = {886--890},
}

@book{hawkins_identification_1980,
	title = {Identification of outliers},
	volume = {11},
	publisher = {Springer},
	author = {Hawkins, Douglas M.},
	year = {1980},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2020-09-23},
	journal = {Int J Comput Vis},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
}

@inproceedings{stallkamp_german_2011,
	title = {The {German} {Traffic} {Sign} {Recognition} {Benchmark}: {A} multi-class classification competition},
	shorttitle = {The {German} {Traffic} {Sign} {Recognition} {Benchmark}},
	doi = {10.1109/IJCNN.2011.6033395},
	abstract = {The “German Traffic Sign Recognition Benchmark” is a multi-category classification competition held at IJCNN 2011. Automatic recognition of traffic signs is required in advanced driver assistance systems and constitutes a challenging real-world computer vision and pattern recognition problem. A comprehensive, lifelike dataset of more than 50,000 traffic sign images has been collected. It reflects the strong variations in visual appearance of signs due to distance, illumination, weather conditions, partial occlusions, and rotations. The images are complemented by several precomputed feature sets to allow for applying machine learning algorithms without background knowledge in image processing. The dataset comprises 43 classes with unbalanced class frequencies. Participants have to classify two test sets of more than 12,500 images each. Here, the results on the first of these sets, which was used in the first evaluation stage of the two-fold challenge, are reported. The methods employed by the participants who achieved the best results are briefly described and compared to human traffic sign recognition performance and baseline results.},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},
	month = jul,
	year = {2011},
	note = {ISSN: 2161-4407},
	keywords = {Benchmark testing, Histograms, Humans, Image color analysis, computer vision, image classification, learning (artificial intelligence), Training, image processing, driver assistance system, driver information systems, German Traffic Sign Recognition Benchmark, Image resolution, Lead, machine learning algorithm, multiclass classification competition, pattern recognition problem, traffic engineering computing},
	pages = {1453--1460},
}

@article{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2020-09-25},
	journal = {arXiv:1610.02357 [cs]},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv: 1610.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huang_densely_2018,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2020-09-27},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jan,
	year = {2018},
	note = {arXiv: 1608.06993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{donahue_adversarial_2017,
	title = {Adversarial {Feature} {Learning}},
	url = {http://arxiv.org/abs/1605.09782},
	abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
	urldate = {2020-09-30},
	journal = {arXiv:1605.09782 [cs, stat]},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	month = apr,
	year = {2017},
	note = {arXiv: 1605.09782},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bergmann_mvtec_2019,
	title = {{MVTec} {AD} -- {A} {Comprehensive} {Real}-{World} {Dataset} for {Unsupervised} {Anomaly} {Detection}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.html},
	urldate = {2020-10-01},
	author = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	year = {2019},
	pages = {9592--9600},
}

@incollection{lee_simple_2018,
	title = {A {Simple} {Unified} {Framework} for {Detecting} {Out}-of-{Distribution} {Samples} and {Adversarial} {Attacks}},
	url = {http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf},
	urldate = {2020-10-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7167--7177},
}

@article{ounkomol_label-free_2018,
	title = {Label-free prediction of three-dimensional fluorescence images from transmitted light microscopy},
	volume = {15},
	issn = {1548-7091},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6212323/},
	doi = {10.1038/s41592-018-0111-2},
	abstract = {Understanding cells as integrated systems is a challenge central to modern biology. The different microscopy approaches used to probe biological organization each present limitations, ultimately restricting insight into unified cellular processes. Fluorescence microscopy can resolve subcellular structure in living cells, but is expensive, slow, and toxic. Here, we present a label-free method for predicting 3D fluorescence directly from transmitted light images and demonstrate its use to generate multi-structure, integrated images.},
	number = {11},
	urldate = {2020-10-05},
	journal = {Nat Methods},
	author = {Ounkomol, Chawin and Seshamani, Sharmishtaa and Maleckar, Mary M. and Collman, Forrest and Johnson, Gregory R.},
	month = nov,
	year = {2018},
	pmid = {30224672},
	pmcid = {PMC6212323},
	pages = {917--920},
}

@inproceedings{yosinski_how_2014,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'14},
	title = {How transferable are features in deep neural networks?},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	month = dec,
	year = {2014},
	pages = {3320--3328},
}

@article{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	shorttitle = {Attention {U}-{Net}},
	url = {http://arxiv.org/abs/1804.03999},
	abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
	urldate = {2020-10-13},
	journal = {arXiv:1804.03999 [cs]},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	month = may,
	year = {2018},
	note = {arXiv: 1804.03999},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{heller_imperfect_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Imperfect {Segmentation} {Labels}: {How} {Much} {Do} {They} {Matter}?},
	isbn = {978-3-030-01364-6},
	shorttitle = {Imperfect {Segmentation} {Labels}},
	doi = {10.1007/978-3-030-01364-6_13},
	abstract = {Labeled datasets for semantic segmentation are imperfect, especially in medical imaging where borders are often subtle or ill-defined. Little work has been done to analyze the effect that label errors have on the performance of segmentation methodologies. Here we present a large-scale study of model performance in the presence of varying types and degrees of error in training data. We trained U-Net, SegNet, and FCN32 several times for liver segmentation with 10 different modes of ground-truth perturbation. Our results show that for each architecture, performance steadily declines with boundary-localized errors, however, U-Net was significantly more robust to jagged boundary errors than the other architectures. We also found that each architecture was very robust to non-boundary-localized errors, suggesting that boundary-localized errors are fundamentally different and more challenging problem than random label errors in a classification setting.},
	language = {en},
	booktitle = {Intravascular {Imaging} and {Computer} {Assisted} {Stenting} and {Large}-{Scale} {Annotation} of {Biomedical} {Data} and {Expert} {Label} {Synthesis}},
	publisher = {Springer International Publishing},
	author = {Heller, Nicholas and Dean, Joshua and Papanikolopoulos, Nikolaos},
	editor = {Stoyanov, Danail and Taylor, Zeike and Balocco, Simone and Sznitman, Raphael and Martel, Anne and Maier-Hein, Lena and Duong, Luc and Zahnd, Guillaume and Demirci, Stefanie and Albarqouni, Shadi and Lee, Su-Lin and Moriconi, Stefano and Cheplygina, Veronika and Mateus, Diana and Trucco, Emanuele and Granger, Eric and Jannin, Pierre},
	year = {2018},
	keywords = {Boundary-localized Errors, Jagged Boundaries, Liver Segmentation, SegNet, Semantic Segmentation Techniques},
	pages = {112--120},
}

@article{tuccillo_deep_2018,
	title = {Deep learning for galaxy surface brightness profile fitting},
	volume = {475},
	issn = {0035-8711},
	url = {https://academic.oup.com/mnras/article/475/1/894/4725057},
	doi = {10.1093/mnras/stx3186},
	abstract = {Abstract.  Numerous ongoing and future large area surveys (e.g. Dark Energy Survey, EUCLID, Large Synoptic Survey Telescope, Wide Field Infrared Survey Telescop},
	language = {en},
	number = {1},
	urldate = {2020-10-15},
	journal = {Mon Not R Astron Soc},
	author = {Tuccillo, D. and Huertas-Company, M. and Decencière, E. and Velasco-Forero, S. and Domínguez Sánchez, H. and Dimauro, P.},
	month = mar,
	year = {2018},
	note = {Publisher: Oxford Academic},
	pages = {894--909},
}

@article{grogin_candels_2011,
	title = {{CANDELS}: {THE} {COSMIC} {ASSEMBLY} {NEAR}-{INFRARED} {DEEP} {EXTRAGALACTIC} {LEGACY} {SURVEY}},
	volume = {197},
	issn = {0067-0049},
	shorttitle = {{CANDELS}},
	url = {https://doi.org/10.1088%2F0067-0049%2F197%2F2%2F35},
	doi = {10.1088/0067-0049/197/2/35},
	abstract = {The Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS) is designed to document the first third of galactic evolution, over the approximate redshift (z) range 8-1.5. It will image {\textgreater}250,000 distant galaxies using three separate cameras on the Hubble Space Telescope, from the mid-ultraviolet to the near-infrared, and will find and measure Type Ia supernovae at z {\textgreater} 1.5 to test their accuracy as standardizable candles for cosmology. Five premier multi-wavelength sky regions are selected, each with extensive ancillary data. The use of five widely separated fields mitigates cosmic variance and yields statistically robust and complete samples of galaxies down to a stellar mass of 109 M ☉ to z ≈ 2, reaching the knee of the ultraviolet luminosity function of galaxies to z ≈ 8. The survey covers approximately 800 arcmin2 and is divided into two parts. The CANDELS/Deep survey (5σ point-source limit H = 27.7 mag) covers ∼125 arcmin2 within Great Observatories Origins Deep Survey (GOODS)-N and GOODS-S. The CANDELS/Wide survey includes GOODS and three additional fields (Extended Groth Strip, COSMOS, and Ultra-deep Survey) and covers the full area to a 5σ point-source limit of H ≳ 27.0 mag. Together with the Hubble Ultra Deep Fields, the strategy creates a three-tiered “wedding-cake” approach that has proven efficient for extragalactic surveys. Data from the survey are nonproprietary and are useful for a wide variety of science investigations. In this paper, we describe the basic motivations for the survey, the CANDELS team science goals and the resulting observational requirements, the field selection and geometry, and the observing design. The Hubble data processing and products are described in a companion paper.},
	language = {en},
	number = {2},
	urldate = {2020-10-15},
	journal = {ApJS},
	author = {Grogin, Norman A. and Kocevski, Dale D. and Faber, S. M. and Ferguson, Henry C. and Koekemoer, Anton M. and Riess, Adam G. and Acquaviva, Viviana and Alexander, David M. and Almaini, Omar and Ashby, Matthew L. N. and Barden, Marco and Bell, Eric F. and Bournaud, Frédéric and Brown, Thomas M. and Caputi, Karina I. and Casertano, Stefano and Cassata, Paolo and Castellano, Marco and Challis, Peter and Chary, Ranga-Ram and Cheung, Edmond and Cirasuolo, Michele and Conselice, Christopher J. and Cooray, Asantha Roshan and Croton, Darren J. and Daddi, Emanuele and Dahlen, Tomas and Davé, Romeel and Mello, Duília F. de and Dekel, Avishai and Dickinson, Mark and Dolch, Timothy and Donley, Jennifer L. and Dunlop, James S. and Dutton, Aaron A. and Elbaz, David and Fazio, Giovanni G. and Filippenko, Alexei V. and Finkelstein, Steven L. and Fontana, Adriano and Gardner, Jonathan P. and Garnavich, Peter M. and Gawiser, Eric and Giavalisco, Mauro and Grazian, Andrea and Guo, Yicheng and Hathi, Nimish P. and Häussler, Boris and Hopkins, Philip F. and Huang, Jia-Sheng and Huang, Kuang-Han and Jha, Saurabh W. and Kartaltepe, Jeyhan S. and Kirshner, Robert P. and Koo, David C. and Lai, Kamson and Lee, Kyoung-Soo and Li, Weidong and Lotz, Jennifer M. and Lucas, Ray A. and Madau, Piero and McCarthy, Patrick J. and McGrath, Elizabeth J. and McIntosh, Daniel H. and McLure, Ross J. and Mobasher, Bahram and Moustakas, Leonidas A. and Mozena, Mark and Nandra, Kirpal and Newman, Jeffrey A. and Niemi, Sami-Matias and Noeske, Kai G. and Papovich, Casey J. and Pentericci, Laura and Pope, Alexandra and Primack, Joel R. and Rajan, Abhijith and Ravindranath, Swara and Reddy, Naveen A. and Renzini, Alvio and Rix, Hans-Walter and Robaina, Aday R. and Rodney, Steven A. and Rosario, David J. and Rosati, Piero and Salimbeni, Sara and Scarlata, Claudia and Siana, Brian and Simard, Luc and Smidt, Joseph and Somerville, Rachel S. and Spinrad, Hyron and Straughn, Amber N. and Strolger, Louis-Gregory and Telford, Olivia and Teplitz, Harry I. and Trump, Jonathan R. and Wel, Arjen van der and Villforth, Carolin and Wechsler, Risa H. and Weiner, Benjamin J. and Wiklind, Tommy and Wild, Vivienne and Wilson, Grant and Wuyts, Stijn and Yan, Hao-Jing and Yun, Min S.},
	month = dec,
	year = {2011},
	note = {Publisher: IOP Publishing},
	pages = {35},
}

@article{pan_survey_2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Data mining, Labeling, Testing, data mining., unsupervised learning, Machine learning, optimisation, Machine learning algorithms, machine learning, data mining, inductive transfer learning, knowledge engineering, Knowledge engineering, knowledge transfer, Knowledge transfer, learning by example, Learning systems, Space technology, survey, Training data, transductive transfer learning, Transfer learning, unsupervised transfer learning},
	pages = {1345--1359},
}

@inproceedings{shocher_zero-shot_2018,
	title = {“{Zero}-{Shot}” {Super}-{Resolution} {Using} {Deep} {Internal} {Learning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Shocher_Zero-Shot_Super-Resolution_Using_CVPR_2018_paper.html},
	urldate = {2021-01-12},
	author = {Shocher, Assaf and Cohen, Nadav and Irani, Michal},
	year = {2018},
	pages = {3118--3126},
}

@inproceedings{glasner_super-resolution_2009,
	title = {Super-resolution from a single image},
	doi = {10.1109/ICCV.2009.5459271},
	abstract = {Methods for super-resolution can be broadly classified into two families of methods: (i) The classical multi-image super-resolution (combining images obtained at subpixel misalignments), and (ii) Example-Based super-resolution (learning correspondence between low and high resolution image patches from a database). In this paper we propose a unified framework for combining these two families of methods. We further show how this combined approach can be applied to obtain super resolution from as little as a single image (with no database or prior examples). Our approach is based on the observation that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Recurrence of patches within the same image scale (at subpixel misalignments) gives rise to the classical super-resolution, whereas recurrence of patches across different scales of the same image gives rise to example-based super-resolution. Our approach attempts to recover at each pixel its best possible resolution increase based on its patch redundancy within and across scales.},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	author = {Glasner, D. and Bagon, S. and Irani, M.},
	month = sep,
	year = {2009},
	note = {ISSN: 2380-7504},
	keywords = {Image databases, Layout, Mathematics, image resolution, Image resolution, Computer science, Computer vision, Equations, example-based super-resolution, Frequency, high resolution image patch, Image reconstruction, image scale, low resolution image patch, multiimage super-resolution, natural image, single image, Strontium, subpixel misalignment},
	pages = {349--356},
}

@inproceedings{holschneider_real-time_1990,
	address = {Berlin, Heidelberg},
	series = {inverse problems and theoretical imaging},
	title = {A {Real}-{Time} {Algorithm} for {Signal} {Analysis} with the {Help} of the {Wavelet} {Transform}},
	isbn = {978-3-642-75988-8},
	doi = {10.1007/978-3-642-75988-8_28},
	abstract = {The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform. We shall briefly describe this transformation in the following. For more details, we refer to the literature [1].},
	language = {en},
	booktitle = {Wavelets},
	publisher = {Springer},
	author = {Holschneider, M. and Kronland-Martinet, R. and Morlet, J. and Tchamitchian, Ph.},
	editor = {Combes, Jean-Michel and Grossmann, Alexander and Tchamitchian, Philippe},
	year = {1990},
	keywords = {Dilation Parameter, Discrete Wavelet, Graph Algebra, Interpolation Filter, Wavelet Transform},
	pages = {286--297},
}

@article{feng_ning_toward_2005,
	title = {Toward automatic phenotyping of developing embryos from videos},
	volume = {14},
	issn = {1941-0042},
	doi = {10.1109/TIP.2005.852470},
	abstract = {We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {{Feng Ning} and Delhomme, D. and LeCun, Y. and Piano, F. and Bottou, L. and Barbano, P. E.},
	month = sep,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Algorithms, Animals, Artificial Intelligence, Image Enhancement, Image Interpretation, Computer-Assisted, Image segmentation, Microscopy, Microscopy, Phase-Contrast, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity, cellular biophysics, optical microscopy, biological techniques, genetics, automatic phenotyping, convolutional network, cytoplasm, embryos, microscopic images, nucleus membrane, Bioinformatics, Biological system modeling, Embryo, Genomics, Motion pictures, Performance analysis, Videos, energy-based model, nonlinear filter, Caenorhabditis elegans, Embryo, Nonmammalian, Fetal Development, Microscopy, Video, Phenotype, image segmentation, Convolutional network},
	pages = {1360--1371},
}

@inproceedings{jain_supervised_2007,
	title = {Supervised {Learning} of {Image} {Restoration} with {Convolutional} {Networks}},
	doi = {10.1109/ICCV.2007.4408909},
	abstract = {Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition. Here we show that they can also be used as a general method for low-level image processing. As an example of our approach, convolutional networks are trained using gradient learning to solve the problem of restoring noisy or degraded images. For our training data, we have used electron microscopic images of neural circuitry with ground truth restorations provided by human experts. On this dataset, Markov random field (MRF), conditional random field (CRF), and anisotropic diffusion algorithms perform about the same as simple thresholding, but superior performance is obtained with a convolutional network containing over 34,000 adjustable parameters. When restored by this convolutional network, the images are clean enough to be used for segmentation, whereas the other approaches fail in this respect. We do not believe that convolutional networks are fundamentally superior to MRFs as a representation for image processing algorithms. On the contrary, the two approaches are closely related. But in practice, it is possible to train complex convolutional networks, while even simple MRF models are hindered by problems with Bayesian learning and inference procedures. Our results suggest that high model complexity is the single most important factor for good performance, and this is possible with convolutional networks.},
	booktitle = {2007 {IEEE} 11th {International} {Conference} on {Computer} {Vision}},
	author = {Jain, V. and Murray, J. F. and Roth, F. and Turaga, S. and Zhigulin, V. and Briggman, K. L. and Helmstaedter, M. N. and Denk, W. and Seung, H. S.},
	month = oct,
	year = {2007},
	note = {ISSN: 2380-7504},
	keywords = {Humans, computer vision, learning (artificial intelligence), object recognition, Markov processes, image restoration, Markov random field, Supervised learning, convolutional networks, Object recognition, Training data, anisotropic diffusion algorithms, Bayesian learning, Circuit noise, conditional random field, Degradation, degraded images, electron microscopic images, Electron microscopy, gradient learning, high-level vision problems, Image processing, Image restoration, inference mechanisms, inference procedures, low-level image processing, Markov random fields, neural circuitry, supervised learning},
	pages = {1--8},
}

@inproceedings{glorot_understanding_2010-1,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
	pages = {249--256},
}

@article{luo_understanding_2017,
	title = {Understanding the {Effective} {Receptive} {Field} in {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.04128},
	abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
	urldate = {2021-01-22},
	journal = {arXiv:1701.04128 [cs]},
	author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04128},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{haselmann_anomaly_2018,
	title = {Anomaly {Detection} {Using} {Deep} {Learning} {Based} {Image} {Completion}},
	doi = {10.1109/ICMLA.2018.00201},
	abstract = {Automated surface inspection is an important task in many manufacturing industries and often requires machine learning driven solutions. Supervised approaches, however, can be challenging, since it is often difficult to obtain large amounts of labeled training data. In this work, we instead perform one-class unsupervised learning on fault-free samples by training a deep convolutional neural network to complete images whose center regions are cut out. Since the network is trained exclusively on fault-free data, it completes the image patches with a fault-free version of the missing image region. The pixel-wise reconstruction error within the cut out region is an anomaly image which can be used for anomaly detection. Results on surface images of decorated plastic parts demonstrate that this approach is suitable for detection of visible anomalies and moreover surpasses all other tested methods.},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Haselmann, M. and Gruber, D. P. and Tabatabai, P.},
	month = dec,
	year = {2018},
	keywords = {image reconstruction, unsupervised learning, Training, deep learning, Inspection, Anomaly detection, anomaly detection, Anomaly Detection, Convolutional neural networks, Image reconstruction, anomaly image, automated surface inspection, automatic optical inspection, convolutional neural nets, Convolutional Neural Networks, deep convolutional neural network, Defect detection, fault-free data, image completion, Image Completion, image patches, Inpainting, manufacturing industries, missing image region, One-class, one-class unsupervised learning, pixel-wise reconstruction error, production engineering computing, surface images, Surface Inspection, Surface reconstruction, Task analysis, Unsupervised},
	pages = {1237--1242},
}

@article{rippel_modeling_2020,
	title = {Modeling the {Distribution} of {Normal} {Data} in {Pre}-{Trained} {Deep} {Features} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2005.14140},
	abstract = {Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance. We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an AUROC value of \$95.8 {\textbackslash}pm 1.2\$ (mean \${\textbackslash}pm\$ SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often sub-par performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption. Code available at https://github.com/ORippler/gaussian-ad-mvtec},
	urldate = {2021-01-25},
	journal = {arXiv:2005.14140 [cs]},
	author = {Rippel, Oliver and Mertens, Patrick and Merhof, Dorit},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.14140},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ma_contrast-based_2003,
	address = {New York, NY, USA},
	series = {{MULTIMEDIA} '03},
	title = {Contrast-based image attention analysis by using fuzzy growing},
	isbn = {978-1-58113-722-4},
	url = {https://doi.org/10.1145/957013.957094},
	doi = {10.1145/957013.957094},
	abstract = {Visual attention analysis provides an alternative methodology to semantic image understanding in many applications such as adaptive content delivery and region-based image retrieval. In this paper, we propose a feasible and fast approach to attention area detection in images based on contrast analysis. The main contributions are threefold: 1) a new saliency map generation method based on local contrast analysis is proposed; 2) by simulating human perception, a fuzzy growing method is used to extract attended areas or objects from the saliency map; and 3) a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points. This framework facilitates visual analysis tools or vision systems to automatically extract attentions from images in a manner like human perception. User study results indicate that the proposed approach is effective and practicable.},
	urldate = {2021-02-04},
	booktitle = {Proceedings of the eleventh {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Yu-Fei and Zhang, Hong-Jiang},
	month = nov,
	year = {2003},
	keywords = {attention detection, contrast analysis, fuzzy growing, image analysis, visual attention model},
	pages = {374--381},
}

@article{wermeskerken_what_2018,
	title = {What {Am} {I} {Looking} at? {Interpreting} {Dynamic} and {Static} {Gaze} {Displays}},
	volume = {42},
	copyright = {Copyright © 2017 The Authors. Cognitive Science published by Wiley Periodicals, Inc. on behalf of Cognitive Science Society.},
	issn = {1551-6709},
	shorttitle = {What {Am} {I} {Looking} at?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12484},
	doi = {https://doi.org/10.1111/cogs.12484},
	abstract = {Displays of eye movements may convey information about cognitive processes but require interpretation. We investigated whether participants were able to interpret displays of their own or others' eye movements. In Experiments 1 and 2, participants observed an image under three different viewing instructions. Then they were shown static or dynamic gaze displays and had to judge whether it was their own or someone else's eye movements and what instruction was reflected. Participants were capable of recognizing the instruction reflected in their own and someone else's gaze display. Instruction recognition was better for dynamic displays, and only this condition yielded above chance performance in recognizing the display as one's own or another person's (Experiments 1 and 2). Experiment 3 revealed that order information in the gaze displays facilitated instruction recognition when transitions between fixated regions distinguish one viewing instruction from another. Implications of these findings are discussed.},
	language = {en},
	number = {1},
	urldate = {2021-02-05},
	journal = {Cognitive Science},
	author = {Wermeskerken, Margot van and Litchfield, Damien and Gog, Tamara van},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12484},
	keywords = {Eye movements, Eye tracking, Gaze display, Gaze interpretation, Gaze recognition},
	pages = {220--252},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2021-02-08},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-02-08},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hinton_parallel_1981,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'81},
	title = {A parallel computation that assigns canonical object-based frames of reference},
	abstract = {A viewpoint-independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation, the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesise an object-based frame and then test the resultant shape description for familiarity. However, it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object-based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process.},
	urldate = {2021-02-08},
	booktitle = {Proceedings of the 7th international joint conference on {Artificial} intelligence - {Volume} 2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hinton, Geoffrey F.},
	month = aug,
	year = {1981},
	pages = {683--685},
}

@inproceedings{hinton_transforming_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transforming {Auto}-{Encoders}},
	isbn = {978-3-642-21735-7},
	doi = {10.1007/978-3-642-21735-7_6},
	abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2011},
	publisher = {Springer},
	author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
	editor = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
	year = {2011},
	keywords = {auto-encoder, Invariance, shape representation},
	pages = {44--51},
}

@incollection{yarbus_eye_1967,
	address = {Boston, MA},
	title = {Eye {Movements} {During} {Perception} of {Complex} {Objects}},
	isbn = {978-1-4899-5379-7},
	url = {https://doi.org/10.1007/978-1-4899-5379-7_8},
	language = {en},
	urldate = {2021-02-09},
	booktitle = {Eye {Movements} and {Vision}},
	publisher = {Springer US},
	author = {Yarbus, Alfred L.},
	editor = {Yarbus, Alfred L.},
	year = {1967},
	doi = {10.1007/978-1-4899-5379-7_8},
	keywords = {Complex Object, Optical Illusion, Retinal Image, Retinal Movement, Stationary Object},
	pages = {171--211},
}

@article{itti_model_1998,
	title = {A model of saliency-based visual attention for rapid scene analysis},
	volume = {20},
	issn = {1939-3539},
	doi = {10.1109/34.730558},
	abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Itti, L. and Koch, C. and Niebur, E.},
	month = nov,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Feature extraction, Image analysis, Layout, computer vision, target detection, neural nets, Biological system modeling, Computer architecture, Neural networks, Object detection, feature extraction, Brain modeling, dynamical neural network, Hardware, image recognition, rapid scene analysis, saliency, scene understanding, target tracking, topographical saliency map, visual attention, visual search, Visual system},
	pages = {1254--1259},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	month = dec,
	year = {2001},
	note = {ISSN: 1063-6919},
	keywords = {Face detection, Filters, Pixel, Robustness, image classification, object detection, learning (artificial intelligence), Detectors, Machine learning, image processing, machine learning, Object detection, Skin, feature extraction, AdaBoost, background regions, boosted simple feature cascade, classifiers, face detection, Focusing, image representation, Image representation, integral image, object specific focus-of-attention mechanism, rapid object detection, real-time applications, statistical guarantees, visual object detection},
	pages = {I--I},
}

@article{viola_robust_2004,
	title = {Robust {Real}-{Time} {Face} {Detection}},
	volume = {57},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000013087.49260.fb},
	doi = {10.1023/B:VISI.0000013087.49260.fb},
	abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
	language = {en},
	number = {2},
	urldate = {2021-02-09},
	journal = {International Journal of Computer Vision},
	author = {Viola, Paul and Jones, Michael J.},
	month = may,
	year = {2004},
	pages = {137--154},
}

@article{denil_learning_2012,
	title = {Learning {Where} to {Attend} with {Deep} {Architectures} for {Image} {Tracking}},
	volume = {24},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00312},
	doi = {10.1162/NECO_a_00312},
	abstract = {We discuss an attentional model for simultaneous object tracking and recognition that is driven by gaze data. Motivated by theories of perception, the model consists of two interacting pathways, identity and control, intended to mirror the what and where pathways in neuroscience models. The identity pathway models object appearance and performs classification using deep (factored)-restricted Boltzmann machines. At each point in time, the observations consist of foveated images, with decaying resolution toward the periphery of the gaze. The control pathway models the location, orientation, scale, and speed of the attended object. The posterior distribution of these states is estimated with particle filtering. Deeper in the control pathway, we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in our previous work, we introduce gaze selection strategies that operate in the presence of partial information and on a continuous action space. We show that a straightforward extension of the existing approach to the partial information setting results in poor performance, and we propose an alternative method based on modeling the reward surface as a gaussian process. This approach gives good performance in the presence of partial information and allows us to expand the action space from a small, discrete set of fixation points to a continuous domain.},
	number = {8},
	urldate = {2021-02-09},
	journal = {Neural Computation},
	author = {Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
	month = apr,
	year = {2012},
	note = {Publisher: MIT Press},
	pages = {2151--2184},
}

@article{bergstra_random_2012,
	title = {Random search for hyper-parameter optimization.},
	volume = {13},
	number = {2},
	journal = {Journal of machine learning research},
	author = {Bergstra, James and Bengio, Yoshua},
	year = {2012},
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	language = {en},
	urldate = {2021-02-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	pages = {91--99},
}

@article{jaderberg_spatial_2016,
	title = {Spatial {Transformer} {Networks}},
	url = {http://arxiv.org/abs/1506.02025},
	abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
	urldate = {2021-02-10},
	journal = {arXiv:1506.02025 [cs]},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
	month = feb,
	year = {2016},
	note = {arXiv: 1506.02025},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{itti_computational_2001,
	title = {Computational modelling of visual attention},
	volume = {2},
	copyright = {2001 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/35058500},
	doi = {10.1038/35058500},
	abstract = {We review recent work on computational models of focal visual attention, with emphasis on the bottom-up, saliency- or image-based control of attentional deployment. We highlight five important trends that have emerged from the computational literature: First, the perceptual saliency of stimuli critically depends on surrounding context; that is, the same object may or may not appear salient depending on the nature and arrangement of other objects in the scene. Computationally, this means that contextual influences, such as non-classical surround interactions, must be included in models. Second, a unique 'saliency map' topographically encoding for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Many successful models are based on such architecture, and electrophysiological as well as psychophysical studies have recently supported the idea that saliency is explicitly encoded in the brain. Third, inhibition-of-return (IOR), the process by which the currently attended location is transiently inhibited, is a critical element of attentional deployment. Without IOR, attention would endlessly be attracted towards the most salient stimulus. IOR thus implements a memory of recently visited locations, and allows attention to thoroughly scan our visual environment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. Understanding the interaction between overt and covert attention is particularly important for models concerned with visual search. Last, scene understanding and object recognition strongly constrain the selection of attended locations. Although several models have approached, in an information-theoretical sense, the problem of optimally deploying attention to analyse a scene, biologically plausible implementations of such a computational strategy remain to be developed.},
	language = {en},
	number = {3},
	urldate = {2021-02-12},
	journal = {Nature Reviews Neuroscience},
	author = {Itti, Laurent and Koch, Christof},
	month = mar,
	year = {2001},
	note = {Number: 3
Publisher: Nature Publishing Group},
	pages = {194--203},
}

@article{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	language = {en},
	urldate = {2021-02-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	urldate = {2021-02-15},
	booktitle = {{arXiv}:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{kolesnikov_big_2020,
	title = {Big {Transfer} ({BiT}): {General} {Visual} {Representation} {Learning}},
	shorttitle = {Big {Transfer} ({BiT})},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	year = {2020},
}

@inproceedings{cherti_out--class_2017,
	title = {Out-of-{Class} {Novelty} {Generation} : {An} {Experimental} {Foundation}},
	shorttitle = {Out-of-{Class} {Novelty} {Generation}},
	doi = {10.1109/ICTAI.2017.00197},
	abstract = {Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.},
	booktitle = {2017 {IEEE} 29th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Cherti, M. and Kégl, B. and Kazakçı, A.},
	month = nov,
	year = {2017},
	note = {ISSN: 2375-0197},
	keywords = {Measurement, learning (artificial intelligence), Training, Machine learning, Pipelines, actionable definition, computational creativity research, Creativity, deep neural nets, entire classes, evaluation of generative models, experimental setup, Gallium nitride, generative model, Generators, knowledge acquisition, knowledge driven creativity, machine learning perspective, novelty generator, out-of-class novelty generation, out-of-distribution novelty, training classes, value of novelty},
	pages = {1312--1319},
}

@article{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {http://arxiv.org/abs/2012.12877},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	urldate = {2021-03-29},
	journal = {arXiv:2012.12877 [cs]},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.12877},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shen_deep_2019,
	title = {Deep image reconstruction from human brain activity},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633},
	doi = {10.1371/journal.pcbi.1006633},
	abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
	language = {en},
	number = {1},
	urldate = {2021-05-05},
	journal = {PLOS Computational Biology},
	author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
	month = jan,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Imaging techniques, Neural networks, Optimization, Functional magnetic resonance imaging, Luminance, Sensory perception, Vision},
	pages = {e1006633},
}

@article{decenciere_teleophta_2013,
	series = {Special issue : {ANR} {TECSAN} : {Technologies} for {Health} and {Autonomy}},
	title = {{TeleOphta}: {Machine} learning and image processing methods for teleophthalmology},
	volume = {34},
	issn = {1959-0318},
	shorttitle = {{TeleOphta}},
	url = {https://www.sciencedirect.com/science/article/pii/S1959031813000237},
	doi = {10.1016/j.irbm.2013.01.010},
	abstract = {A complete prototype for the automatic detection of normal examinations on a teleophthalmology network for diabetic retinopathy screening is presented. The system combines pathological pattern mining methods, with specific lesion detection methods, to extract information from the images. This information, plus patient and other contextual data, is used by a classifier to compute an abnormality risk. Such a system should reduce the burden on readers on teleophthalmology networks.},
	language = {en},
	number = {2},
	urldate = {2021-05-05},
	journal = {IRBM},
	author = {Decencière, E. and Cazuguel, G. and Zhang, X. and Thibault, G. and Klein, J. -C. and Meyer, F. and Marcotegui, B. and Quellec, G. and Lamard, M. and Danno, R. and Elie, D. and Massin, P. and Viktor, Z. and Erginay, A. and Laÿ, B. and Chabouis, A.},
	month = apr,
	year = {2013},
	pages = {196--203},
}

@inproceedings{glorot_deep_2011,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
	language = {en},
	urldate = {2021-05-06},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {315--323},
}

@article{tolstikhin_mlp-mixer_2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	shorttitle = {{MLP}-{Mixer}},
	url = {http://arxiv.org/abs/2105.01601},
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	urldate = {2021-06-07},
	journal = {arXiv:2105.01601 [cs]},
	author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	month = may,
	year = {2021},
	note = {arXiv: 2105.01601},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{baum_capabilities_1988,
	title = {On the capabilities of multilayer perceptrons},
	volume = {4},
	issn = {0885-064X},
	url = {https://www.sciencedirect.com/science/article/pii/0885064X88900209},
	doi = {10.1016/0885-064X(88)90020-9},
	abstract = {What is the smallest multilayer perceptron able to compute arbitrary and random functions? Previous results show that a net with one hidden layer containing N − 1 threshold units is capable of implementing an arbitrary dichotomy of N points. A construction is presented here for implementing an arbitrary dichotomy with one hidden layer containing [Nd] units, for any set of N points in general position in d dimensions. This is in fact the smallest such net as dichotomies which cannot be implemented by any net with fewer units are described. Several constructions are presented of one-hidden-layer nets implementing arbitrary functions into the e-dimensional hypercube. One of these has only [4Nd][e[log2(Nd)]] units in its hidden layer. Arguments based on a function counting theorem of Cover establish that any net implementing arbitrary functions must have at least Nelog2(N) weights, so that no net with one hidden layer containing less than Ne/(d log2(N)) units will suffice. Simple counts also show that if the weights are only allowed to assume one of ng possible values, no net with fewer than Nelog2(ng) weights will suffice. Thus the gain coming from using real valued synapses appears to be only logarithmic. The circuit implementing functions into the e hypercube realizes such logarithmic gains. Since the counting arguments limit below only the number of weights, the possibility is suggested that, if suitable restrictions are imposed on the input vector set to avoid topological obstructions, two-hidden-layer nets with O(N) weights but only O(√N) threshold units might suffice for arbitrary dichotomies. Interesting and potentially sufficient restrictions include (a) if the vectors are binary, i.e., lie on the d hypercube or (b) if they are randomly and uniformly selected from a bounded region.},
	language = {en},
	number = {3},
	urldate = {2021-06-10},
	journal = {Journal of Complexity},
	author = {Baum, Eric B},
	month = sep,
	year = {1988},
	pages = {193--215},
}

@article{pelt_improving_2018,
	title = {Improving {Tomographic} {Reconstruction} from {Limited} {Data} {Using} {Mixed}-{Scale} {Dense} {Convolutional} {Neural} {Networks}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2313-433X/4/11/128},
	doi = {10.3390/jimaging4110128},
	abstract = {In many applications of tomography, the acquired data are limited in one or more ways due to unavoidable experimental constraints. In such cases, popular direct reconstruction algorithms tend to produce inaccurate images, and more accurate iterative algorithms often have prohibitively high computational costs. Using machine learning to improve the image quality of direct algorithms is a recently proposed alternative, for which promising results have been shown. However, previous attempts have focused on using encoder\&ndash;decoder networks, which have several disadvantages when applied to large tomographic images, preventing wide application in practice. Here, we propose the use of the Mixed-Scale Dense convolutional neural network architecture, which was specifically designed to avoid these disadvantages, to improve tomographic reconstruction from limited data. Results are shown for various types of data limitations and object types, for both simulated data and large-scale real-world experimental data. The results are compared with popular tomographic reconstruction algorithms and machine learning algorithms, showing that Mixed-Scale Dense networks are able to significantly improve reconstruction quality even with severely limited data, and produce more accurate results than existing algorithms.},
	language = {en},
	number = {11},
	urldate = {2021-06-25},
	journal = {Journal of Imaging},
	author = {Pelt, Daniël M. and Batenburg, Kees Joost and Sethian, James A.},
	month = nov,
	year = {2018},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {image reconstruction, deep learning, machine learning, tomography},
	pages = {128},
}

@inproceedings{roussel_recurrent_2019,
	title = {A {Recurrent} {Neural} {Network} for the {Prediction} of {Vital} {Sign} {Evolution} and {Sepsis} in {ICU}},
	booktitle = {2019 {Computing} in {Cardiology} ({CinC})},
	publisher = {IEEE},
	author = {Roussel, Benjamin and Behar, Joachim and Oster, Julien},
	year = {2019},
	pages = {Page--1},
}

@inproceedings{decenciere_dealing_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dealing with {Topological} {Information} {Within} a {Fully} {Convolutional} {Neural} {Network}},
	isbn = {978-3-030-01449-0},
	doi = {10.1007/978-3-030-01449-0_39},
	abstract = {A fully convolutional neural network has a receptive field of limited size and therefore cannot exploit global information, such as topological information. A solution is proposed in this paper to solve this problem, based on pre-processing with a geodesic operator. It is applied to the segmentation of histological images of pigmented reconstructed epidermis acquired via Whole Slide Imaging.},
	language = {en},
	booktitle = {Advanced {Concepts} for {Intelligent} {Vision} {Systems}},
	publisher = {Springer International Publishing},
	author = {Decencière, Etienne and Velasco-Forero, Santiago and Min, Fu and Chen, Juanjuan and Burdin, Hélène and Gauthier, Gervais and Laÿ, Bruno and Bornschloegl, Thomas and Baldeweck, Thérèse},
	editor = {Blanc-Talon, Jacques and Helbert, David and Philips, Wilfried and Popescu, Dan and Scheunders, Paul},
	year = {2018},
	keywords = {Convolutional neural network, Mathematical morphology, Geodesic operators, Histological image segmentation},
	pages = {462--471},
}

@inproceedings{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	urldate = {2021-09-29},
	booktitle = {{ICLR}},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.08803},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bergmann_mvtec_2021,
	title = {The {MVTec} {Anomaly} {Detection} {Dataset}: {A} {Comprehensive} {Real}-{World} {Dataset} for {Unsupervised} {Anomaly} {Detection}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {The {MVTec} {Anomaly} {Detection} {Dataset}},
	url = {https://doi.org/10.1007/s11263-020-01400-4},
	doi = {10.1007/s11263-020-01400-4},
	abstract = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the field of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec anomaly detection dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth annotations for all anomalies. We conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pretrained convolutional neural networks, as well as classical computer vision methods. We highlight the advantages and disadvantages of multiple performance metrics as well as threshold estimation techniques. This benchmark indicates that methods that leverage descriptors of pretrained networks outperform all other approaches and deep-learning-based generative models show considerable room for improvement.},
	language = {en},
	number = {4},
	urldate = {2021-10-01},
	journal = {Int J Comput Vis},
	author = {Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	month = apr,
	year = {2021},
	pages = {1038--1059},
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2021-10-01},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{duque-arias_power_2021,
	address = {Vienne (on line), Austria},
	title = {On power {Jaccard} losses for semantic segmentation},
	url = {https://hal.archives-ouvertes.fr/hal-03139997},
	abstract = {In this work, a new generalized loss function is proposed called power Jaccard to perform semantic segmentation tasks. It is compared with classical loss functions in different scenarios, including gray level and color image segmentation, as well as 3D point cloud segmentation. The results show improved performance, stability and convergence. We made available the code with our proposal with a demonstrative example.},
	urldate = {2021-10-19},
	booktitle = {{VISAPP} 2021 : 16th {International} {Conference} on {Computer} {Vision} {Theory} and {Applications}},
	author = {Duque-Arias, David and Velasco-Forero, Santiago and Deschaud, Jean-Emmanuel and Goulette, Francois and Serna, Andrés and Decencière, Etienne and Marcotegui, Beatriz},
	month = feb,
	year = {2021},
	keywords = {Image segmentation, deep learning, Jaccard loss, Loss functions, U-net architecture},
}

@article{alais_fast_2020,
	title = {Fast macula detection and application to retinal image quality assessment},
	volume = {55},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809419301417},
	doi = {10.1016/j.bspc.2019.101567},
	abstract = {In this article, we present a segmentation algorithm for assessing retinal image quality with respect to the visibility of the macular region. An image is considered of acceptable quality if the macular region is clearly visible and entirely in the field of view. Additionally, for acceptable images, the method is able to locate the fovea with a maximal error of 0.34 mm. The algorithm is based on a lightweight fully-convolutional network, several thousand times smaller than state-of-the-art networks investigated so far in preliminary studies. We obtain near-human performance for assessing macula visibility and fovea localization. The presented method can easily be embedded in tabletop or handheld retinographs, decreasing the number of ungradable images, saving both patient and physician time. It is an important step towards automatic screening of retinal pathologies, including diabetic retinopathy, which is a major global healthcare issue.},
	language = {en},
	urldate = {2021-10-19},
	journal = {Biomedical Signal Processing and Control},
	author = {Alais, Robin and Dokládal, Petr and Erginay, Ali and Figliuzzi, Bruno and Decencière, Etienne},
	month = jan,
	year = {2020},
	keywords = {Convolutional neural networks, Image quality assessment, Macula detection, Retinal imaging},
	pages = {101567},
}
