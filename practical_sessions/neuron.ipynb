{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single neuron tutorial\n",
    "\n",
    "This exercice aims at understanding the capacities of a single artificial neuron.\n",
    "\n",
    "<img src=\"graphics/neuron2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Magic used by the notebook to show figures inline\n",
    "%matplotlib inline\n",
    "# matplotlib default values\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random_seed = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for classification problems\n",
    "\n",
    "The data below correspond to classification problems. Each input dataset $X$ contains points of $\\mathbb{R}^2$. The output datasets $y$ give one of two classes, coded as $0$ or $1$, to each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display data\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "\n",
    "figure = plt.figure(figsize=(13, 4))\n",
    "cm = plt.cm.RdBu\n",
    "cm_red_blue = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Linearly separable\n",
    "X_lin, y_lin = make_blobs(n_features=2, random_state=random_seed, centers=[[-1, -1], [1, 1]])\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Gaussian clouds\", fontsize=24)\n",
    "ax.scatter(X_lin[:, 0], X_lin[:, 1], c=y_lin, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "# Moons\n",
    "X_moons, y_moons = make_moons(noise=0.1, random_state=random_seed)\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Moons\", fontsize=24)\n",
    "ax.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "# Circles (factor: ratio between inner and outer circles radius)\n",
    "X_circ, y_circ = make_circles(noise=0.1, factor=0.5, random_state=random_seed)\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Circles\", fontsize=24)\n",
    "ax.scatter(X_circ[:, 0], X_circ[:, 1], c=y_circ, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "datasets = {\"lin\": [X_lin, y_lin], \"circ\": [X_circ, y_circ], \"moons\": [X_moons, y_moons]}\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the data set that will be used in the following\n",
    "my_set = \"lin\"  # can be any of \"lin\", \"circ\" or \"moons\"\n",
    "X_sel = datasets[my_set][0]\n",
    "y_sel = datasets[my_set][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neuron\n",
    "\n",
    "This neuron model takes two inputs $\\textbf{x} = (x_0, x_1)$ , applies to them an affine transformation with weights $\\textbf{w} = (w_0, w_1)$ and bias $b$ and then sends the output through and activation function $g$:\n",
    "\n",
    "$$(x_1, x_2) \\mapsto g(b + w_0 x_0 + w_1 x_1)$$\n",
    "\n",
    "The loss function here is the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ArtificialNeuron2d(object):\n",
    "    \"\"\"Artificial neuron with two inputs.\n",
    "    \n",
    "    Activation: to be chosen at instantiation time.\n",
    "    Loss: squared error.\n",
    "    Optimization method: backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, act, random_state=None, sigma=0.01):\n",
    "        \"\"\"Initialization\n",
    "        \n",
    "        Parameters:\n",
    "        act: an activation class from activations.py.\n",
    "        random_state: None means that no random seed is given.\n",
    "        sigma: standard deviation of the normal random variable used for initializing the weights.\n",
    "        \"\"\"\n",
    "        self.activation = act() # act is a class; has to be instantiated\n",
    "        np.random.seed(random_state)\n",
    "        self.sigma = sigma\n",
    "        self.w, self.b = self.init_weights()\n",
    "        self.dL_dw = None\n",
    "        self.dL_db = None\n",
    "        self.dL_dt = None\n",
    "        self.dL_dy = None\n",
    "    \n",
    "    def init_weights(self):\n",
    "        return np.random.randn(2) * self.sigma, np.random.randn(1) * self.sigma\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Compute neuron prediction.\n",
    "        \n",
    "        x: numpy array containing 2 elements.\n",
    "        \"\"\"\n",
    "        return #TODO: return neuron prediction\n",
    "    \n",
    "    def print_gradients(self):\n",
    "        print(\"dL/dy = \", self.dL_dy)\n",
    "        print(\"dL/dt = \", self.dL_dt)\n",
    "        print(\"dL/db = \", self.dL_db)\n",
    "        print(\"dL/dw = \", self.dL_dw)\n",
    "    \n",
    "    def forward_back_propagation(self, x, y_gt, learning_rate, verbose=False):\n",
    "        \"\"\"Forward pass, backward pass with parameters update, on a single sample.\n",
    "        \n",
    "        x: numpy array containing a single training sample (two coordinates).\n",
    "        y_gt: expected output (ground-truth).\n",
    "        learning_rate: positive real value, typically smaller than 1.\n",
    "        verbose: toggles verbose mode\n",
    "        \n",
    "        Returns:\n",
    "        l: loss value for the given input (before back propagation)\n",
    "        \"\"\"\n",
    "        # Compute forward pass and local gradients\n",
    "        t = self.b + np.dot(self.w, x)\n",
    "        dt_dw = x\n",
    "        dt_db = 1\n",
    "        \n",
    "        y = self.activation(t)\n",
    "        dy_dt = self.activation.grad(t)\n",
    "        \n",
    "        L = np.square(y - y_gt).sum() # Note: this value is not used in the optimization. It is only returned by the function.\n",
    "        if verbose is True: print(\"Predicted output (y): \", y)\n",
    "        \n",
    "        # Back propagate gradient\n",
    "        self.dL_dy = 2 * (y - y_gt)\n",
    "        self.dL_dt = self.dL_dy * dy_dt\n",
    "        self.dL_db = self.dL_dt * dt_db\n",
    "        self.dL_dw = self.dL_dt * dt_dw\n",
    "        \n",
    "        # Update parameters\n",
    "        self.b = self.b - learning_rate * self.dL_db\n",
    "        self.w = self.w - learning_rate * self.dL_dw\n",
    "\n",
    "        return L\n",
    "        \n",
    "\n",
    "    def batch_forward_back_propagation(self, x, y_gt, learning_rate, verbose=False):\n",
    "        \"\"\"Forward pass, backward pass with parameters update, on mini-batch.\n",
    "        \n",
    "        x: numpy array containing n training samples.\n",
    "        y_gt: expected output (ground-truth).\n",
    "        learning_rate: positive real value, typically smaller than 1.\n",
    "        verbose: toggles verbose mode\n",
    "        \n",
    "        Returns:\n",
    "        l: loss value for the given batch (before back propagation)\n",
    "        \"\"\"\n",
    "        n = x.shape[0]\n",
    "        # Compute forward pass and local gradients\n",
    "        t = np.matmul(x, self.w) + self.b\n",
    "        dt_dw = np.mean(x, axis=0)\n",
    "        dt_db = 1\n",
    "        \n",
    "        y = self.activation(t)\n",
    "        dy_dt = np.mean(self.activation.grad(t))\n",
    "        \n",
    "        # Note: this value is not used in the optimization. It is only returned by the function.\n",
    "        L = np.square((y - y_gt) ** 2).sum()  # scalar\n",
    "        if verbose is True: print(\"Predicted output (y): \", y)\n",
    "        \n",
    "        # Back propagate gradient\n",
    "        self.dL_dy = 2 * np.mean(y - y_gt)\n",
    "        self.dL_dt = self.dL_dy * dy_dt\n",
    "        self.dL_db = self.dL_dt * dt_db\n",
    "        self.dL_dw = self.dL_dt * dt_dw\n",
    "        \n",
    "        # Update parameters\n",
    "        self.b = self.b - learning_rate * self.dL_db\n",
    "        self.w = self.w - learning_rate * self.dL_dw\n",
    "\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, epochs, learning_rate, batch=1, verbose=False):\n",
    "        \"\"\"Learn model using gradient descent.\n",
    "        \n",
    "        X: input examples\n",
    "        Y: output examples\n",
    "        learning_rate: learning rate\n",
    "        batch: size of mini-batch\n",
    "        verbose: toggle verbose mode\n",
    "        \"\"\"\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss_list = []\n",
    "            if batch == 1:\n",
    "                for (x, y) in zip(X, Y):\n",
    "                    epoch_loss_list += [self.forward_back_propagation(x, y, learning_rate)]\n",
    "            if batch > 1:\n",
    "                n = X.shape[0]  # number of samples\n",
    "                for i in range(n // batch):\n",
    "                    x = X[i * batch: (i+1) * batch]\n",
    "                    y = Y[i * batch: (i+1) * batch]\n",
    "                    epoch_loss_list += [self.batch_forward_back_propagation(x, y, learning_rate)]\n",
    "            if verbose is True: print(\"Epoch \", epoch, \" : loss = \", np.mean(epoch_loss_list))\n",
    "            loss_hist += [np.mean(epoch_loss_list)]\n",
    "                \n",
    "        return loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "We are now going to choose the activation that will be used by the neuron. Some among the most popular ones are avaible in the activations.py file. You can choose between:\n",
    "\n",
    "* ActivationSigmoid\n",
    "* ActivationTanh\n",
    "* ActivationRelu\n",
    "* ActivationIdentity\n",
    "\n",
    "__Question__: given the classification problem presented above, what activation would be the most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlia_tools import activations\n",
    "\n",
    "act = activations.ActivationSigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the artificial neuron\n",
    "\n",
    "We will now \"instantiate\" the neuron and see what the initial predictions are, without any adaptation to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate artificial neuron and begin playing with it\n",
    "an = ArtificialNeuron2d(act, random_state=random_seed)\n",
    "print(\"Model params:\", an.w, an.b)\n",
    "\n",
    "# Create grid for visualization\n",
    "h = 0.1\n",
    "x0_min = X_sel[:, 0].min() - h\n",
    "x0_max = X_sel[:, 0].max() + h\n",
    "x1_min = X_sel[:, 1].min() - h\n",
    "x1_max = X_sel[:, 1].max() + h\n",
    "xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, h), np.arange(x1_min, x1_max, h))\n",
    "xx0_ravel = xx0.ravel()\n",
    "xx1_ravel = xx1.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single epoch \n",
    "\n",
    "The objective of the training is that our simple model, made out of a single neuron, predicts for each point of $\\cal{R}^2$ its \"correct\" class.\n",
    "\n",
    "The model is trained using a gradient descent, point by point. The learning rate parameter plays an important role in the optimization procedure.\n",
    "\n",
    "We will first train for a single epoch, an see how the prediction on the whole grid evolves. The result can be compared to the precedent one, obtained with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single epoch - look at the evolution of the model\n",
    "loss = []\n",
    "learning_rate = 0.01\n",
    "for (x, y) in zip(X_sel, y_sel):\n",
    "    loss += [an.forward_back_propagation(x, y, learning_rate)]\n",
    "# print(\"Model params:\", an.w, an.b)\n",
    "print(\"Mean loss: \", np.mean(loss))\n",
    "# an.print_gradients()\n",
    "\n",
    "# predict on all grid points\n",
    "i = 0\n",
    "y_pred_ravel = np.zeros(len(xx0_ravel))\n",
    "for (x0, x1) in zip(xx0_ravel, xx1_ravel):\n",
    "    y_pred_ravel[i] = an.predict(np.array([x0, x1]))\n",
    "    i += 1\n",
    "y_pred = y_pred_ravel.reshape(xx0.shape)\n",
    "\n",
    "# Plot current model\n",
    "figure = plt.figure(figsize=(4, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.set_title(\"Current prediction\")\n",
    "ax.contourf(xx0, xx1, y_pred, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_sel[:, 0], X_sel[:, 1], c=y_sel, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a new neuron\n",
    "an = ArtificialNeuron2d(act, random_state=random_seed)\n",
    "\n",
    "# Train N epochs\n",
    "epochs = 200\n",
    "learning_rate = 0.1\n",
    "loss_hist = an.fit(X_sel, y_sel, epochs, learning_rate, batch=10 )\n",
    "print(\"Final loss: \", loss_hist[-1])\n",
    "\n",
    "# predict on all grid points\n",
    "i = 0\n",
    "for (x0, x1) in zip(xx0_ravel, xx1_ravel):\n",
    "    y_pred_ravel[i] = an.predict(np.array([x0, x1]))\n",
    "    i += 1\n",
    "y_pred = y_pred_ravel.reshape(xx0.shape)\n",
    "\n",
    "# Plot current model\n",
    "figure = plt.figure(figsize=(4, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.set_title(\"Current prediction\")\n",
    "ax.contourf(xx0, xx1, y_pred, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_sel[:, 0], X_sel[:, 1], c=y_sel, cmap=cm_red_blue, edgecolors='k')\n",
    "# ax.scatter(xx0, xx1, c=y_pred, cmap=cm_red_blue, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loss_hist))\n",
    "plt.plot(loss_hist)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_ravel[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
