{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent is the workhorse of machine learning. In this workshop we will develop the basic algorithms in the context of two common problems: a simple linear regression and logistic regression for binary classification. This workshop borrows some code from https://am207.github.io/2017/wiki/gradientdescent.html#batch-gradient-descent, and some inspiration from http://www.di.ens.fr/%7Efbach/orsay2018.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #1 - linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we need some data. We can use the `make_regression()` function from Python machine learning library *scikit-learn* (http://scikit-learn.org) to create a synthetic dataset. For illustrative purposes, we choose a single input variable, but everything we cover applies to higher dimensional data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_regression \n",
    "\n",
    "X, y = make_regression(n_samples=100, \n",
    "                       n_features=1,\n",
    "                       n_informative=1,\n",
    "                       noise=20,\n",
    "                       random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by visualising our data. `Matplotlib` is the paramount plotting library in Python, so let's import it into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# use vector graphics for a crisper plot!\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a scatter plot from our data. A typical workflow is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='x', ylabel='y')\n",
    "# plot data on new axis\n",
    "ax.scatter(X, y, color='blue', alpha=0.5)\n",
    "ax.set_title('Our dataset')\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our data, a feature matrix of $N$ samples in $d$ dimensions $\\mathbf{X} \\in \\mathbb{R}^{nxd}$ and output vector $y \\in \\mathbb{R}^{N}$, our objective will be to learn a linear function,\n",
    "\n",
    "$$\\hat{y}_i = \\beta_0 + \\beta_1x_i$$\n",
    "\n",
    "such that the sum of square errors $\\sum_i(y_i - \\hat{y}_i)^2$ is minimised. Each parameter corresponds to a column (feature) of $\\mathbf{X}$ and to learn an intercept term also, we append a column of 1s to $\\mathbf{X}$. This is called the *bias trick*:\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\dots & x_{1d} \\\\\n",
    "    x_{21} & x_{22} & \\dots & x_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{nd}\n",
    "\\end{bmatrix}}_{\\mathbf{X}} \\to\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & \\dots & x_{1d} \\\\\n",
    "    1 & x_{21} & x_{22} & \\dots & x_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\dots & x_{nd}\n",
    "\\end{bmatrix}}_{\\text{$\\mathbf{X}$ with bias variable}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# augment data with bias trick\n",
    "N = X.shape[0]\n",
    "X_bt = np.concatenate([np.ones((N, 1)), X], axis=1)\n",
    "# print the first few values\n",
    "print(X_bt[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B** In the following we will work with `X_bt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this basic setting there is an analytic solution to our minimisation problem. Recall that least squares linear regression uses a quadratic loss function:\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol\\beta) = \\frac{1}{2N}\\sum_{i=1}^N(y_i - \\boldsymbol\\beta^T\\mathbf{x}_i)^2 = \\frac{1}{2N}(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)$$\n",
    "\n",
    "With some vector calculus, we can differentiate and obtain the *normal equation*,\n",
    "\n",
    "$$\\boldsymbol\\beta^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "where $\\boldsymbol\\beta^*$ are the parameters minimising the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Calculate the optimal parameter vector $\\boldsymbol\\beta^*$ by solving the normal equation for `x_bt` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "beta = inv(X_bt.T.dot(X_bt)).dot(X_bt.T).dot(y)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Make prediction $\\hat{\\mathbf{y}} = \\mathbf{X}{\\boldsymbol\\beta^*}$ on training data using the learned parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_bt.dot(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise our regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='x', ylabel='y')\n",
    "# plot data on new axis\n",
    "ax.scatter(X, y, color='blue', alpha=0.5)\n",
    "# plot regression line\n",
    "ax.plot(X, y_pred, color='red')\n",
    "ax.set_title('Our dataset with regression line')\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** To quantify the error, we can use the mean square error (MSE). Implement this the quadratic loss function given above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(X, y, beta):\n",
    "    return (X.dot(beta) - y).dot(X.dot(beta) - y) / X.shape[0]\n",
    "\n",
    "mse = mean_square_error(X_bt, y, beta)\n",
    "print('MSE: %.02f RMSE: %.02f' % (mse, np.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the closeness of the RMSE and the noise parameter we used to create the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "A closed-form solution like least squares will not always be available to us, however, and we instead consider numerical optimisation techniques. *Gradient descent* finds a convex minimum by making progressive steps in the direction opposite the gradient. That is,\n",
    "\n",
    "$$\\boldsymbol\\beta_{k+1} = \\boldsymbol\\beta_{k} - \\alpha\\nabla_{\\boldsymbol\\beta}\\mathcal{L}$$\n",
    "\n",
    "for each iteration $k$ and some step size $\\alpha$, known as the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch gradient descent\n",
    "\n",
    "To perform a gradient descent, we need to be able to query the gradient at each iteration. Therefore, we implement a function to evaluate the gradient. The gradient of a linear regression is,\n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\beta}\\mathcal{L} = \\frac{1}{N}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol\\beta - y)$$\n",
    "\n",
    "which is an $\\mathcal{O}(ND)$ computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Implement a function to compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient(X, y, beta):\n",
    "    return X.T.dot(X.dot(beta) - y) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the function, let's look at the gradient at our least squares solution (it should be zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "print(norm(evaluate_gradient(X_bt, y, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Now we implement a batch gradient descent algorithm. Fill in the gradient update step where indicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, lr=1e-1, max_iters=30, tol=1e-2):\n",
    "    # randomly initialise beta\n",
    "    N, D = X.shape\n",
    "    beta = np.random.rand(D)\n",
    "    # initialise histories\n",
    "    losses = [mean_square_error(X, y, beta)]\n",
    "    betas = [beta.copy()]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        grad = evaluate_gradient(X, y, beta)\n",
    "        beta -= lr * grad\n",
    "        loss = mean_square_error(X, y, beta)\n",
    "        losses.append(loss)\n",
    "        betas.append(beta.copy())\n",
    "\n",
    "        if np.sqrt(grad.dot(grad)) < tol: break\n",
    "\n",
    "    return np.array(betas), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it on our data. Note the final parameters. How close are they to the OLS solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch gradient descent\n",
    "betas, losses = batch_gradient_descent(X_bt, y)\n",
    "print(betas[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='iteration', ylabel='loss')\n",
    "# plot data on new axis\n",
    "ax.plot(losses, color='blue', marker='*', alpha=0.5)\n",
    "ax.set_title('Gradient descent')\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make a nice 3-dimensional plot of our gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# create meshgrid\n",
    "ms = np.linspace(beta[0] - 20 , beta[0] + 20, 20)\n",
    "bs = np.linspace(beta[1] - 40 , beta[1] + 40, 40)\n",
    "M, B = np.meshgrid(ms, bs)\n",
    "zs = np.array([mean_square_error(X_bt, y, theta)\n",
    "               for theta in zip(np.ravel(M), np.ravel(B))])\n",
    "Z = zs.reshape(M.shape)\n",
    "\n",
    "# create 3D axis object\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d', xlabel='Intercept',\n",
    "                     ylabel='Slope', zlabel='Cost')\n",
    "\n",
    "# plot mse loss hypersurface and contours\n",
    "ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.2)\n",
    "ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)\n",
    "\n",
    "# plot start and end points\n",
    "ax.plot([betas[0][0]], [betas[0][1]], [losses[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7);\n",
    "ax.plot([betas[-1][0]], [betas[-1][1]], [losses[-1]] , markerfacecolor='r', marker='o', markersize=7);\n",
    "\n",
    "# plot gradient descent curves\n",
    "ax.plot(betas[:, 0], betas[:, 1], losses, markeredgecolor='r', marker='.', markersize=2);\n",
    "ax.plot(betas[:, 0], betas[:, 1], 0, markeredgecolor='r', marker='.', markersize=2);\n",
    "\n",
    "# set viewpoint\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strong Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong convexity implies a quadractic lower bound on a function. A multivariate function $\\mathcal{L}(\\boldsymbol\\beta)$ is $\\mu$-strongly convex iff $\\nabla_{\\boldsymbol\\beta\\boldsymbol\\beta}\\mathcal{L} \\succeq \\mu$ for $\\mu > 0$, that is, its Hessian matrix is positive definite. The gradient descent of a strongly convex function is guaranteed convergence with,\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol\\beta_t) - \\mathcal{L}(\\boldsymbol\\beta_*) \\leq (1 - \\mu/L)^{2t}[\\mathcal{L}(\\boldsymbol\\beta_t) - \\mathcal{L}(\\boldsymbol\\beta_*)]$$\n",
    "\n",
    "where $\\boldsymbol\\beta_t$ is the parameter vector at iteration $t$, $\\boldsymbol\\beta_*$ are the optimal parameters, and $\\mu$ and $L$, used as the learning rate, are respectively the smallest and largest eigenvalues of the $\\mathcal{L}$. That is, the *regret* converges exponentially to zero by the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** A matrix is positive definite iff its eigenvalues are strictly positive. For our quadratic loss function, the Hessian matrix, also known as the Gram matrix, is given by $$\\nabla_{\\beta\\beta}\\mathcal{L} = \\frac{1}{N}\\mathbf{X}^T\\mathbf{X}$$\n",
    "\n",
    "Compute the eigenvalues of the Gram matrix and verify the strong convexity of our regression's quadratic loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eigvals\n",
    "eigens = eigvals(X_bt.T.dot(X_bt) / X_bt.shape[0])\n",
    "print(eigens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Calculate a range of gradient descent losses including $1/L$, where $L$ is the largest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [1e-2, 1e-1, 1 / max(eigens)]\n",
    "# Now we run gradient descent for each\n",
    "_, losses1 = batch_gradient_descent(X_bt, y, lr=lr[0])\n",
    "_, losses2 = batch_gradient_descent(X_bt, y, lr=lr[1])\n",
    "_, losses3 = batch_gradient_descent(X_bt, y, lr=lr[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualise the losses. This time we use a log scale so as to observe the exponential improvement towards the least squares solution. We subtract the optimal mse computed above so that the losses converge to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='iteration', ylabel='loss - mse')\n",
    "# plot data on new axis\n",
    "ax.plot(losses1 - mse, color='red', marker='*', alpha=0.5, label='%.02f' % lr[0])\n",
    "ax.plot(losses2 - mse, color='green', marker='*', alpha=0.5, label='%.02f' % lr[1])\n",
    "ax.plot(losses3 - mse, color='blue', marker='*', alpha=0.5, label='%.02f' % lr[2])\n",
    "ax.set_title('Gradient descent with learning rates')\n",
    "ax.set_yscale('log')\n",
    "# add legend\n",
    "plt.legend()\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different learning rates (e.g. higher and lower) and observe the effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #2 - binary classification\n",
    "\n",
    "For this problem, we will use an external dataset. Download the following: http://www.di.ens.fr/%7Efbach/orsay2017/data_orsay_2017.mat. Although a MATLAB file, we can use a handy scipy routine to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We extract the relevant data from this dictionary object and print its dimensions:\n",
    "data, target = make_hastie_10_2()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target[:, None], test_size=0.1)\n",
    "\n",
    "print('X_train shape: %s' % str(X_train.shape))\n",
    "print('y_train shape: %s' % str(y_train.shape))\n",
    "print('X_test shape: %s' % str(X_test.shape))\n",
    "print('y_test shape: %s' % str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we perform the bias trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bt = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test_bt = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a logistic regression we encode the positive and negative classes as $y \\in \\{-1, 1\\}$. Then, $p(y = 1|\\mathbf{x}^T\\boldsymbol\\beta) = \\sigma(\\mathbf{x}^T\\boldsymbol\\beta)$ and $p(y = -1|\\mathbf{x}^T\\boldsymbol\\beta) = 1 - \\sigma(\\mathbf{x}^T\\boldsymbol\\beta) = \\sigma(-\\mathbf{x}^T\\boldsymbol\\beta)$, where $\\sigma(x) = 1/(1 + \\exp(-x))$ is the *sigmoid* or *logistic* function. To learn the model, we minimise the mean cross-entropy between the predicted distribution and the ground truth, which we can take to be *one-hot*, putting all probability on the correct class. This simplifies the loss function to,\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N\\log(1 + \\exp(-y_i\\mathbf{x}^T\\boldsymbol\\beta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Implement the loss function of a logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, exp\n",
    "\n",
    "def log_loss(X, y, beta):\n",
    "    return np.sum(log(1 + exp(-y * X.dot(beta)))) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Implement a function to return the logistic loss gradient:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal{L} = \\sum_{i=1}^N \\frac{-\\mathbf{x}_i\\cdot y_i}{1 + \\exp(y_i\\mathbf{x}_i^T\\beta)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient(X, y, beta):\n",
    "    return np.sum(-X * y / (1 + exp(y * X.dot(beta))), axis=0) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "In larger applications, computing the full gradient can be expensive. Moreover, a sample of size $M < N$ from a large dataset at each iteration is often sufficient to make an accurate descent step. *Minibatch gradient descent* uses a size $M$ subsample of the data at each iteration. In the extreme case that $M = 1$ we have what is known as *stochastic gradient descent*. The gradient computation therefore goes from a complexity of $\\mathcal{O}(ND)$ to $\\mathcal{O}(MD)$. In the training of deep neural networks, minibatch gradient descent (and its variants) is overwhelmingly the most popular approach, where a stochastic element additionally may help to avoid local minima of their non-convex loss functions.\n",
    "\n",
    "Operationally, the only thing that changes is the amount of data, we feed our gradient function, what we will call the *batch*. The most obvious strategy for selecting a batch is to cycle through the (pre-shuffled) data and slice the next $M$ values. Thus, at iteration $i$, we calculate the gradient as,\n",
    "\n",
    "$$\\nabla_{\\beta}\\mathcal{L}_{\\text{batch}} = \\frac{1}{M}\\mathbf{X}_{i:i+M}^T(\\mathbf{X}_{i:i+M}\\beta - y_{i:i+M})$$\n",
    "\n",
    "A full cycle of the training data is known as an *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Implement a cycling strategy for `minibatch_gradient_descent` with a given `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, batch_size=10, lr=1e-1, max_iters=1000, tol=1e-5):\n",
    "    # randomly initialise beta\n",
    "    N, D = X.shape\n",
    "    beta = np.random.rand(D, 1)\n",
    "    # initialise history variables\n",
    "    losses = [log_loss(X, y, beta)]\n",
    "    betas = [beta]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        start = i * batch_size % N\n",
    "        end = min(start + batch_size, N)\n",
    "        idx = np.arange(start, end)\n",
    "        batchX = X[idx]\n",
    "        batchY = y[idx]\n",
    "        grad = evaluate_gradient(batchX, batchY, beta)\n",
    "        grad = grad[:, np.newaxis]\n",
    "        beta -= lr * grad\n",
    "        if i % 10 == 0:\n",
    "            loss = log_loss(X, y, beta)\n",
    "            losses.append(loss)\n",
    "        betas.append(beta.copy())\n",
    "\n",
    "        if np.sqrt(grad.T.dot(grad)) < tol: break\n",
    "\n",
    "    return betas, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run minibatch gradient descent on our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch gradient descent\n",
    "betas, losses = minibatch_gradient_descent(X_train_bt, y_train, batch_size=10, lr=1e-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Calculate probabilties $\\hat{\\mathbf{p}} = \\sigma(\\mathbf{X}{\\beta})$ on our test data using the learned parameters. Use scipy's `expit` function or implement the sigmoid yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "beta = betas[-1]\n",
    "probs = expit(X_test_bt.dot(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each test sample $\\mathbf{x}_i$ the logistic function returns a probability $p_i \\in [0, 1]$. To make a prediction, we threshold the probabilities at 0.5: any probabilty $\\geq0.5$ we will say is positive; any $<0.5$ negative. Because we chose to encode our outputs as $y \\in {-1, 1}$, we will map our positives and negatives to $1$ and $-1$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = list(map(lambda x : 1 if x >= 0.5 else -1, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the accuracy of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Choose different batch sizes. Try $M = 1$ for pure stochastic gradient descent and try a large batch size for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = [1, 5, 100]\n",
    "_, losses1 = minibatch_gradient_descent(X_train, y_train, batch_size=bs[0])\n",
    "_, losses2 = minibatch_gradient_descent(X_train, y_train, batch_size=bs[1])\n",
    "_, losses3 = minibatch_gradient_descent(X_train, y_train, batch_size=bs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our descent curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='iteration', ylabel='loss')\n",
    "# plot data on new axis\n",
    "ax.plot(losses1, color='red', marker='.', alpha=0.5, label='M = %s'%bs[0])\n",
    "ax.plot(losses2, color='green', marker='.', alpha=0.5, label='M = %s'%bs[1])\n",
    "ax.plot(losses3, color='blue', marker='.', alpha=0.5, label='M = %s'%bs[2])\n",
    "ax.set_title('Stochastic gradient descent')\n",
    "# display lengend\n",
    "plt.legend()\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** An alternative to our cycling strategy is to randomly sample a batch at each iteration. Numpy's `randint` function can be used to sample integers (with replacement) from a range. Use this to implement a sampling strategy for minibatch gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "def minibatch_gradient_descent_sampling(X, y, batch_size=10, lr=1e-1, max_iters=1000, tol=1e-5):\n",
    "    # randomly initialise beta\n",
    "    N, D = X.shape\n",
    "    beta = np.random.rand(D, 1)\n",
    "    # initialise history variables\n",
    "    losses = [log_loss(X, y, beta)]\n",
    "    betas = [beta]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        idx = np.random.randint(X.shape[0], size=batch_size)\n",
    "        batchX = X[idx]\n",
    "        batchY = y[idx]\n",
    "        grad = evaluate_gradient(batchX, batchY, beta)\n",
    "        grad = grad[:, np.newaxis]\n",
    "        beta -= lr * grad\n",
    "        if i % 10 == 0:\n",
    "            loss = log_loss(X, y, beta)\n",
    "            losses.append(loss)\n",
    "        betas.append(beta.copy())\n",
    "\n",
    "        if np.sqrt(grad.T.dot(grad)) < tol: break\n",
    "\n",
    "    return betas, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compare the two strageties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = [1, 10]\n",
    "_, losses1 = minibatch_gradient_descent(X_train, y_train, batch_size=bs[0])\n",
    "_, losses2 = minibatch_gradient_descent_sampling(X_train, y_train, batch_size=bs[0])\n",
    "_, losses3 = minibatch_gradient_descent(X_train, y_train, batch_size=bs[1])\n",
    "_, losses4 = minibatch_gradient_descent_sampling(X_train, y_train, batch_size=bs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "# add subplot (rows, cols, number)\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='iteration', ylabel='loss')\n",
    "# plot data on new axis\n",
    "ax.plot(losses1, color='red', marker='.', alpha=0.5, label='Cycling (M = %d)' % bs[0])\n",
    "ax.plot(losses2, color='blue', marker='.', alpha=0.5, label='Sampling (M = %d)' % bs[0])\n",
    "ax.plot(losses3, color='red', marker='*', alpha=0.5, label='Cycling (M = %d)' % bs[1])\n",
    "ax.plot(losses4, color='blue', marker='*', alpha=0.5, label='Sampling (M = %d)' % bs[1])\n",
    "# display plot\n",
    "ax.set_title('Cycling vs. sampling minibatch gradient descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
