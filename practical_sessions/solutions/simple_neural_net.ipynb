{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural net tutorial\n",
    "\n",
    "In this notebook, we will program a 2 layer neural network. It will classify two dimensional data into two classes.\n",
    "\n",
    "![Simple neural network](graphics/simple_nn.png)\n",
    "\n",
    "\n",
    "The first layer will contain $q$ neurons and the second one $r$. Thus:\n",
    "\n",
    "- $\\mathbf{x}$ is a vector containing two elements;\n",
    "- $\\mathbf{W}_1$ is a matrix of size $(q, 2)$ and $\\mathbf{b}_1$ a vector of size $q$;\n",
    "- $\\mathbf{W}_2$ is a matrix of size $(r, 2)$ and $\\mathbf{b}_2$ a vector of size $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Magic used by the notebook to show figures inline\n",
    "%matplotlib inline\n",
    "# matplotlib default values\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random_seed = 4\n",
    "\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for classification problems\n",
    "\n",
    "The data below correspond to classification problems. Each input dataset $\\mathbf{X}$ contains points of $\\mathbb{R}^2$. The output datasets $\\mathbf{y}$ give one of two classes, coded as $0$ or $1$, to each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display data\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "\n",
    "figure = plt.figure(figsize=(13, 4))\n",
    "cm = plt.cm.RdBu\n",
    "cm_red_blue = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Linearly separable\n",
    "X_lin, y_lin = make_blobs(n_features=2, random_state=random_seed, centers=[[-1, -1], [1, 1]])\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Gaussian clouds\", fontsize=24)\n",
    "ax.scatter(X_lin[:, 0], X_lin[:, 1], c=y_lin, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "# Moons\n",
    "X_moons, y_moons = make_moons(noise=0.1, random_state=random_seed)\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Moons\", fontsize=24)\n",
    "ax.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "# Circles (factor: ratio between inner and outer circles radius)\n",
    "X_circ, y_circ = make_circles(noise=0.1, factor=0.5, random_state=random_seed)\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Circles\", fontsize=24)\n",
    "ax.scatter(X_circ[:, 0], X_circ[:, 1], c=y_circ, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "\n",
    "datasets = {\"lin\": [X_lin, y_lin], \"circ\": [X_circ, y_circ], \"moons\": [X_moons, y_moons]}\n",
    "\n",
    "plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing the data set that will be used in the following\n",
    "my_set = \"moons\"  # can be any of \"lin\", \"circ\" or \"moons\"\n",
    "X_sel = datasets[my_set][0]\n",
    "y_sel = datasets[my_set][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(object):\n",
    "    \"\"\"Two-layer neural network, with two inputs and one output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, q, r, act1, act2, random_state=None, sigma=0.01):\n",
    "        \"\"\"Neural network initialization.\n",
    "        \n",
    "        Arguments:\n",
    "           q, r: number of neurons in the each layer.\n",
    "           act1, act2: classes from activations.py used to instantiate the first and second activations.\n",
    "           random_state: any real value. None means that no random seed is given.\n",
    "           sigma: standard deviation of the normal random variable used for initializing the weights\n",
    "           \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.sigma = sigma\n",
    "        self.activation1 = act1()\n",
    "        self.activation2 = act2()\n",
    "\n",
    "        self.W1 = np.random.randn(q, 2) * self.sigma\n",
    "        self.b1 = np.random.rand(q)\n",
    "        self.W2 = np.random.randn(r, q) * self.sigma\n",
    "        self.b2 = np.random.rand(r)\n",
    "\n",
    "        self.dl_dy2 = None\n",
    "        self.dl_dt2 = None\n",
    "        self.dl_dW2 = None\n",
    "        self.dl_db2 = None\n",
    "        self.dl_dy1 = None\n",
    "        self.dl_dt1 = None\n",
    "        self.dl_dW1 = None\n",
    "        self.dl_db1 = None\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Compute NN prediction.\n",
    "        \n",
    "        x: numpy array containing 2 elements.\n",
    "        \"\"\"\n",
    "        t1 = np.matmul(self.W1, x) + self.b1\n",
    "        y1 = self.activation1(t1)\n",
    "        t2 = np.matmul(self.W2, y1) + self.b2\n",
    "        y2 = self.activation2(t2)\n",
    "        \n",
    "        return y2\n",
    "\n",
    "    \n",
    "    def forward_back_propagation(self, x, y_gt, learning_rate, verbose=False):\n",
    "        \"\"\"Forward pass, backward pass with parameters update, on a single sample.\n",
    "        \n",
    "        x: numpy array containing a single training sample (two coordinates).\n",
    "        y_gt: expected output (ground-truth).\n",
    "        learning_rate: positive real value, typically smaller than 1.\n",
    "        verbose: toggles verbose mode\n",
    "        \n",
    "        Returns:\n",
    "        l: loss value for the given input (before back propagation)\n",
    "        \"\"\"\n",
    "        # Compute forward pass and local gradients\n",
    "        t1 = np.matmul(self.W1, x) + self.b1\n",
    "        dt1_dW1_transp = x\n",
    "        dt1_db1 = np.full(q, 1)\n",
    "\n",
    "        y1 = self.activation1(t1)\n",
    "        dy1_dt1 = self.activation1.grad(t1)\n",
    "        \n",
    "        t2 = np.matmul(self.W2, y1) + self.b2\n",
    "        dt2_dy1 = self.W2\n",
    "        dt2_dW2_transp = y1\n",
    "        dt2_db2 = np.full(r, 1)\n",
    "\n",
    "        y2 = self.activation2(t2)\n",
    "        dy2_dt2 = self.activation2.grad(t2)\n",
    "        \n",
    "        l = np.square(y2 - y_gt).sum() # Note: this value is not used in the optimization. It is only returned by the function.\n",
    "        if verbose is True: print(\"Predicted output (y): \", y)\n",
    "        \n",
    "        # Back propagate gradient\n",
    "        self.dl_dy2 = 2 * (y2 - y_gt)\n",
    "        self.dl_dt2 = self.dl_dy2 * dy2_dt2\n",
    "        self.dl_dW2 = np.matmul(self.dl_dt2[..., np.newaxis], dt2_dW2_transp[np.newaxis,...])  # Waiting for @\n",
    "        self.dl_db2 = self.dl_dt2\n",
    "        self.dl_dy1 = np.matmul(self.dl_dt2, dt2_dy1)\n",
    "        self.dl_dt1 = self.dl_dy1 * dy1_dt1\n",
    "        self.dl_dW1 = (np.matmul(self.dl_dt1[..., np.newaxis], dt1_dW1_transp[np.newaxis,...]))\n",
    "        self.dl_db1 = self.dl_dy1 * dy1_dt1\n",
    "        \n",
    "        # Update parameters\n",
    "        self.b1 = self.b1 - learning_rate * self.dl_db1\n",
    "        self.W1 = self.W1 - learning_rate * self.dl_dW1\n",
    "        self.b2 = self.b2 - learning_rate * self.dl_db2\n",
    "        self.W2 = self.W2 - learning_rate * self.dl_dW2\n",
    "\n",
    "        return l\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y, epochs, learning_rate, verbose=False):\n",
    "        \"\"\"Learn model using gradient descent.\n",
    "        \n",
    "        X: input examples\n",
    "        Y: output examples\n",
    "        learning_rate: learning rate\n",
    "        batch: size of mini-batch\n",
    "        verbose: toggle verbose mode\n",
    "        \"\"\"\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss_list = []\n",
    "            for (x, y) in zip(X, Y):\n",
    "                epoch_loss_list += [self.forward_back_propagation(x, y, learning_rate)]\n",
    "            if verbose is True: print(\"Epoch \", epoch, \" : loss = \", np.mean(epoch_loss_list))\n",
    "            loss_hist += [np.mean(epoch_loss_list)]\n",
    "                \n",
    "        return loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlia_tools import activations\n",
    "\n",
    "act1 = activations.ActivationRelu\n",
    "act2 = activations.ActivationRelu\n",
    "\n",
    "q = 8\n",
    "r = 1\n",
    "\n",
    "nn = SimpleNeuralNet(q, r, act1, act2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for visualization\n",
    "h = 0.1\n",
    "x0_min = X_sel[:, 0].min() - h\n",
    "x0_max = X_sel[:, 0].max() + h\n",
    "x1_min = X_sel[:, 1].min() - h\n",
    "x1_max = X_sel[:, 1].max() + h\n",
    "xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, h), np.arange(x1_min, x1_max, h))\n",
    "xx0_ravel = xx0.ravel()\n",
    "xx1_ravel = xx1.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single epoch - look at the evolution of the model\n",
    "loss = []\n",
    "learning_rate = 0.1\n",
    "for (x, y) in zip(X_sel, y_sel):\n",
    "    loss += [nn.forward_back_propagation(x, y, learning_rate)]\n",
    "\n",
    "# predict on all grid points\n",
    "i = 0\n",
    "y_pred_ravel = np.zeros(len(xx0_ravel))\n",
    "for (x0, x1) in zip(xx0_ravel, xx1_ravel):\n",
    "    y_pred_ravel[i] = nn.predict(np.array([x0, x1]))\n",
    "    i += 1\n",
    "y_pred = y_pred_ravel.reshape(xx0.shape)\n",
    "\n",
    "# Plot current model\n",
    "figure = plt.figure(figsize=(4, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.set_title(\"Current prediction\")\n",
    "ax.contourf(xx0, xx1, y_pred, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_sel[:, 0], X_sel[:, 1], c=y_sel, cmap=cm_red_blue,\n",
    "               edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SimpleNeuralNet(q, r, act1, act2, random_state=random_seed)\n",
    "\n",
    "# Train N epochs\n",
    "epochs = 200\n",
    "learning_rate = 0.1\n",
    "loss_hist = nn.fit(X_sel, y_sel, epochs, learning_rate)\n",
    "print(\"Final loss: \", loss_hist[-1])\n",
    "\n",
    "# predict on all grid points\n",
    "i = 0\n",
    "for (x0, x1) in zip(xx0_ravel, xx1_ravel):\n",
    "    y_pred_ravel[i] = nn.predict(np.array([x0, x1]))\n",
    "    i += 1\n",
    "y_pred = y_pred_ravel.reshape(xx0.shape)\n",
    "\n",
    "# Plot current model\n",
    "figure = plt.figure(figsize=(4, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.set_title(\"Current prediction\")\n",
    "ax.contourf(xx0, xx1, y_pred, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_sel[:, 0], X_sel[:, 1], c=y_sel, cmap=cm_red_blue, edgecolors='k')\n",
    "# ax.scatter(xx0, xx1, c=y_pred, cmap=cm_red_blue, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
