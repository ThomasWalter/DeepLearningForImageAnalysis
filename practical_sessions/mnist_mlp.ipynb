{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying digits using a fully connected neural network\n",
    "\n",
    "In this practical exercice a fully connected neural network (also called multi-layer perceptron) is built using keras. It is then trained to classify image digits from the MNIST database.\n",
    "\n",
    "Some baseline results:\n",
    "\n",
    "| Method                                                                      | Test error (%) |\n",
    "|-----------------------------------------------------------------------------|---------------:|\n",
    "| Linear classifier (LeCun et al. 1998)                                       |           12.0 |\n",
    "| K-nearest-neighbors, Euclidean (L2) (LeCun et al. 1998)                     |            5.0 |\n",
    "| 3-layer NN, 500-300, softmax, cross entropy, weight decay (Hinton, 2005)    |            1.5 |\n",
    "| Convolutional net LeNet-4 (LeCun et al. 1998)                               |            1.1 |\n",
    "| Virtual SVM deg-9 poly [data augmentation] (LeCun et al. 1998)              |            0.8 |\n",
    "| 6-layer NN with [data augmentation] (Ciresan et al. 2010)                   |           0.35 |\n",
    "| Deep conv. net, 7 layers [data augmentation] (Ciresan et al. IJCAI 2011)    |           0.35 |\n",
    "\n",
    "More results are available from: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Try to improve on some of these results, at least on those that do not use data augmentation or convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist as db\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Magic used by the notebook to show figures inline\n",
    "%matplotlib inline\n",
    "# matplotlib default values\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and have a look at the data\n",
    "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
    "\n",
    "# Visualize a single digit, with its class\n",
    "plt_r,plt_c = 4,4\n",
    "f, ax = plt.subplots(plt_r,plt_c, figsize=(16,16))\n",
    "for i in range(plt_r):\n",
    "    for j in range(plt_c):\n",
    "        index = np.random.randint(x.shape[0])\n",
    "        ax[i][j].imshow(x[index])\n",
    "        ax[i][j].axis('off')\n",
    "        ax[i][j].set_title(\"Example: {}, Class: {}\".format(index, y[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "val_nb = 5000  # number of validation samples\n",
    "nb_samples = x.shape[0]\n",
    "\n",
    "if val_nb > nb_samples:\n",
    "    raise ValueError(\"You need some samples to train your network!\")\n",
    "\n",
    "x = x.reshape(nb_samples, 784)\n",
    "x_test = x_test_ori.reshape(x_test_ori.shape[0], 784)\n",
    "x = x.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_val = x[:val_nb, ]\n",
    "x_train = x[val_nb:, ]\n",
    "y_val = y[:val_nb]\n",
    "y_train = y[val_nb:]\n",
    "\n",
    "print(x_train.shape, 'x train samples')\n",
    "print(x_val.shape, 'x val samples')\n",
    "print(x_test.shape, 'x test samples')\n",
    "print(y_train.shape, 'y train samples')\n",
    "print(y_val.shape, 'y val samples')\n",
    "print(y_test_ori.shape, 'y test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = max(y) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test_ori, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "The following model uses keras to build a fully convolutional network. It has to respect some constraints:\n",
    "\n",
    "- The input shape has to match the size of each input sample. \n",
    "- The ouptput should be of size 10 (num_classes)\n",
    "\n",
    "Other than that, you change the number of layers as well as the size of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let define a first simple model without any hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(num_classes, activation='softmax', input_shape=(x_train.shape[1],)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "The following section takes care of training.\n",
    "\n",
    "Firstly, the model has to be 'compiled'. This operations lets the user choose the loss, the optimizer and the metrics, then configures the model for training.\n",
    "\n",
    "Secondly, the 'fit' method runs the optmization. Training and validation data are specified here, as well as batch size and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "output = model.fit(x_train, y_train,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(x_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the weights\n",
    "\n",
    "With this simple model is possible to have an intuition of what the neural network has learned looking at\n",
    "the matrix of the weights $W$. \n",
    "In fact $W\\in\\mathbb{R}^{784\\times 10}$, so for each class $i$ we can plot the weights corresponding to this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the weights \n",
    "W, b = model.layers[0].get_weights()\n",
    "vmin = W.min()\n",
    "vmax = W.max()\n",
    "f, ax = plt.subplots(2, 5, figsize=(16,6))\n",
    "for plt_row in range(2):\n",
    "    for plt_col in range(5):\n",
    "        ax[plt_row][plt_col].imshow(W[:,plt_row*5 + plt_col].reshape(28,28), vmin=vmin, vmax=vmax, cmap=plt.cm.bwr)\n",
    "        ax[plt_row][plt_col].axis('off')\n",
    "        ax[plt_row][plt_col].set_title(\"Weights for the class {}\".format(plt_row*5 + plt_col))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imroving performances adding hidden layers\n",
    "\n",
    "In order to improve the performances of our prediction is possible to add hidden layers between the input layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "output = model.fit(x_train, y_train,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(x_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the results\n",
    "\n",
    "Visualizing what is going on is extremely important. For that:\n",
    "\n",
    "- inspecting traning and validation performance is essential;\n",
    "\n",
    "- looking at the errors might also be interesting.\n",
    "\n",
    "Is there overfitting? How can it be reduced?\n",
    "Is the network 'confident' when making errors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output.epoch, output.history['loss'], label='train')\n",
    "plt.plot(output.epoch, output.history['val_loss'], label='val')\n",
    "plt.title('Training and validation performance')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "# plt.ylim(0.2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba = model.predict(x_test)\n",
    "# print(\"Prediction example:\", y_predict_proba[0])\n",
    "\n",
    "y_predict = np.argmax(y_predict_proba, 1)\n",
    "# print(y_predict[3000:3010])\n",
    "\n",
    "diff = y_test_ori != y_predict\n",
    "# print(\"Difference mask: \", diff)\n",
    "x_test_errors = x_test_ori[diff]\n",
    "y_test_errors = y_test_ori[diff]\n",
    "y_predict_errors = y_predict[diff]\n",
    "y_predict_proba_errors = y_predict_proba[diff]\n",
    "\n",
    "index = 0\n",
    "print(\"Correct label is: \", y_test_errors[index])\n",
    "print(\"Predicted label is: \", y_predict_errors[index])\n",
    "print(\"Probabilities: \", y_predict_proba_errors[index])\n",
    "plt.imshow(x_test_errors[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is the last stage of the learning process. Good practice recommends to do it only once, when you have completely finished with the optimization of the network parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with a more complex database\n",
    "\n",
    "In the second cell, you can replace:\n",
    "\n",
    "<code>from keras.datasets import mnist as db</code>\n",
    "\n",
    "with:\n",
    "\n",
    "<code>from keras.datasets import fashion_mnist as db</code>\n",
    "\n",
    "in order to experiment with a more complex database. The best test accuracy reported on this database is 0.967 (see https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "You can use the following dictionary to transform number labels into meaningfull labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dict = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "    }\n",
    "\n",
    "print(fashion_dict[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "If you run into overfitting problems, you can try to regularize your network. You can use L1 and L2 regularization and try different regularization weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2\n",
    "\n",
    "reg_weight = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(\n",
    "        128,\n",
    "        kernel_regularizer=l1(reg_weight),\n",
    "        activation='relu', \n",
    "        input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(\n",
    "        128,\n",
    "        kernel_regularizer=l1(reg_weight),\n",
    "        activation='relu'))\n",
    "model.add(Dense(\n",
    "        num_classes,\n",
    "        kernel_regularizer=l1(reg_weight),\n",
    "        activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
