{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying digits using a fully connected neural network\n",
    "\n",
    "In this practical exercice we will build a fully convolutional neural network using keras. We will train it to classify image digits from the MNIST database.\n",
    "\n",
    "Based on https://raw.githubusercontent.com/keras-team/keras/master/examples/mnist_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Magic used by the notebook to show figures inline\n",
    "%matplotlib inline\n",
    "# matplotlib default values\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# auto-reloading packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "val_nb = 5000  # number of validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and have a look at the data\n",
    "(x, y), (x_test_ori, y_test_ori) = mnist.load_data()\n",
    "\n",
    "# Visualize a single digit, with its class\n",
    "index = 2\n",
    "plt.imshow(x[index])\n",
    "print(\"Class: \", y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "\n",
    "nb_samples = x.shape[0]\n",
    "\n",
    "if val_nb > nb_samples:\n",
    "    raise ValueError(\"You need some samples to train your network!\")\n",
    "\n",
    "x = x.reshape(nb_samples, 784)\n",
    "x_test = x_test_ori.reshape(x_test_ori.shape[0], 784)\n",
    "x = x.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_val = x[:val_nb, ]\n",
    "x_train = x[val_nb:, ]\n",
    "y_val = y[:val_nb]\n",
    "y_train = y[val_nb:]\n",
    "\n",
    "print(x_train.shape, 'x train samples')\n",
    "print(x_val.shape, 'x val samples')\n",
    "print(x_test.shape, 'x test samples')\n",
    "print(y_train.shape, 'y train samples')\n",
    "print(y_val.shape, 'y val samples')\n",
    "print(y_test_ori.shape, 'y test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = max(y) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test_ori, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "The following model uses keras to build a fully convolutional network. It has to respect some constraints:\n",
    "\n",
    "- The input shape has to match the size of each input sample. \n",
    "- The ouptput should be of size 10 (num_classes)\n",
    "\n",
    "Other than that, you change the number of layers as well as the size of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "The following model takes care of training.\n",
    "\n",
    "Firstly, the model has to be 'compiled'. This operations lets the user choose the loss, the optmizer and the metrics, then configures the model for training.\n",
    "\n",
    "Secondly, the 'fit' method run the optmization. Traning and validation data are specified here, as well as batch size and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "output = model.fit(x_train, y_train,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(x_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the results\n",
    "\n",
    "Visualizing what is going on is extremely important. For that:\n",
    "\n",
    "- inspecting traning and validation performance is essential;\n",
    "\n",
    "- looking at the errors might also be interesting.\n",
    "\n",
    "Is there overfitting? How can it be reduced?\n",
    "Is the network 'confident' when making errors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output.epoch, output.history['loss'], label='train')\n",
    "plt.plot(output.epoch, output.history['val_loss'], label='val')\n",
    "plt.title('Training and validation performance')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "# plt.ylim(0.2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba = model.predict(x_test)\n",
    "# print(\"Prediction example:\", y_predict_proba[0])\n",
    "\n",
    "y_predict = np.argmax(y_predict_proba, 1)\n",
    "# print(y_predict[3000:3010])\n",
    "\n",
    "diff = y_test_ori != y_predict\n",
    "# print(\"Difference mask: \", diff)\n",
    "x_test_errors = x_test_ori[diff]\n",
    "y_test_errors = y_test_ori[diff]\n",
    "y_predict_errors = y_predict[diff]\n",
    "y_predict_proba_errors = y_predict_proba[diff]\n",
    "\n",
    "index = 0\n",
    "print(\"Correct label is: \", y_test_errors[index])\n",
    "print(\"Predicted label is: \", y_predict_errors[index])\n",
    "print(\"Probabilities: \", y_predict_proba_errors[index])\n",
    "plt.imshow(x_test_errors[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is the last stage of the learning process. Good practice recommends to do it only once, when you have completely finised with the optimization of the network parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
