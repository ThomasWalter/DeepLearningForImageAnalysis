{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules in the MNIST_FASHION Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will study the effect of a Learning Rate Policy on the SGD\n",
    "for the MNIST-FASHION Database.\n",
    "\n",
    "1. Check the difficulty for Classical SGD to learn in this example,\n",
    "2. Check how CyclicLR helps SGD to find the good path!,\n",
    "3. Can you improve the performance of your networks by changing the optimizer from: SGD, ADAM, NADAM, RMSprop?\n",
    "4. What if you use 'triangular2' instead of 'triangular' Learning Rate Policy? \n",
    "5. What are the top-3 methods according to accuracy on Training dataset?\n",
    "6. What are the top-3 methods according to accuracy on Validation dataset?\n",
    "7. What are the top-3 methods according to accuracy on Testing dataset?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs=25 #Number of Epochs\n",
    "ini_LR=.1 #Initial value of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"SOURCE: https://github.com/bckenstler/CLR\"\"\"\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization, Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=np.random.randint(300))\n",
    "\n",
    "x_train=np.expand_dims(x_train,3)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "x_val=np.expand_dims(x_val,3)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "\n",
    "x_test=np.expand_dims(x_test,3)\n",
    "#y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "x_val=x_val/255\n",
    "input_shape=(28,28,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getModel(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(8, (3,3), strides=(1,1), activation='relu',kernel_initializer='glorot_normal')(X_input)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Conv2D(16, (3,3), strides=(1,1), activation='relu',kernel_initializer='glorot_normal')(X)\n",
    "    X = MaxPooling2D((2,2))(X)    \n",
    "    X = Conv2D(32, (2,2), strides=(1,1), activation='relu',kernel_initializer='glorot_normal')(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(.3)(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "    model = Model(inputs=[X_input], outputs=[X])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 24,298\n",
      "Trainable params: 24,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=getModel(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 9s 196us/step - loss: 6.5935 - acc: 0.1186 - val_loss: 2.3028 - val_acc: 0.0972\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 9s 188us/step - loss: 2.3030 - acc: 0.0986 - val_loss: 2.3036 - val_acc: 0.0993\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 9s 190us/step - loss: 2.3030 - acc: 0.0995 - val_loss: 2.3031 - val_acc: 0.0993\n",
      "Epoch 4/25\n",
      "21824/48000 [============>.................] - ETA: 4s - loss: 2.3031 - acc: 0.0985"
     ]
    }
   ],
   "source": [
    "model=getModel(input_shape)\n",
    "model.compile(optimizer=SGD(ini_LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "training=model.fit(x_train, y_train, batch_size=64, epochs=num_epochs, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = training.history['loss']\n",
    "val_loss = training.history['val_loss']\n",
    "acc = training.history['acc']\n",
    "val_acc = training.history['val_acc']\n",
    "\n",
    "# loss plot\n",
    "tra = plt.plot(loss)\n",
    "val = plt.plot(val_loss, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# accuracy plot\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training a model using a \"Learning Rate Policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "model2=getModel(input_shape)\n",
    "model2.compile(optimizer=SGD(ini_LR), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Learning Rate Policy is included in the Callbacks\n",
    "training2=model2.fit(x_train, y_train, batch_size=64, epochs=num_epochs, validation_data=(x_val, y_val), callbacks=[clr_triangular], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])\n",
    "plt.show()\n",
    "\n",
    "loss = training3.history['loss']\n",
    "val_loss = training3.history['val_loss']\n",
    "acc = training3.history['acc']\n",
    "val_acc = training3.history['val_acc']\n",
    "\n",
    "tra = plt.plot(loss)\n",
    "val = plt.plot(val_loss, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# accuracy plot\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for easy access to object classes\n",
    "objects = {0: 'T-shirt',\n",
    "           1: 'Trouser',\n",
    "           2: 'Pullover',\n",
    "           3: 'Dress',\n",
    "           4: 'Coat',\n",
    "           5: 'Sandal',\n",
    "           6: 'Shirt',\n",
    "           7: 'Sneaker',\n",
    "           8: 'Bag',\n",
    "           9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Example of Analysis of Performance for a Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=np.argmax(model.predict(x_test),axis=1)\n",
    "cnf_matrix = confusion_matrix(y_pred, y_test)\n",
    "np.set_printoptions(precision=3)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#VISUALIZING SOME ERRORS\n",
    "val=np.nonzero(abs(y_pred-y_test))[0]\n",
    "f, axes = plt.subplots(4, 4, figsize=(9,9))\n",
    "for row in axes:\n",
    "    for axe in row:\n",
    "        index = np.random.randint(len(val))\n",
    "        img = x_test[val[index]]\n",
    "        obj = y_test[val[index]] #TRUE CLASS\n",
    "        obj2 = y_pred[val[index]] #PREDICTED CLASS\n",
    "        axe.imshow(img[:,:,0]*255, cmap='gray')\n",
    "        axe.set_title(objects[obj]+'/'+objects[obj2])\n",
    "        axe.set_axis_off()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
